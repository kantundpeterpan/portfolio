[
  {
    "objectID": "datacamp_site/guided.html",
    "href": "datacamp_site/guided.html",
    "title": "Datacamp",
    "section": "",
    "text": "Do students describe professors differently based on gender?\n\n\n\npython\n\n\nnlp\n\n\ndata prep\n\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nExploring ingredients of cosmetics\n\n\n\npython\n\n\nnlp\n\n\nviz\n\n\nunsupervised\n\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nExploring movie plots using NLP\n\n\n\npython\n\n\nnlp\n\n\nunsupervised\n\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nSQL - Electric vehicle charging\n\n\n\nsql\n\n\neda\n\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nSQL - International debt statistics\n\n\n\nsql\n\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nSQL - London transportation dataset\n\n\n\nsql\n\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nSQL - Mental health of international students\n\n\n\nsql\n\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nTrump vs. Trudeau: Tweet classification\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n - Rise and Fall of Programming Languages\n\n\n\nr\n\n\nviz\n\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Bio\nI’m an aspiring data scientist with a keen interest in any application of large-scale computing, machine learning and modeling especialy in biology and chemistry. By training, I am a pharmacist holding the equivalent of Master’s degree in medicinal chemistry from Friedrich-Schiller-University, Jena Germany and a PhD in biochemistry/microbiology from Sorbonne University, Paris, France.\nI am currently working on my transition into a data intenvsive role may it be in a health related field or not.\nOn this site you will find my CV, a list of my publications and number of my programming and data related projects.\nAt the end of summer, I hope that I will be admitted to the Master of Statistics and Data Science at the University of Hasselt, Belgium, so that I will be able to expand and deepen my theoretical understanding of and practical skill in Statistics and Data Science.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html",
    "title": "Exploring movie plots using NLP",
    "section": "",
    "text": "We all love watching movies! There are some movies we like, some we don’t. Most people have a preference for movies of a similar genre. Some of us love watching action movies, while some of us like watching horror. Some of us like watching movies that have ninjas in them, while some of us like watching superheroes.\n\n\nMovies within a genre often share common base parameters. Consider the following two movies:\n\n \n\nBoth movies, 2001: A Space Odyssey and Close Encounters of the Third Kind, are movies based on aliens coming to Earth. I’ve seen both, and they indeed share many similarities. We could conclude that both of these fall into the same genre of movies based on intuition, but that’s no fun in a data science context. In this notebook, we will quantify the similarity of movies based on their plot summaries available on IMDb and Wikipedia, then separate them into groups, also known as clusters. We’ll create a dendrogram to represent how closely the movies are related to each other.\n\n\nLet’s start by importing the dataset and observing the data provided.\n\n\n# Import modules\nimport numpy as np\nimport pandas as pd\nimport nltk\n\n# Set seed for reproducibility\nnp.random.seed(5)\n\n# Read in IMDb and Wikipedia movie data (both in same file)\nmovies_df = pd.read_csv('datasets/movies.csv')\n\nprint(\"Number of movies loaded: %s \" % (len(movies_df)))\n\n# Display the data\nmovies_df.head(10)\n\nNumber of movies loaded: 100 \n\n\n\n\n\n\n\n\n\nrank\ntitle\ngenre\nwiki_plot\nimdb_plot\n\n\n\n\n0\n0\nThe Godfather\n[u' Crime', u' Drama']\nOn the day of his only daughter's wedding, Vit...\nIn late summer 1945, guests are gathered for t...\n\n\n1\n1\nThe Shawshank Redemption\n[u' Crime', u' Drama']\nIn 1947, banker Andy Dufresne is convicted of ...\nIn 1947, Andy Dufresne (Tim Robbins), a banker...\n\n\n2\n2\nSchindler's List\n[u' Biography', u' Drama', u' History']\nIn 1939, the Germans move Polish Jews into the...\nThe relocation of Polish Jews from surrounding...\n\n\n3\n3\nRaging Bull\n[u' Biography', u' Drama', u' Sport']\nIn a brief scene in 1964, an aging, overweight...\nThe film opens in 1964, where an older and fat...\n\n\n4\n4\nCasablanca\n[u' Drama', u' Romance', u' War']\nIt is early December 1941. American expatriate...\nIn the early years of World War II, December 1...\n\n\n5\n5\nOne Flew Over the Cuckoo's Nest\n[u' Drama']\nIn 1963 Oregon, Randle Patrick \"Mac\" McMurphy ...\nIn 1963 Oregon, Randle Patrick McMurphy (Nicho...\n\n\n6\n6\nGone with the Wind\n[u' Drama', u' Romance', u' War']\n\\nPart 1\\n \\n Part 1 Part 1 \\n \\n On the...\nThe film opens in Tara, a cotton plantation ow...\n\n\n7\n7\nCitizen Kane\n[u' Drama', u' Mystery']\n\\n\\n\\n\\nOrson Welles as Charles Foster Kane\\n\\...\nIt's 1941, and newspaper tycoon Charles Foster...\n\n\n8\n8\nThe Wizard of Oz\n[u' Adventure', u' Family', u' Fantasy', u' Mu...\nThe film starts in sepia-tinted Kansas in the ...\nDorothy Gale (Judy Garland) is an orphaned tee...\n\n\n9\n9\nTitanic\n[u' Drama', u' Romance']\nIn 1996, treasure hunter Brock Lovett and his ...\nIn 1996, treasure hunter Brock Lovett and his ...",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-and-observe-dataset",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-and-observe-dataset",
    "title": "Exploring movie plots using NLP",
    "section": "",
    "text": "We all love watching movies! There are some movies we like, some we don’t. Most people have a preference for movies of a similar genre. Some of us love watching action movies, while some of us like watching horror. Some of us like watching movies that have ninjas in them, while some of us like watching superheroes.\n\n\nMovies within a genre often share common base parameters. Consider the following two movies:\n\n \n\nBoth movies, 2001: A Space Odyssey and Close Encounters of the Third Kind, are movies based on aliens coming to Earth. I’ve seen both, and they indeed share many similarities. We could conclude that both of these fall into the same genre of movies based on intuition, but that’s no fun in a data science context. In this notebook, we will quantify the similarity of movies based on their plot summaries available on IMDb and Wikipedia, then separate them into groups, also known as clusters. We’ll create a dendrogram to represent how closely the movies are related to each other.\n\n\nLet’s start by importing the dataset and observing the data provided.\n\n\n# Import modules\nimport numpy as np\nimport pandas as pd\nimport nltk\n\n# Set seed for reproducibility\nnp.random.seed(5)\n\n# Read in IMDb and Wikipedia movie data (both in same file)\nmovies_df = pd.read_csv('datasets/movies.csv')\n\nprint(\"Number of movies loaded: %s \" % (len(movies_df)))\n\n# Display the data\nmovies_df.head(10)\n\nNumber of movies loaded: 100 \n\n\n\n\n\n\n\n\n\nrank\ntitle\ngenre\nwiki_plot\nimdb_plot\n\n\n\n\n0\n0\nThe Godfather\n[u' Crime', u' Drama']\nOn the day of his only daughter's wedding, Vit...\nIn late summer 1945, guests are gathered for t...\n\n\n1\n1\nThe Shawshank Redemption\n[u' Crime', u' Drama']\nIn 1947, banker Andy Dufresne is convicted of ...\nIn 1947, Andy Dufresne (Tim Robbins), a banker...\n\n\n2\n2\nSchindler's List\n[u' Biography', u' Drama', u' History']\nIn 1939, the Germans move Polish Jews into the...\nThe relocation of Polish Jews from surrounding...\n\n\n3\n3\nRaging Bull\n[u' Biography', u' Drama', u' Sport']\nIn a brief scene in 1964, an aging, overweight...\nThe film opens in 1964, where an older and fat...\n\n\n4\n4\nCasablanca\n[u' Drama', u' Romance', u' War']\nIt is early December 1941. American expatriate...\nIn the early years of World War II, December 1...\n\n\n5\n5\nOne Flew Over the Cuckoo's Nest\n[u' Drama']\nIn 1963 Oregon, Randle Patrick \"Mac\" McMurphy ...\nIn 1963 Oregon, Randle Patrick McMurphy (Nicho...\n\n\n6\n6\nGone with the Wind\n[u' Drama', u' Romance', u' War']\n\\nPart 1\\n \\n Part 1 Part 1 \\n \\n On the...\nThe film opens in Tara, a cotton plantation ow...\n\n\n7\n7\nCitizen Kane\n[u' Drama', u' Mystery']\n\\n\\n\\n\\nOrson Welles as Charles Foster Kane\\n\\...\nIt's 1941, and newspaper tycoon Charles Foster...\n\n\n8\n8\nThe Wizard of Oz\n[u' Adventure', u' Family', u' Fantasy', u' Mu...\nThe film starts in sepia-tinted Kansas in the ...\nDorothy Gale (Judy Garland) is an orphaned tee...\n\n\n9\n9\nTitanic\n[u' Drama', u' Romance']\nIn 1996, treasure hunter Brock Lovett and his ...\nIn 1996, treasure hunter Brock Lovett and his ...",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#combine-wikipedia-and-imdb-plot-summaries",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#combine-wikipedia-and-imdb-plot-summaries",
    "title": "Exploring movie plots using NLP",
    "section": "2. Combine Wikipedia and IMDb plot summaries",
    "text": "2. Combine Wikipedia and IMDb plot summaries\n\nThe dataset we imported currently contains two columns titled wiki_plot and imdb_plot. They are the plot found for the movies on Wikipedia and IMDb, respectively. The text in the two columns is similar, however, they are often written in different tones and thus provide context on a movie in a different manner of linguistic expression. Further, sometimes the text in one column may mention a feature of the plot that is not present in the other column. For example, consider the following plot extracts from The Godfather:\n\n\n\nWikipedia: “On the day of his only daughter’s wedding, Vito Corleone”\n\n\nIMDb: “In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone’s daughter Connie”\n\n\n\nWhile the Wikipedia plot only mentions it is the day of the daughter’s wedding, the IMDb plot also mentions the year of the scene and the name of the daughter.\n\n\nLet’s combine both the columns to avoid the overheads in computation associated with extra columns to process.\n\n\n# Combine wiki_plot and imdb_plot into a single column\nmovies_df['plot'] = movies_df['wiki_plot'].astype(str) + \"\\n\" + \\\n                 movies_df['imdb_plot'].astype(str)\n\n# Inspect the new DataFrame\nmovies_df.head()\n\n\n\n\n\n\n\n\nrank\ntitle\ngenre\nwiki_plot\nimdb_plot\nplot\n\n\n\n\n0\n0\nThe Godfather\n[u' Crime', u' Drama']\nOn the day of his only daughter's wedding, Vit...\nIn late summer 1945, guests are gathered for t...\nOn the day of his only daughter's wedding, Vit...\n\n\n1\n1\nThe Shawshank Redemption\n[u' Crime', u' Drama']\nIn 1947, banker Andy Dufresne is convicted of ...\nIn 1947, Andy Dufresne (Tim Robbins), a banker...\nIn 1947, banker Andy Dufresne is convicted of ...\n\n\n2\n2\nSchindler's List\n[u' Biography', u' Drama', u' History']\nIn 1939, the Germans move Polish Jews into the...\nThe relocation of Polish Jews from surrounding...\nIn 1939, the Germans move Polish Jews into the...\n\n\n3\n3\nRaging Bull\n[u' Biography', u' Drama', u' Sport']\nIn a brief scene in 1964, an aging, overweight...\nThe film opens in 1964, where an older and fat...\nIn a brief scene in 1964, an aging, overweight...\n\n\n4\n4\nCasablanca\n[u' Drama', u' Romance', u' War']\nIt is early December 1941. American expatriate...\nIn the early years of World War II, December 1...\nIt is early December 1941. American expatriate...",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#tokenization",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#tokenization",
    "title": "Exploring movie plots using NLP",
    "section": "3. Tokenization",
    "text": "3. Tokenization\n\nTokenization is the process by which we break down articles into individual sentences or words, as needed. Besides the tokenization method provided by NLTK, we might have to perform additional filtration to remove tokens which are entirely numeric values or punctuation.\n\n\nWhile a program may fail to build context from “While waiting at a bus stop in 1981” (Forrest Gump), because this string would not match in any dictionary, it is possible to build context from the words “while”, “waiting” or “bus” because they are present in the English dictionary.\n\n\nLet us perform tokenization on a small extract from The Godfather.\n\n\n# Tokenize a paragraph into sentences and store in sent_tokenized\nsent_tokenized = [sent for sent in nltk.sent_tokenize(\"\"\"\n                        Today (May 19, 2016) is his only daughter's wedding. \n                        Vito Corleone is the Godfather.\n                        \"\"\")]\n\n# Word Tokenize first sentence from sent_tokenized, save as words_tokenized\nwords_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]\n\n# Remove tokens that do not contain any letters from words_tokenized\nimport re\n\nfiltered = [word for word in words_tokenized if re.search('[a-zA-Z]', word)]\n\n# Display filtered words to observe words after tokenization\nfiltered\n\n['Today', 'May', 'is', 'his', 'only', 'daughter', \"'s\", 'wedding']",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#stemming",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#stemming",
    "title": "Exploring movie plots using NLP",
    "section": "4. Stemming",
    "text": "4. Stemming\n\nStemming is the process by which we bring down a word from its different forms to the root word. This helps us establish meaning to different forms of the same words without having to deal with each form separately. For example, the words ‘fishing’, ‘fished’, and ‘fisher’ all get stemmed to the word ‘fish’.\n\n\nConsider the following sentences:\n\n\n\n“Young William Wallace witnesses the treachery of Longshanks” ~ Gladiator\n\n\n“escapes to the city walls only to witness Cicero’s death” ~ Braveheart\n\n\n\nInstead of building separate dictionary entries for both witnesses and witness, which mean the same thing outside of quantity, stemming them reduces them to ‘wit’.\n\n\nThere are different algorithms available for stemming such as the Porter Stemmer, Snowball Stemmer, etc. We shall use the Snowball Stemmer.\n\n\n# Import the SnowballStemmer to perform stemming\nfrom nltk.stem.snowball import SnowballStemmer\n\n# Create an English language SnowballStemmer object\nstemmer = SnowballStemmer(\"english\")\n\n# Print filtered to observe words without stemming\nprint(\"Without stemming: \", filtered)\n\n# Stem the words from filtered and store in stemmed_words\nstemmed_words = [stemmer.stem(word) for word in filtered]\n\n# Print the stemmed_words to observe words after stemming\nprint(\"After stemming:   \", stemmed_words)\n\nWithout stemming:  ['Today', 'May', 'is', 'his', 'only', 'daughter', \"'s\", 'wedding']\nAfter stemming:    ['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#club-together-tokenize-stem",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#club-together-tokenize-stem",
    "title": "Exploring movie plots using NLP",
    "section": "5. Club together Tokenize & Stem",
    "text": "5. Club together Tokenize & Stem\n\nWe are now able to tokenize and stem sentences. But we may have to use the two functions repeatedly one after the other to handle a large amount of data, hence we can think of wrapping them in a function and passing the text to be tokenized and stemmed as the function argument. Then we can pass the new wrapping function, which shall perform both tokenizing and stemming instead of just tokenizing, as the tokenizer argument while creating the TF-IDF vector of the text.\n\n\nWhat difference does it make though? Consider the sentence from the plot of The Godfather: “Today (May 19, 2016) is his only daughter’s wedding.” If we do a ‘tokenize-only’ for this sentence, we have the following result:\n\n\n\n‘today’, ‘may’, ‘is’, ‘his’, ‘only’, ‘daughter’, “‘s”, ’wedding’\n\n\n\nBut when we do a ‘tokenize-and-stem’ operation we get:\n\n\n\n‘today’, ‘may’, ‘is’, ‘his’, ‘onli’, ‘daughter’, “‘s”, ’wed’\n\n\n\nAll the words are in their root form, which will lead to a better establishment of meaning as some of the non-root forms may not be present in the NLTK training corpus.\n\n\n# Define a function to perform both stemming and tokenization\ndef tokenize_and_stem(text):\n    \n    # Tokenize by sentence, then by word\n    tokens = [sent for sent in nltk.sent_tokenize(text)]\n    tokens = [word for x in tokens for word in nltk.word_tokenize(x)]\n    \n    # Filter out raw tokens to remove noise\n    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n    \n    # Stem the filtered_tokens\n    stems = [stemmer.stem(token) for token in filtered_tokens]\n    \n    return stems\n\nwords_stemmed = tokenize_and_stem(\"Today (May 19, 2016) is his only daughter's wedding.\")\nprint(words_stemmed)\n\n['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#create-tfidfvectorizer",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#create-tfidfvectorizer",
    "title": "Exploring movie plots using NLP",
    "section": "6. Create TfidfVectorizer",
    "text": "6. Create TfidfVectorizer\n\nComputers do not understand text. These are machines only capable of understanding numbers and performing numerical computation. Hence, we must convert our textual plot summaries to numbers for the computer to be able to extract meaning from them. One simple method of doing this would be to count all the occurrences of each word in the entire vocabulary and return the counts in a vector. Enter CountVectorizer.\n\n\nConsider the word ‘the’. It appears quite frequently in almost all movie plots and will have a high count in each case. But obviously, it isn’t the theme of all the movies! Term Frequency-Inverse Document Frequency (TF-IDF) is one method which overcomes the shortcomings of CountVectorizer. The Term Frequency of a word is the measure of how often it appears in a document, while the Inverse Document Frequency is the parameter which reduces the importance of a word if it frequently appears in several documents.\n\n\nFor example, when we apply the TF-IDF on the first 3 sentences from the plot of The Wizard of Oz, we are told that the most important word there is ‘Toto’, the pet dog of the lead character. This is because the movie begins with ‘Toto’ biting someone due to which the journey of Oz begins!\n\n\nIn simplest terms, TF-IDF recognizes words which are unique and important to any given document. Let’s create one for our purposes.\n\n\n# Import TfidfVectorizer to create TF-IDF vectors\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Instantiate TfidfVectorizer object with stopwords and tokenizer\n# parameters for efficient processing of text\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=0.2, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem,\n                                 ngram_range=(1,3))",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#fit-transform-tfidfvectorizer",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#fit-transform-tfidfvectorizer",
    "title": "Exploring movie plots using NLP",
    "section": "7. Fit transform TfidfVectorizer",
    "text": "7. Fit transform TfidfVectorizer\n\nOnce we create a TF-IDF Vectorizer, we must fit the text to it and then transform the text to produce the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we use the fit_transform() method of the TfidfVectorizer object.\n\n\nIf we observe the TfidfVectorizer object we created, we come across a parameter stopwords. ‘stopwords’ are those words in a given text which do not contribute considerably towards the meaning of the sentence and are generally grammatical filler words. For example, in the sentence ‘Dorothy Gale lives with her dog Toto on the farm of her Aunt Em and Uncle Henry’, we could drop the words ‘her’ and ‘the’, and still have a similar overall meaning to the sentence. Thus, ‘her’ and ‘the’ are stopwords and can be conveniently dropped from the sentence.\n\n\nOn setting the stopwords to ‘english’, we direct the vectorizer to drop all stopwords from a pre-defined list of English language stopwords present in the nltk module. Another parameter, ngram_range, defines the length of the ngrams to be formed while vectorizing the text.\n\n\n# Fit and transform the tfidf_vectorizer with the \"plot\" of each movie\n# to create a vector representation of the plot summaries\ntfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[\"plot\"]])\n\nprint(tfidf_matrix.shape)\n\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n  warnings.warn(\n\n\n(100, 564)",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-kmeans-and-create-clusters",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-kmeans-and-create-clusters",
    "title": "Exploring movie plots using NLP",
    "section": "8. Import KMeans and create clusters",
    "text": "8. Import KMeans and create clusters\n\nTo determine how closely one movie is related to the other by the help of unsupervised learning, we can use clustering techniques. Clustering is the method of grouping together a number of items such that they exhibit similar properties. According to the measure of similarity desired, a given sample of items can have one or more clusters.\n\n\nA good basis of clustering in our dataset could be the genre of the movies. Say we could have a cluster ‘0’ which holds movies of the ‘Drama’ genre. We would expect movies like Chinatown or Psycho to belong to this cluster. Similarly, the cluster ‘1’ in this project holds movies which belong to the ‘Adventure’ genre (Lawrence of Arabia and the Raiders of the Lost Ark, for example).\n\n\nK-means is an algorithm which helps us to implement clustering in Python. The name derives from its method of implementation: the given sample is divided into K clusters where each cluster is denoted by the mean of all the items lying in that cluster.\n\n\nWe get the following distribution for the clusters:\n\n\n\n\n\n# Import k-means to perform clusters\nfrom sklearn.cluster import KMeans\n\n# Create a KMeans object with 5 clusters and save as km\nkm = KMeans(n_clusters=5)\n\n# Fit the k-means object with tfidf_matrix\nkm.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()\n\n# Create a column cluster to denote the generated cluster for each movie\nmovies_df[\"cluster\"] = clusters\n\n# Display number of films per cluster (clusters from 0 to 4)\nmovies_df['cluster'].value_counts() \n\ncluster\n1    38\n2    27\n3    18\n4    14\n0     3\nName: count, dtype: int64",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#calculate-similarity-distance",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#calculate-similarity-distance",
    "title": "Exploring movie plots using NLP",
    "section": "9. Calculate similarity distance",
    "text": "9. Calculate similarity distance\n\nConsider the following two sentences from the movie The Wizard of Oz:\n\n\n\n“they find in the Emerald City”\n\n\n“they finally reach the Emerald City”\n\n\n\nIf we put the above sentences in a CountVectorizer, the vocabulary produced would be “they, find, in, the, Emerald, City, finally, reach” and the vectors for each sentence would be as follows:\n\n\n\n1, 1, 1, 1, 1, 1, 0, 0\n\n\n1, 0, 0, 1, 1, 1, 1, 1\n\n\n\nWhen we calculate the cosine angle formed between the vectors represented by the above, we get a score of 0.667. This means the above sentences are very closely related. Similarity distance is 1 - cosine similarity angle. This follows from that if the vectors are similar, the cosine of their angle would be 1 and hence, the distance between then would be 1 - 1 = 0.\n\n\nLet’s calculate the similarity distance for all of our movies.\n\n\n# Import cosine_similarity to calculate similarity of movie plots\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Calculate the similarity distance\nsimilarity_distance = 1 - cosine_similarity(tfidf_matrix)",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-matplotlib-linkage-and-dendrograms",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-matplotlib-linkage-and-dendrograms",
    "title": "Exploring movie plots using NLP",
    "section": "10. Import Matplotlib, Linkage, and Dendrograms",
    "text": "10. Import Matplotlib, Linkage, and Dendrograms\n\nWe shall now create a tree-like diagram (called a dendrogram) of the movie titles to help us understand the level of similarity between them visually. Dendrograms help visualize the results of hierarchical clustering, which is an alternative to k-means clustering. Two pairs of movies at the same level of hierarchical clustering are expected to have similar strength of similarity between the corresponding pairs of movies. For example, the movie Fargo would be as similar to North By Northwest as the movie Platoon is to Saving Private Ryan, given both the pairs exhibit the same level of the hierarchy.\n\n\nLet’s import the modules we’ll need to create our dendrogram.\n\n\n# Import matplotlib.pyplot for plotting graphs\nimport matplotlib.pyplot as plt\n\n# Configure matplotlib to display the output inline\n%matplotlib inline\n\n# Import modules necessary to plot dendrogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#create-merging-and-plot-dendrogram",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#create-merging-and-plot-dendrogram",
    "title": "Exploring movie plots using NLP",
    "section": "11. Create merging and plot dendrogram",
    "text": "11. Create merging and plot dendrogram\n\nWe shall plot a dendrogram of the movies whose similarity measure will be given by the similarity distance we previously calculated. The lower the similarity distance between any two movies, the lower their linkage will make an intercept on the y-axis. For instance, the lowest dendrogram linkage we shall discover will be between the movies, It’s a Wonderful Life and A Place in the Sun. This indicates that the movies are very similar to each other in their plots.\n\n\n# Create mergings matrix \nmergings = linkage(similarity_distance, method='complete')\n\n# Plot the dendrogram, using title as label column\ndendrogram_ = dendrogram(mergings,\n               labels=[x for x in movies_df[\"title\"]],\n               leaf_rotation=90,\n               leaf_font_size=16,\n)\n\n# Adjust the plot\nfig = plt.gcf()\n_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]\nfig.set_size_inches(108, 21)\n\n# Show the plotted dendrogram\nplt.show()",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#which-movies-are-most-similar",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#which-movies-are-most-similar",
    "title": "Exploring movie plots using NLP",
    "section": "12. Which movies are most similar?",
    "text": "12. Which movies are most similar?\n\nWe can now determine the similarity between movies based on their plots! To wrap up, let’s answer one final question: which movie is most similar to the movie Braveheart?\n\n\n#find row index of Braveheart\nbidx = movies_df.title.eq('Braveheart').idxmax()\n#extract the vector containing the similarity vector for Bravheart\n#sort indices by descending similarity\n#first entry is Braveheart itself, second index points to most similar movie\nsimilar_idx = np.argsort(similarity_distance[bidx])[1]\n\n\n# Answer the question\nans = movies_df.title.iloc[similar_idx]\nprint(ans)\n\nGladiator\n\n\nWell, that makes sense!",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Nlp Movies Similarity",
      "Exploring movie plots using NLP"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html",
    "title": "Exploring ingredients of cosmetics",
    "section": "",
    "text": "Whenever I want to try a new cosmetic item, it’s so difficult to choose. It’s actually more than difficult. It’s sometimes scary because new items that I’ve never tried end up giving me skin trouble. We know the information we need is on the back of each product, but it’s really hard to interpret those ingredient lists unless you’re a chemist. You may be able to relate to this situation.\n\n\n\n\n\nSo instead of buying and hoping for the best, why don’t we use data science to help us predict which products may be good fits for us? In this notebook, we are going to create a content-based recommendation system where the ‘content’ will be the chemical components of cosmetics. Specifically, we will process ingredient lists for 1472 cosmetics on Sephora via word embedding, then visualize ingredient similarity using a machine learning method called t-SNE and an interactive visualization library called Bokeh. Let’s inspect our data first.\n\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.manifold import TSNE\n\n# Load the data\ndf = pd.read_csv('datasets/cosmetics.csv')\n\n# Check the first five rows \ndisplay(df.sample(5))\n\n# Inspect the types of products\nprint(df['Label'].value_counts())\n\n\n\n\n\n\n\n\nLabel\nBrand\nName\nPrice\nRank\nIngredients\nCombination\nDry\nNormal\nOily\nSensitive\n\n\n\n\n63\nMoisturizer\nDR. JART+\nCicapair ™ Tiger Grass Color Correcting Treatm...\n52\n4.2\nWater, Centella Asiatica Leaf Water, Isononyl ...\n1\n1\n1\n1\n1\n\n\n1458\nSun protect\nCOOLA\nSport Continuous Spray SPF 30 - Unscented\n32\n5.0\nAlcohol (Organic), Algae Extract (Organic), Al...\n1\n1\n1\n1\n1\n\n\n696\nTreatment\nPERRICONE MD\nNo Foundation Foundation Serum SPF 30\n60\n4.3\nCyclopentasiloxane, Dimethicone, Water, Titani...\n1\n1\n1\n1\n1\n\n\n357\nCleanser\nKATE SOMERVILLE\nEradiKate® Daily Cleanser Acne Treatment\n38\n4.3\nWater, Sodium Cocoyl Isethionate, Coco-Glucosi...\n1\n0\n1\n1\n0\n\n\n279\nMoisturizer\nCLINIQUE\nLimited Edition Dramatically Different Moistur...\n39\n0.0\nWater, Mineral Oil/Paraffinum Liquidum/Huile M...\n1\n1\n0\n0\n0\n\n\n\n\n\n\n\nMoisturizer    298\nCleanser       281\nFace Mask      266\nTreatment      248\nEye cream      209\nSun protect    170\nName: Label, dtype: int64",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#cosmetics-chemicals-its-complicated",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#cosmetics-chemicals-its-complicated",
    "title": "Exploring ingredients of cosmetics",
    "section": "",
    "text": "Whenever I want to try a new cosmetic item, it’s so difficult to choose. It’s actually more than difficult. It’s sometimes scary because new items that I’ve never tried end up giving me skin trouble. We know the information we need is on the back of each product, but it’s really hard to interpret those ingredient lists unless you’re a chemist. You may be able to relate to this situation.\n\n\n\n\n\nSo instead of buying and hoping for the best, why don’t we use data science to help us predict which products may be good fits for us? In this notebook, we are going to create a content-based recommendation system where the ‘content’ will be the chemical components of cosmetics. Specifically, we will process ingredient lists for 1472 cosmetics on Sephora via word embedding, then visualize ingredient similarity using a machine learning method called t-SNE and an interactive visualization library called Bokeh. Let’s inspect our data first.\n\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.manifold import TSNE\n\n# Load the data\ndf = pd.read_csv('datasets/cosmetics.csv')\n\n# Check the first five rows \ndisplay(df.sample(5))\n\n# Inspect the types of products\nprint(df['Label'].value_counts())\n\n\n\n\n\n\n\n\nLabel\nBrand\nName\nPrice\nRank\nIngredients\nCombination\nDry\nNormal\nOily\nSensitive\n\n\n\n\n63\nMoisturizer\nDR. JART+\nCicapair ™ Tiger Grass Color Correcting Treatm...\n52\n4.2\nWater, Centella Asiatica Leaf Water, Isononyl ...\n1\n1\n1\n1\n1\n\n\n1458\nSun protect\nCOOLA\nSport Continuous Spray SPF 30 - Unscented\n32\n5.0\nAlcohol (Organic), Algae Extract (Organic), Al...\n1\n1\n1\n1\n1\n\n\n696\nTreatment\nPERRICONE MD\nNo Foundation Foundation Serum SPF 30\n60\n4.3\nCyclopentasiloxane, Dimethicone, Water, Titani...\n1\n1\n1\n1\n1\n\n\n357\nCleanser\nKATE SOMERVILLE\nEradiKate® Daily Cleanser Acne Treatment\n38\n4.3\nWater, Sodium Cocoyl Isethionate, Coco-Glucosi...\n1\n0\n1\n1\n0\n\n\n279\nMoisturizer\nCLINIQUE\nLimited Edition Dramatically Different Moistur...\n39\n0.0\nWater, Mineral Oil/Paraffinum Liquidum/Huile M...\n1\n1\n0\n0\n0\n\n\n\n\n\n\n\nMoisturizer    298\nCleanser       281\nFace Mask      266\nTreatment      248\nEye cream      209\nSun protect    170\nName: Label, dtype: int64",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#focus-on-one-product-category-and-one-skin-type",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#focus-on-one-product-category-and-one-skin-type",
    "title": "Exploring ingredients of cosmetics",
    "section": "2. Focus on one product category and one skin type",
    "text": "2. Focus on one product category and one skin type\n\nThere are six categories of product in our data (moisturizers, cleansers, face masks, eye creams, and sun protection) and there are five different skin types (combination, dry, normal, oily and sensitive). Because individuals have different product needs as well as different skin types, let’s set up our workflow so its outputs (a t-SNE model and a visualization of that model) can be customized. For the example in this notebook, let’s focus in on moisturizers for those with dry skin by filtering the data accordingly.\n\n\ndf.columns\n\nIndex(['Label', 'Brand', 'Name', 'Price', 'Rank', 'Ingredients', 'Combination',\n       'Dry', 'Normal', 'Oily', 'Sensitive'],\n      dtype='object')\n\n\n\ndf.Label.unique()\n\narray(['Moisturizer', 'Cleanser', 'Treatment', 'Face Mask', 'Eye cream',\n       'Sun protect'], dtype=object)\n\n\n\n# Filter for moisturizers\nmoisturizers = df.query('Label == \"Moisturizer\"')\n\n# Filter for dry skin as well\nmoisturizers_dry = moisturizers.query('Dry == 1')\n\n# Reset index\nmoisturizers_dry = moisturizers_dry.reset_index(drop = True)",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#tokenizing-the-ingredients",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#tokenizing-the-ingredients",
    "title": "Exploring ingredients of cosmetics",
    "section": "3. Tokenizing the ingredients",
    "text": "3. Tokenizing the ingredients\n\nTo get to our end goal of comparing ingredients in each product, we first need to do some preprocessing tasks and bookkeeping of the actual words in each product’s ingredients list. The first step will be tokenizing the list of ingredients in Ingredients column. After splitting them into tokens, we’ll make a binary bag of words. Then we will create a dictionary with the tokens, ingredient_idx, which will have the following format:\n\n\n{ “ingredient”: index value, … }\n\n\n# Initialize dictionary, list, and initial index\ningredient_idx = {}\ncorpus = []\nidx = 0\n\n# For loop for tokenization\nfor i in range(len(moisturizers_dry)):    \n    ingredients = moisturizers_dry['Ingredients'][i]\n    ingredients_lower = ingredients.lower()\n    tokens = ingredients_lower.split(', ')\n    corpus.append(tokens)\n    for ingredient in tokens:\n        if ingredient not in ingredient_idx:\n            ingredient_idx[ingredient] = idx\n            idx += 1\n            \n# Check the result \nprint(\"The index for decyl oleate is\", ingredient_idx['decyl oleate'])\n\nThe index for decyl oleate is 25",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#initializing-a-document-term-matrix-dtm",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#initializing-a-document-term-matrix-dtm",
    "title": "Exploring ingredients of cosmetics",
    "section": "4. Initializing a document-term matrix (DTM)",
    "text": "4. Initializing a document-term matrix (DTM)\n\nThe next step is making a document-term matrix (DTM). Here each cosmetic product will correspond to a document, and each chemical composition will correspond to a term. This means we can think of the matrix as a “cosmetic-ingredient” matrix. The size of the matrix should be as the picture shown below.  To create this matrix, we’ll first make an empty matrix filled with zeros. The length of the matrix is the total number of cosmetic products in the data. The width of the matrix is the total number of ingredients. After initializing this empty matrix, we’ll fill it in the following tasks.\n\n\n# Get the number of items and tokens \nM = moisturizers_dry.shape[0]\nN = len(ingredient_idx.keys())\n\n# Initialize a matrix of zeros\nA = np.zeros((M, N))",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#creating-a-counter-function",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#creating-a-counter-function",
    "title": "Exploring ingredients of cosmetics",
    "section": "5. Creating a counter function",
    "text": "5. Creating a counter function\n\nBefore we can fill the matrix, let’s create a function to count the tokens (i.e., an ingredients list) for each row. Our end goal is to fill the matrix with 1 or 0: if an ingredient is in a cosmetic, the value is 1. If not, it remains 0. The name of this function, oh_encoder, will become clear next.\n\n\n# Define the oh_encoder function\ndef oh_encoder(tokens):\n    x = [0 for i in range(N)]\n    for ingredient in tokens:\n        # Get the index for each ingredient\n        idx = ingredient_idx[ingredient]\n        # Put 1 at the corresponding indices\n        x[idx] = 1\n    return x",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#the-cosmetic-ingredient-matrix",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#the-cosmetic-ingredient-matrix",
    "title": "Exploring ingredients of cosmetics",
    "section": "6. The Cosmetic-Ingredient matrix!",
    "text": "6. The Cosmetic-Ingredient matrix!\n\nNow we’ll apply the oh_encoder() functon to the tokens in corpus and set the values at each row of this matrix. So the result will tell us what ingredients each item is composed of. For example, if a cosmetic item contains water, niacin, decyl aleate and sh-polypeptide-1, the outcome of this item will be as follows.  This is what we called one-hot encoding. By encoding each ingredient in the items, the Cosmetic-Ingredient matrix will be filled with binary values.\n\n\n# Make a document-term matrix\ni = 0\nfor tokens in corpus:\n    A[i, :] = oh_encoder(tokens)\n    i += 1",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#dimension-reduction-with-t-sne",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#dimension-reduction-with-t-sne",
    "title": "Exploring ingredients of cosmetics",
    "section": "7. Dimension reduction with t-SNE",
    "text": "7. Dimension reduction with t-SNE\n\nThe dimensions of the existing matrix is (190, 2233), which means there are 2233 features in our data. For visualization, we should downsize this into two dimensions. We’ll use t-SNE for reducing the dimension of the data here.\n\n\nT-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique that is well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, this technique can reduce the dimension of data while keeping the similarities between the instances. This enables us to make a plot on the coordinate plane, which can be said as vectorizing. All of these cosmetic items in our data will be vectorized into two-dimensional coordinates, and the distances between the points will indicate the similarities between the items.\n\n\n# Dimension reduction with t-SNE\nmodel = TSNE(n_components = 2, learning_rate=200, random_state = 42)\ntsne_features = model.fit_transform(A)\n\n# Make X, Y columns \nmoisturizers_dry['X'] = tsne_features[:,0]\nmoisturizers_dry['Y'] = tsne_features[:,1]",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#lets-map-the-items-with-bokeh",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#lets-map-the-items-with-bokeh",
    "title": "Exploring ingredients of cosmetics",
    "section": "8. Let’s map the items with Bokeh",
    "text": "8. Let’s map the items with Bokeh\n\nWe are now ready to start creating our plot. With the t-SNE values, we can plot all our items on the coordinate plane. And the coolest part here is that it will also show us the name, the brand, the price and the rank of each item. Let’s make a scatter plot using Bokeh and add a hover tool to show that information. Note that we won’t display the plot yet as we will make some more additions to it.\n\n\nfrom bokeh.io import show, output_notebook, push_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool\noutput_notebook()\n\n# Make a source and a scatter plot  \nsource = ColumnDataSource(moisturizers_dry)\nplot = figure(x_axis_label = 'X', \n              y_axis_label = 'Y', \n              width = 500, height = 400)\nplot.circle(x = 'X', \n    y = 'Y', \n    source = source, \n    size = 10, color = '#FF7373', alpha = .8)\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\nGlyphRenderer(id = '1195', …)data_source = ColumnDataSource(id='1157', ...),glyph = Circle(id='1193', ...),hover_glyph = None,js_event_callbacks = {},js_property_callbacks = {},level = 'glyph',muted = False,muted_glyph = None,name = None,nonselection_glyph = Circle(id='1194', ...),selection_glyph = None,subscribed_events = [],tags = [],view = CDSView(id='1196', ...),visible = True,x_range_name = 'default',y_range_name = 'default')",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#adding-a-hover-tool",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#adding-a-hover-tool",
    "title": "Exploring ingredients of cosmetics",
    "section": "9. Adding a hover tool",
    "text": "9. Adding a hover tool\n\nWhy don’t we add a hover tool? Adding a hover tool allows us to check the information of each item whenever the cursor is directly over a glyph. We’ll add tooltips with each product’s name, brand, price, and rank (i.e., rating).\n\n\n# Create a HoverTool object\nhover = HoverTool(tooltips = [('Item', '@Name'),\n                              ('Brand', '@Brand'),\n                              ('Price', '$@Price'),\n                              ('Rank', '@Rank')])\nplot.add_tools(hover)",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#mapping-the-cosmetic-items",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#mapping-the-cosmetic-items",
    "title": "Exploring ingredients of cosmetics",
    "section": "10. Mapping the cosmetic items",
    "text": "10. Mapping the cosmetic items\n\nFinally, it’s show time! Let’s see how the map we’ve made looks like. Each point on the plot corresponds to the cosmetic items. Then what do the axes mean here? The axes of a t-SNE plot aren’t easily interpretable in terms of the original data. Like mentioned above, t-SNE is a visualizing technique to plot high-dimensional data in a low-dimensional space. Therefore, it’s not desirable to interpret a t-SNE plot quantitatively.\n\n\nInstead, what we can get from this map is the distance between the points (which items are close and which are far apart). The closer the distance between the two items is, the more similar the composition they have. Therefore this enables us to compare the items without having any chemistry background.\n\n\n# Plot the map\nshow(plot)",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#comparing-two-products",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#comparing-two-products",
    "title": "Exploring ingredients of cosmetics",
    "section": "11. Comparing two products",
    "text": "11. Comparing two products\n\nSince there are so many cosmetics and so many ingredients, the plot doesn’t have many super obvious patterns that simpler t-SNE plots can have (example). Our plot requires some digging to find insights, but that’s okay!\n\n\nSay we enjoyed a specific product, there’s an increased chance we’d enjoy another product that is similar in chemical composition. Say we enjoyed AmorePacific’s Color Control Cushion Compact Broad Spectrum SPF 50+. We could find this product on the plot and see if a similar product(s) exist. And it turns out it does! If we look at the points furthest left on the plot, we see LANEIGE’s BB Cushion Hydra Radiance SPF 50 essentially overlaps with the AmorePacific product. By looking at the ingredients, we can visually confirm the compositions of the products are similar (though it is difficult to do, which is why we did this analysis in the first place!), plus LANEIGE’s version is $22 cheaper and actually has higher ratings.\n\n\nIt’s not perfect, but it’s useful. In real life, we can actually use our little ingredient-based recommendation engine help us make educated cosmetic purchase choices.\n\n\n# Print the ingredients of two similar cosmetics\ncosmetic_1 = moisturizers_dry[moisturizers_dry['Name'] == \"Color Control Cushion Compact Broad Spectrum SPF 50+\"]\ncosmetic_2 = moisturizers_dry[moisturizers_dry['Name'] == \"BB Cushion Hydra Radiance SPF 50\"]\n\n# Display each item's data and ingredients\ndisplay(cosmetic_1)\nprint(cosmetic_1.Ingredients.values)\ndisplay(cosmetic_2)\nprint(cosmetic_2.Ingredients.values)\n\n\n\n\n\n\n\n\nLabel\nBrand\nName\nPrice\nRank\nIngredients\nCombination\nDry\nNormal\nOily\nSensitive\nX\nY\n\n\n\n\n45\nMoisturizer\nAMOREPACIFIC\nColor Control Cushion Compact Broad Spectrum S...\n60\n4.0\nPhyllostachis Bambusoides Juice, Cyclopentasil...\n1\n1\n1\n1\n1\n2.775364\n-0.274434\n\n\n\n\n\n\n\n['Phyllostachis Bambusoides Juice, Cyclopentasiloxane, Cyclohexasiloxane, Peg-10 Dimethicone, Phenyl Trimethicone, Butylene Glycol, Butylene Glycol Dicaprylate/Dicaprate, Alcohol, Arbutin, Lauryl Peg-9 Polydimethylsiloxyethyl Dimethicone, Acrylates/Ethylhexyl Acrylate/Dimethicone Methacrylate Copolymer, Polyhydroxystearic Acid, Sodium Chloride, Polymethyl Methacrylate, Aluminium Hydroxide, Stearic Acid, Disteardimonium Hectorite, Triethoxycaprylylsilane, Ethylhexyl Palmitate, Lecithin, Isostearic Acid, Isopropyl Palmitate, Phenoxyethanol, Polyglyceryl-3 Polyricinoleate, Acrylates/Stearyl Acrylate/Dimethicone Methacrylate Copolymer, Dimethicone, Disodium Edta, Trimethylsiloxysilicate, Ethylhexyglycerin, Dimethicone/Vinyl Dimethicone Crosspolymer, Water, Silica, Camellia Japonica Seed Oil, Camillia Sinensis Leaf Extract, Caprylyl Glycol, 1,2-Hexanediol, Fragrance, Titanium Dioxide, Iron Oxides (Ci 77492, Ci 77491, Ci77499).']\n\n\n\n\n\n\n\n\n\nLabel\nBrand\nName\nPrice\nRank\nIngredients\nCombination\nDry\nNormal\nOily\nSensitive\nX\nY\n\n\n\n\n55\nMoisturizer\nLANEIGE\nBB Cushion Hydra Radiance SPF 50\n38\n4.3\nWater, Cyclopentasiloxane, Zinc Oxide (CI 7794...\n1\n1\n1\n1\n1\n2.814905\n-0.277909\n\n\n\n\n\n\n\n['Water, Cyclopentasiloxane, Zinc Oxide (CI 77947), Ethylhexyl Methoxycinnamate, PEG-10 Dimethicone, Cyclohexasiloxane, Phenyl Trimethicone, Iron Oxides (CI 77492), Butylene Glycol Dicaprylate/Dicaprate, Niacinamide, Lauryl PEG-9 Polydimethylsiloxyethyl Dimethicone, Acrylates/Ethylhexyl Acrylate/Dimethicone Methacrylate Copolymer, Titanium Dioxide (CI 77891 , Iron Oxides (CI 77491), Butylene Glycol, Sodium Chloride, Iron Oxides (CI 77499), Aluminum Hydroxide, HDI/Trimethylol Hexyllactone Crosspolymer, Stearic Acid, Methyl Methacrylate Crosspolymer, Triethoxycaprylylsilane, Phenoxyethanol, Fragrance, Disteardimonium Hectorite, Caprylyl Glycol, Yeast Extract, Acrylates/Stearyl Acrylate/Dimethicone Methacrylate Copolymer, Dimethicone, Trimethylsiloxysilicate, Polysorbate 80, Disodium EDTA, Hydrogenated Lecithin, Dimethicone/Vinyl Dimethicone Crosspolymer, Mica (CI 77019), Silica, 1,2-Hexanediol, Polypropylsilsesquioxane, Chenopodium Quinoa Seed Extract, Magnesium Sulfate, Calcium Chloride, Camellia Sinensis Leaf Extract, Manganese Sulfate, Zinc Sulfate, Ascorbyl Glucoside.']",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Cosmetic Ingredients",
      "Exploring ingredients of cosmetics"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html",
    "href": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html",
    "title": "SQL - Mental health of international students",
    "section": "",
    "text": "Illustration of silhouetted heads",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "Student Mental Health",
      "SQL - Mental health of international students"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html#quick-look-at-the-data",
    "href": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html#quick-look-at-the-data",
    "title": "SQL - Mental health of international students",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n-- Run this code to view the data in students\nSELECT * \nFROM students;\n\n\n\n\n\n\n\n\ninter_dom\nregion\ngender\nacademic\nage\nage_cate\nstay\nstay_cate\njapanese\njapanese_cate\nenglish\nenglish_cate\nintimate\nreligion\nsuicide\ndep\ndeptype\ntodep\ndepsev\ntosc\napd\nahome\naph\nafear\nacs\naguilt\namiscell\ntoas\npartner\nfriends\nparents\nrelative\nprofess\nphone\ndoctor\nreli\nalone\nothers\ninternet\npartner_bi\nfriends_bi\nparents_bi\nrelative_bi\nprofessional_bi\nphone_bi\ndoctor_bi\nreligion_bi\nalone_bi\nothers_bi\ninternet_bi\n\n\n\n\n0\nInter\nSEA\nMale\nGrad\n24.0\n4.0\n5.0\nLong\n3.0\nAverage\n5.0\nHigh\n\nYes\nNo\nNo\nNo\n0.0\nMin\n34.0\n23.0\n9.0\n11.0\n8.0\n11.0\n2.0\n27.0\n91.0\n5.0\n5.0\n6.0\n3.0\n2.0\n1.0\n4.0\n1.0\n3.0\n4.0\nNaN\nYes\nYes\nYes\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n1\nInter\nSEA\nMale\nGrad\n28.0\n5.0\n1.0\nShort\n4.0\nHigh\n4.0\nHigh\n\nNo\nNo\nNo\nNo\n2.0\nMin\n48.0\n8.0\n7.0\n5.0\n4.0\n3.0\n2.0\n10.0\n39.0\n7.0\n7.0\n7.0\n4.0\n4.0\n4.0\n4.0\n1.0\n1.0\n1.0\nNaN\nYes\nYes\nYes\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n2\nInter\nSEA\nMale\nGrad\n25.0\n4.0\n6.0\nLong\n4.0\nHigh\n4.0\nHigh\nYes\nYes\nNo\nNo\nNo\n2.0\nMin\n41.0\n13.0\n4.0\n7.0\n6.0\n4.0\n3.0\n14.0\n51.0\n3.0\n3.0\n3.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n3\nInter\nEA\nFemale\nGrad\n29.0\n5.0\n1.0\nShort\n2.0\nLow\n3.0\nAverage\nNo\nNo\nNo\nNo\nNo\n3.0\nMin\n37.0\n16.0\n10.0\n10.0\n8.0\n6.0\n4.0\n21.0\n75.0\n5.0\n5.0\n5.0\n5.0\n5.0\n2.0\n2.0\n2.0\n4.0\n4.0\nNaN\nYes\nYes\nYes\nYes\nYes\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n4\nInter\nEA\nFemale\nGrad\n28.0\n5.0\n1.0\nShort\n1.0\nLow\n3.0\nAverage\nYes\nNo\nNo\nNo\nNo\n3.0\nMin\n37.0\n15.0\n12.0\n5.0\n8.0\n7.0\n4.0\n31.0\n82.0\n5.0\n5.0\n5.0\n2.0\n5.0\n2.0\n5.0\n5.0\n4.0\n4.0\nNaN\nYes\nYes\nYes\nNo\nYes\nNo\nYes\nYes\nNo\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n281\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n128\n140\n\n\n\n\n\n\n\n\n\n\n\n282\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n137\n131\n\n\n\n\n\n\n\n\n\n\n\n283\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n66\n202\n\n\n\n\n\n\n\n\n\n\n\n284\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n61\n207\n\n\n\n\n\n\n\n\n\n\n\n285\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n30\n238\n\n\n\n\n\n\n\n\n\n\n\n\n\n286 rows × 50 columns",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "Student Mental Health",
      "SQL - Mental health of international students"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html#summary-statistics-of-internationsl-students-conditioned-on-their-length-of-stay",
    "href": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html#summary-statistics-of-internationsl-students-conditioned-on-their-length-of-stay",
    "title": "SQL - Mental health of international students",
    "section": "Summary statistics of internationsl students conditioned on their length of stay",
    "text": "Summary statistics of internationsl students conditioned on their length of stay\n\nExplore and analyze the students data to see how the length of stay (stay) impacts the average mental health diagnostic scores of the international students present in the study.\n\n\n\nReturn a table with nine rows and five columns.\n\n\nThe five columns should be aliased as: stay, count_int, average_phq, average_scs, and average_as, in that order.\n\n\nThe average columns should contain the average of the todep (PHQ-9 test), tosc (SCS test), and toas (ASISS test) columns for each length of stay, rounded to two decimal places.\n\n\nThe count_int column should be the number of international students for each length of stay.\n\n\nSort the results by the length of stay in descending order.\n\n\n\n-- query\nSELECT \n    stay, \n    COUNT(inter_dom) AS count_int,\n    ROUND(AVG(todep),2) AS average_phq,\n    ROUND(AVG(tosc),2) AS average_scs,\n    ROUND(AVG(toas),2) AS average_as\nFROM students\nWHERE inter_dom = 'Inter'\nGROUP BY stay\nORDER BY stay DESC\n\n\n\n\n\n\n\n\nstay\ncount_int\naverage_phq\naverage_scs\naverage_as\n\n\n\n\n0\n10\n1\n13.00\n32.00\n50.00\n\n\n1\n8\n1\n10.00\n44.00\n65.00\n\n\n2\n7\n1\n4.00\n48.00\n45.00\n\n\n3\n6\n3\n6.00\n38.00\n58.67\n\n\n4\n5\n1\n0.00\n34.00\n91.00\n\n\n5\n4\n14\n8.57\n33.93\n87.71\n\n\n6\n3\n46\n9.09\n37.13\n78.00\n\n\n7\n2\n39\n8.28\n37.08\n77.67\n\n\n8\n1\n95\n7.48\n38.11\n72.80",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "Student Mental Health",
      "SQL - Mental health of international students"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/ev_charging/notebook.html",
    "href": "datacamp_projects/workspace/SQL/ev_charging/notebook.html",
    "title": "SQL - Electric vehicle charging",
    "section": "",
    "text": "As electronic vehicles (EVs) become more popular, there is an increasing need for access to charging stations, also known as ports. To that end, many modern apartment buildings have begun retrofitting their parking garages to include shared charging stations. A charging station is shared if it is accessible by anyone in the building.\n\nBut with increasing demand comes competition for these ports — nothing is more frustrating than coming home to find no charging stations available! In this project, you will use a dataset to help apartment building managers better understand their tenants’ EV charging habits.\nThe data has been loaded into a PostgreSQL database with a table named charging_sessions with the following columns:",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "Ev Charging",
      "SQL - Electric vehicle charging"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#unique-users-per-garage",
    "href": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#unique-users-per-garage",
    "title": "SQL - Electric vehicle charging",
    "section": "Unique users per garage",
    "text": "Unique users per garage\nFind the number of unique individuals that use each garage’s shared charging stations. The output should contain two columns: garage_id and num_unique_users. Sort your results by the number of unique users from highest to lowest. Save the result as unique_users_per_garage.\n\n-- unique_users_per_garage\nSELECT garage_id, COUNT(DISTINCT user_id) as num_unique_users\nFROM charging_sessions\nWHERE user_type = 'Shared'\nGROUP BY garage_id\nORDER BY num_unique_users DESC\n\n\n\n\n\n\n\n\ngarage_id\nnum_unique_users\n\n\n\n\n0\nBl2\n18\n\n\n1\nAsO2\n17\n\n\n2\nUT9\n16\n\n\n3\nAdO3\n3\n\n\n4\nMS1\n2\n\n\n5\nSR2\n2\n\n\n6\nAdA1\n1\n\n\n7\nRis\n1",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "Ev Charging",
      "SQL - Electric vehicle charging"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#most-popular-starting-times-per-weekday-for-shared-charging-columns",
    "href": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#most-popular-starting-times-per-weekday-for-shared-charging-columns",
    "title": "SQL - Electric vehicle charging",
    "section": "Most popular starting times per weekday for shared charging columns",
    "text": "Most popular starting times per weekday for shared charging columns\nFind the top 10 most popular charging start times (by weekday and start hour) for sessions that use shared charging stations. Your result should contain three columns: weekdays_plugin, start_plugin_hour, and a column named num_charging_sessions containing the number of plugins on that weekday at that hour. Sort your results from the most to the least number of sessions. Save the result as most_popular_shared_start_times.\n\n-- most_popular_shared_start_times\nSELECT weekdays_plugin, start_plugin_hour, Count(*) as num_charging_sessions\nFROM charging_sessions\nWHERE user_type = 'Shared'\nGROUP BY weekdays_plugin, start_plugin_hour\nORDER BY num_charging_sessions DESC\nLIMIT 10\n\n\n\n\n\n\n\n\nweekdays_plugin\nstart_plugin_hour\nnum_charging_sessions\n\n\n\n\n0\nSunday\n17\n30\n\n\n1\nFriday\n15\n28\n\n\n2\nThursday\n19\n26\n\n\n3\nThursday\n16\n26\n\n\n4\nWednesday\n19\n25\n\n\n5\nSunday\n18\n25\n\n\n6\nSunday\n15\n25\n\n\n7\nMonday\n15\n24\n\n\n8\nFriday\n16\n24\n\n\n9\nTuesday\n16\n23",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "Ev Charging",
      "SQL - Electric vehicle charging"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#users-with-longest-avergae-charging-time-on-shared-columns",
    "href": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#users-with-longest-avergae-charging-time-on-shared-columns",
    "title": "SQL - Electric vehicle charging",
    "section": "Users with longest avergae charging time on shared columns",
    "text": "Users with longest avergae charging time on shared columns\nFind the users whose average charging duration last longer than 10 hours when using shared charging stations. Your result should contain two columns: user_id and avg_charging_duration. Sort your result from highest to lowest average charging duration. Save the result as long_duration_shared_users.\n\n-- long_duration_shared_users\nSELECT * FROM(\n    SELECT user_id, AVG(duration_hours) as avg_charging_duration\n    FROM public.charging_sessions\n    WHERE user_type = 'Shared' \n    GROUP BY public.charging_sessions.user_id\n    ORDER BY avg_charging_duration DESC\n    ) as calc\nWHERE calc.avg_charging_duration is not null AND calc.avg_charging_duration &gt; 10\n\n\n\n\n\n\n\n\nuser_id\navg_charging_duration\n\n\n\n\n0\nShare-9\n16.845833\n\n\n1\nShare-17\n12.894556\n\n\n2\nShare-25\n12.214475\n\n\n3\nShare-18\n12.088807\n\n\n4\nShare-8\n11.550431\n\n\n5\nAdO3-1\n10.369387",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "Ev Charging",
      "SQL - Electric vehicle charging"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/solution/notebook-solution.html",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/solution/notebook-solution.html",
    "title": "Do students describe professors differently based on gender?",
    "section": "",
    "text": "Language plays a crucial role in shaping our perceptions and attitudes towards gender in the workplace, in classrooms, and personal relationships. Studies have shown that gender bias in language can have a significant impact on the way people are perceived and treated. For example, research has found that job advertisements that use masculine-coded language tend to attract more male applicants, while those that use feminine-coded language tend to attract more female applicants. Similarly, gendered language can perpetuate differences in the classroom.In this project, we’ll using scraped student reviews from ratemyprofessors.com to identify differences in language commonly used for male vs. female professors, and explore subtleties in how language in the classroom can be gendered.This excellent tool created by Ben Schmidt allows us to enter the words and phrases that we find in our analysis and explore them in more depth. We’ll do this at the end.Catalyst also does some incredible work on decoding gendered language."
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/solution/notebook-solution.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/solution/notebook-solution.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "title": "Do students describe professors differently based on gender?",
    "section": "3a. What additional package imports are required for data visualization and NLP?",
    "text": "3a. What additional package imports are required for data visualization and NLP?\n::: {#8d6a7a01-887d-4df1-98b6-5a3c95e35519 .cell jupyter=‘{“outputs_hidden”:false,“source_hidden”:false}’ executionTime=‘841’ lastSuccessfullyExecutedCode=’import numpy as np # For manipulating matrices during NLP\nimport nltk # Natural language toolkit from nltk.tokenize import word_tokenize # Used for breaking up strings of text (e.g. sentences) into words from nltk.stem.porter import PorterStemmer # Used to return the dictionary base of a word from nltk.tokenize import WhitespaceTokenizer # Used for breaking up strings of text (e.g. sentences) into words based on white space\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # Used to count the occurences of words and phrases from sklearn.feature_extraction import text # Using to extrat features from text"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/solution/notebook-solution.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/solution/notebook-solution.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "title": "Do students describe professors differently based on gender?",
    "section": "Task 3d. What are the most important words being used to describe professors in reviews?",
    "text": "Task 3d. What are the most important words being used to describe professors in reviews?\nLet’s write a custom function that tokenizes and lemmatizes our list of words.- Word tokenization: process of splitting text into individual words, called tokens. A common preprocessing step in natural language processing (NLP) so that text can be analyzed and processed more easily. Methods include whitespace tokenization, regular expression-based tokenization, and rule-based tokenization. We’ll be using the word_tokenize tokenizer from nltk, with all its defaults.- Lemmatization: process of reducing words to their base or dictionary form, called the lemma. Also a common pre-processing step in NLP, so that words with a common base form are treated the same way. For example, the lemma of “am” is “be”, of “running” is “run”, and of “mice” is “mouse”.\n\ndef tokenize(text):    tk = WhitespaceTokenizer()    tokens = tk.tokenize(text)    stems = []    for item in tokens:        stems.append(PorterStemmer().stem(item))    return stems    # return tokens\n\nLet’s import a list of stop words, which are common English words that we will be ignoring in our analysis. sklearn provides a common list of stop words, and we can append additional words to this list. Below, we append pronouns, along with the words “class” and “student”.\n\nmy_stop_words = text.ENGLISH_STOP_WORDS.union([\"he\",\"she\",\"his\",\"her\",                                              \"himself\",\"herself\", \"hers\",\"shes\"                                              \"class\",\"student\"])\n\nFor the purpose of analyzing review texts, we want to move from having one row for each professor to one row for each review. Lets do this with .explode() from pandas.\n\ndf_quality = df[(df['review'].apply(len) == df['quality'].apply(len))]q = df_quality[['pronouns','review','quality']].explode(['review','quality'], ignore_index=True).dropna()q['quality'] = q['quality'].astype(float)\n\nTFIDF vectorization is the process of assigning scores to each review in a document based on how frequently the word occurs, normalized by how frequently the word occurs in the dataset overall.We’ll use TfidfVectorizer() to generate these scores. This will return a matrix, with as many rows as reviews, and as many columns as words in our dataset.\n\nvec = TfidfVectorizer(tokenizer=tokenize, stop_words=my_stop_words,                     ngram_range=(1,4))X = vec.fit_transform(q['review'])feature_names = vec.get_feature_names_out()\n\nX is a sparse matrix. We’ll now move into filtering X for:- Male professors only- Female professors only- Rows with male professors and reviews of high quality - Rows with female professors and reviews of high quality - Rows with male professors and reviews of low quality - Rows with female professors and reviews of low quality We can explore feature importance in each of these to get a sense of which words and phrases are coming up most often in the data.\n\nm_pos = X[(q['pronouns']=='M') & (q['quality']&gt;=4.5),:] f_pos = X[(q['pronouns']=='F') & (q['quality']&gt;=4.5),:] m_neg = X[(q['pronouns']=='M') & (q['quality']&lt;2.5),:] f_neg = X[(q['pronouns']=='M') & (q['quality']&lt;2.5),:] \n\nLet’s have a look at what language students are using to describe male professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(m_pos.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'great', 'hi', 'veri', 'class', 'best', 'good',\n       'professor', 'realli', 'teacher', \"he'\", 'wa', 'thi', 'make',\n       'help', 'easi', 'love', 'prof', 'awesom', 'know', 'class.',\n       'learn', 'lectur', 'amaz', 'cours', 'excel', 'just', 'test',\n       'alway', 'prof.', 'lot', 'work', 'nice', 'ha', \"i'v\", 'teach',\n       'teacher.', 'hi class', 'want', 'best professor', 'him.',\n       'professor.', 'like', 'guy', 'class,', 'dr.', 'funni', 'hard',\n       'need', 'read', 'know hi', 'fun', 'clear', 'materi', 'enjoy',\n       'great teacher', 'recommend', 'studi', 'exam', 'care', 'note',\n       'guy.', 'time', 'best prof', 'understand', 'best teacher',\n       'teacher,', 'everi', 'definit', \"don't\", \"you'll\", 'it.', 'onli',\n       'everyth', 'man', 'becaus', 'ani', 'fair', 'took', 'students.',\n       'extrem', 'veri good', 'use', 'had.', 'knowledg', 'explain',\n       'prof!', 'interesting.', 'well.', 'grade', 'veri help', 'question',\n       'fantast', 'math', 'book', 'thi class', 'hi lectur', 'write',\n       'highli', 'think', 'doe', 'attend', 'actual', 'professor,',\n       'thing', 'pretti', 'stuff', 'assign', 'prof,', 'hi students.',\n       'got', 'helpful.', 'befor', 'favorit', 'peopl', 'sure', 'him!',\n       'way', 'care hi', \"it'\", 'long', 'talk', 'absolut', 'super',\n       'guy,', '-', 'passion', 'littl', 'funny,', 'tough', 'hi class.',\n       'cool', '&', 'tri', 'professor!', 'great teacher,', 'wish', 'tell',\n       'person', 'a.', 'good teacher', 'helpful,', 'final', 'make sure',\n       'year', 'expect', 'difficult', 'make class', 'come', 'ask',\n       'love thi', 'subject', 'everyon', 'teacher!', 'man.', 'hi test',\n       'look', 'u', 'material.', 'answer', 'listen', 'sens', 'pay',\n       'class wa', 'great prof.', 'paper', 'offic', 'great professor.',\n       'wa veri', 'worth', 'say', 'far', 'histori', \"you'r\", 'hi stuff',\n       'great teacher.', 'great prof!', 'know hi stuff', 'interesting,',\n       'veri easi', 'easy.', 'bit', 'learn lot', 'veri nice', \"doesn't\",\n       'veri clear', 'classes.', 'thought', \"he' veri\", 'best.', 'smart',\n       'fine.', 'anyon', \"i'v had.\", 'realli enjoy', 'onlin', 'you.',\n       'love hi', 'prepar', 'thi class.', 'stori', 'essay', 'exampl',\n       'entertain', 'attent', 'better', 'course.', 'mani', '2',\n       'homework', 'taken', 'engag', 'bad', 'great prof,', 'notes.',\n       'textbook', 'truli', 'realli know', 'awesome.', 'real',\n       'excel teacher.', \"didn't\", 'inform', 'miss', 'challeng', 'review',\n       'old', 'class!', \"professor i'v\", 'thi guy', 'problem', 'let',\n       'genuin', 'great!', 'too.', 'did', 'work.', 'topic', 'great guy.',\n       \"he'll\", 'best!', 'enjoy hi', 'brilliant', 'veri knowledg',\n       'concept', 'overal', 'bore', \"i'm\", 'dure', \"teacher i'v\",\n       'alway help', 'approach', 'stuff.', 'feel', 'thi professor',\n       'pay attent', 'intellig', 'mark', 'effort', 'midterm', 'great.',\n       'extra', 'tests.', 'veri helpful.', 'instructor', 'text', 'reason',\n       'hi stuff.', 'taught', 'quizz', 'goe', 'lab', 'end', 'teacher!!',\n       'easy,', ':)', 'lot.', 'great professor', 'know hi stuff.',\n       'probabl', 'grade.', 'quit', 'pass', 'exams.', 'fair.', 'kind',\n       'hand', 'incred', 'avail', 'realli know hi', 'highli recommend',\n       \"can't\", 'wonder', 'humor', 'lectures.', 'job', 'him,',\n       'great prof', 'major', 'hour'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(f_pos.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'veri', 'great', 'class', 'help', \"she'\", 'easi',\n       'realli', 'best', 'wa', 'good', 'professor', 'thi', 'teacher',\n       'make', 'love', 'lot', 'prof', 'class.', 'work', 'learn', 'prof.',\n       'nice', 'lectur', 'cours', 'like', 'amaz', 'want', 'excel',\n       'teacher.', 'care', 'know', 'her.', 'alway', 'ha', 'wonder',\n       'just', 'extrem', 'hard', 'awesom', 'recommend', 'dr.', 'need',\n       'test', 'fair', 'time', 'understand', 'professor.', \"don't\",\n       'studi', 'best teacher', 'teach', 'her!', 'read', 'exam', 'grade',\n       'assign', 'class,', 'veri help', 'highli', 'thing',\n       'best professor', 'helpful.', 'materi', 'clear', 'interesting.',\n       'super', \"it'\", \"i'v\", 'onlin', 'great.', 'awesome!', 'enjoy',\n       'extra', 'sure', 'ani', 'thi class', 'definit', 'students.',\n       'pretti', 'becaus', 'question', 'ladi', 'pay', 'everi', 'her,',\n       'talk', 'mrs.', 'veri nice', 'actual', \"you'll\", 'work,',\n       'passion', 'took', 'teacher!', 'veri good', 'homework',\n       \"she' veri\", 'everyth', 'fun', 'class!', 'best prof', 'professor,',\n       'well.', 'come', 'onli', 'better', '-', 'think', 'great prof.',\n       'explain', 'wa veri', 'real', 'highli recommend', 'doe', 'tough',\n       'wish', 'world', 'a.', 'excel prof', 'long', 'use', 'littl',\n       'person', 'material.', 'attent', 'great teacher.', 'sweet',\n       'veri easi', 'peopl', 'prof,', 'helpful,', 'credit', 'had.', '&',\n       'it.', 'anyth', 'great!', 'lab', 'knowledg', 'expect', 'prof!',\n       'problem', 'write', 'professor!', 'teacher,', \"you'r\", 'approach',\n       'look', 'math', 'pass', 'thi cours', 'pay attent', 'discuss',\n       'note', 'you.', 'absolut', 'class wa', 'course.', 'mani', 'ever!',\n       'befor', 'extra credit', 'sens', 'especi', 'taught', 'person.',\n       'got', 'learn lot', 'help.', 'bit', 'work.',\n       'best professor ever!', 'understand.', 'classes.', 'fair.',\n       'lectures.', 'ask', 'great prof', 'nice,', 'grade.', 'tell',\n       'year', 'lady.', 'professor ever!', 'point', 'thought', 'attend',\n       'cool', 'quizz', 'make sure', 'way', 'veri helpful.', 'answer',\n       'students,', 'fine.', \"she'll\", 'easi understand.', 'amazing.',\n       'did', 'end', 'instructor', 'great teacher', 'project', 'challeng',\n       'respect', 'offer', 'great professor!', 'entertain', 'essay',\n       'good lectur', 'veri clear', 'favorit', 'difficult', 'english',\n       'exampl', 'anyon', 'truli', 'great prof!', 'great teacher,', 'goe',\n       'realli want', \"i'm\", 'nice lady.', 'intellig', 'classes!',\n       'awsom', 'concept', 'veri helpful,', 'lot.', 'book', \"doesn't\",\n       'group', 'lectures,', 'them.', 'listen', 'kind', 'smart', '3',\n       'funni', \"i'd\", 'easy.', 'veri fair', 'great professor,', 'tri',\n       \"she' realli\", 'make class', 'open', 'guid', 'recommend her.',\n       'inspir', 'spanish', 'account', 'probabl', 'offic', 'easi grade!',\n       'paper', 'school', 'woman', 'great prof,', 'great teacher!',\n       'questions.', 'taken', 'help prof', 'real world', 'love her!',\n       'stori', 'good professor.', 'let', \"isn't\", 'effort', \"prof i'v\",\n       'worth', 'alot', 'avail', 'funny.', 'subject', 'book.', 'hour',\n       'job', 'too.', 'yummer', 'best!!', 'ã\\x82â', 'knowledgable.',\n       'research', 'say', 'clearli', 'engag', 'realli know', 'helpful!',\n       'requir', 'prepar', 'demand', 'exams.', 'midterm', 'thank', 'far',\n       'great professor.', 'easy,'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe male professors negatively.\n\nimportance = np.argsort(np.asarray(m_neg.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'hi', 'thi', 'class', 'veri', 'wa', 'teach', 'worst',\n       'test', 'lectur', \"doesn't\", 'class.', 'like', 'hard', 'just',\n       \"don't\", 'professor', 'doe', 'know', 'grade', 'make', 'avoid',\n       'bore', 'teacher', \"he'\", 'question', 'onli', 'terribl', 'good',\n       'read', 'horribl', 'ha', 'learn', 'cours', 'prof', 'time', 'exam',\n       'understand', 'thi class', 'talk', 'becaus', 'help', 'realli',\n       'guy', 'him.', 'explain', 'bad', 'say', 'ask', 'ani', 'hi class',\n       \"can't\", 'thing', 'think', \"didn't\", 'way', 'book', 'thi guy',\n       'noth', 'need', 'materi', 'answer', 'wast', 'hi lectur', 'extrem',\n       'work', 'everi', 'want', 'expect', 'nice', 'teacher.', 'class,',\n       'assign', 'worst professor', 'all.', 'studi', 'professor.', 'did',\n       'note', 'anyth', 'use', \"i'v\", 'difficult', 'final', 'unclear',\n       'tell', 'drop', 'hi test', 'complet', 'worst prof', 'confus',\n       'dont', 'man', 'tri', 'absolut', 'got', 'time.', 'everyth', '-',\n       'easi', 'had.', 'rude', 'it.', 'write', 'fail', 'stay',\n       'worst teacher', '3', 'point', 'math', 'hard.', 'class wa', 'lot',\n       'mark', \"it'\", 'day', 'took', 'students.', 'clear', 'anoth',\n       'homework', 'tests.', 'useless', 'recommend', 'awful.', 'peopl',\n       'goe', 'littl', 'entir', 'unless', 'word', 'hi class.', 'someon',\n       'actual', 'come', 'textbook', 'hour', 'boring.', 'thi professor',\n       'lectures.', 'problem', 'dure', 'attend', 'better', 'half', 'went',\n       'veri hard', 'aw', 'care', 'pass', 'doesnt', 'prepar', 'everyon',\n       'person', 'paper', 'course.', 'ever.', 'look', '2', 'whi', 'mean',\n       'feel', 'year', 'arrog', 'midterm', 'prof.', 'ask question',\n       'thi class.', 'u', 'alway', \"i'm\", 'pretti', \"you'r\", 'hardest',\n       'speak', 'hate', 'imposs', 'old', 'befor', '&', 'end', 'thi cours',\n       'poor', 'subject', 'semest', 'said', 'told', 'guy,', 'someth',\n       'test.', 'basic', 'grade.', 'project', 'thi man', \"isn't\", 'mani',\n       'onlin', 'rambl', 'listen', 'idea', 'sit', 'base', 'differ',\n       'tough', 'incred', 'taught', 'essay', 'him,', 'away', 'hear',\n       'major', 'you.', \"won't\", 'terrible.', 'school', 'total', 'random',\n       'refus', 'concept', 'teach.', 'high', 'sure', 'exampl', 'slide',\n       'probabl', \"you'll\", 'anything.', 'veri difficult', 'week',\n       'hi teach', 'averag', 'questions.', 'follow', 'smart', 'save',\n       \"i'v had.\", 'possibl', 'avoid thi', 'right', 'life', 'wrote',\n       'far', 'boring,', 'bore lectur', 'topic', 'offic', 'knowledg',\n       'long', 'present', 'costs.', 'hi note', 'dr.', \"he'll\",\n       'answer question', 'believ', 'worth', 'pleas', \"don't know\", 'els',\n       'lab', '10', 'wrong.', 'run', 'harder', 'material.', 'wast time.',\n       'teacher,', \"doesn't explain\", 'let', 'nice guy', 'again.',\n       'veri bore', 'definit', 'quizz', 'chang', 'email', 'teaching.',\n       'them.', \"can't teach\", 'hi exam', 'minut', 'lecture.',\n       'thi teacher', 'pick', 'veri unclear', 'taken', 'avoid costs.',\n       'hard,', 'theori', 'book.', 'life.', \"wasn't\", 'suck', 'guy.',\n       ':(', \"worst professor i'v\", 'text', 'prof ever.', '....', 'group'],\n      dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively.\n\nimportance = np.argsort(np.asarray(f_neg.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'hi', 'thi', 'class', 'veri', 'wa', 'teach', 'worst',\n       'test', 'lectur', \"doesn't\", 'class.', 'like', 'hard', 'just',\n       \"don't\", 'professor', 'doe', 'know', 'grade', 'make', 'avoid',\n       'bore', 'teacher', \"he'\", 'question', 'onli', 'terribl', 'good',\n       'read', 'horribl', 'ha', 'learn', 'cours', 'prof', 'time', 'exam',\n       'understand', 'thi class', 'talk', 'becaus', 'help', 'realli',\n       'guy', 'him.', 'explain', 'bad', 'say', 'ask', 'ani', 'hi class',\n       \"can't\", 'thing', 'think', \"didn't\", 'way', 'book', 'thi guy',\n       'noth', 'need', 'materi', 'answer', 'wast', 'hi lectur', 'extrem',\n       'work', 'everi', 'want', 'expect', 'nice', 'teacher.', 'class,',\n       'assign', 'worst professor', 'all.', 'studi', 'professor.', 'did',\n       'note', 'anyth', 'use', \"i'v\", 'difficult', 'final', 'unclear',\n       'tell', 'drop', 'hi test', 'complet', 'worst prof', 'confus',\n       'dont', 'man', 'tri', 'absolut', 'got', 'time.', 'everyth', '-',\n       'easi', 'had.', 'rude', 'it.', 'write', 'fail', 'stay',\n       'worst teacher', '3', 'point', 'math', 'hard.', 'class wa', 'lot',\n       'mark', \"it'\", 'day', 'took', 'students.', 'clear', 'anoth',\n       'homework', 'tests.', 'useless', 'recommend', 'awful.', 'peopl',\n       'goe', 'littl', 'entir', 'unless', 'word', 'hi class.', 'someon',\n       'actual', 'come', 'textbook', 'hour', 'boring.', 'thi professor',\n       'lectures.', 'problem', 'dure', 'attend', 'better', 'half', 'went',\n       'veri hard', 'aw', 'care', 'pass', 'doesnt', 'prepar', 'everyon',\n       'person', 'paper', 'course.', 'ever.', 'look', '2', 'whi', 'mean',\n       'feel', 'year', 'arrog', 'midterm', 'prof.', 'ask question',\n       'thi class.', 'u', 'alway', \"i'm\", 'pretti', \"you'r\", 'hardest',\n       'speak', 'hate', 'imposs', 'old', 'befor', '&', 'end', 'thi cours',\n       'poor', 'subject', 'semest', 'said', 'told', 'guy,', 'someth',\n       'test.', 'basic', 'grade.', 'project', 'thi man', \"isn't\", 'mani',\n       'onlin', 'rambl', 'listen', 'idea', 'sit', 'base', 'differ',\n       'tough', 'incred', 'taught', 'essay', 'him,', 'away', 'hear',\n       'major', 'you.', \"won't\", 'terrible.', 'school', 'total', 'random',\n       'refus', 'concept', 'teach.', 'high', 'sure', 'exampl', 'slide',\n       'probabl', \"you'll\", 'anything.', 'veri difficult', 'week',\n       'hi teach', 'averag', 'questions.', 'follow', 'smart', 'save',\n       \"i'v had.\", 'possibl', 'avoid thi', 'right', 'life', 'wrote',\n       'far', 'boring,', 'bore lectur', 'topic', 'offic', 'knowledg',\n       'long', 'present', 'costs.', 'hi note', 'dr.', \"he'll\",\n       'answer question', 'believ', 'worth', 'pleas', \"don't know\", 'els',\n       'lab', '10', 'wrong.', 'run', 'harder', 'material.', 'wast time.',\n       'teacher,', \"doesn't explain\", 'let', 'nice guy', 'again.',\n       'veri bore', 'definit', 'quizz', 'chang', 'email', 'teaching.',\n       'them.', \"can't teach\", 'hi exam', 'minut', 'lecture.',\n       'thi teacher', 'pick', 'veri unclear', 'taken', 'avoid costs.',\n       'hard,', 'theori', 'book.', 'life.', \"wasn't\", 'suck', 'guy.',\n       ':(', \"worst professor i'v\", 'text', 'prof ever.', '....', 'group'],\n      dtype=object)"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/solution/notebook-solution.html#congratulations-on-making-it-to-the-end-where-to-from-here--we-can-feed-these-words-into-ben-schmidts-tool-to-derive-insights-by-field.--if-youre-interested-in-learning-more-about-web-scraping-take-our-courses-on-web-scraping-in-python--if-youre-intersted-in-diving-in-to-the-world-of-natural-language-processing-explore-our-skill-track.",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/solution/notebook-solution.html#congratulations-on-making-it-to-the-end-where-to-from-here--we-can-feed-these-words-into-ben-schmidts-tool-to-derive-insights-by-field.--if-youre-interested-in-learning-more-about-web-scraping-take-our-courses-on-web-scraping-in-python--if-youre-intersted-in-diving-in-to-the-world-of-natural-language-processing-explore-our-skill-track.",
    "title": "Do students describe professors differently based on gender?",
    "section": "Congratulations on making it to the end! ### Where to from here?- We can feed these words into Ben Schmidt’s tool to derive insights by field.- If you’re interested in learning more about web scraping, take our courses on Web Scraping in Python- If you’re intersted in diving in to the world of Natural Language Processing, explore our skill track.",
    "text": "Congratulations on making it to the end! ### Where to from here?- We can feed these words into Ben Schmidt’s tool to derive insights by field.- If you’re interested in learning more about web scraping, take our courses on Web Scraping in Python- If you’re intersted in diving in to the world of Natural Language Processing, explore our skill track."
  },
  {
    "objectID": "uncategorized_projects/breast_cancer_sns/seaborn_eda.html",
    "href": "uncategorized_projects/breast_cancer_sns/seaborn_eda.html",
    "title": "Tumor Diagnosis (Part 1): Exploratory Data Analysis",
    "section": "",
    "text": "About the Dataset:\nThe Breast Cancer Diagnostic data is available on the UCI Machine Learning Repository. This database is also available through the UW CS ftp server.\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”, Optimization Methods and Software 1, 1992, 23-34].\nAttribute Information:\n\nID number\nDiagnosis (M = malignant, B = benign) 3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\nradius (mean of distances from center to points on the perimeter)\ntexture (standard deviation of gray-scale values)\nperimeter\narea\nsmoothness (local variation in radius lengths)\ncompactness (perimeter^2 / area - 1.0)\nconcavity (severity of concave portions of the contour)\nconcave points (number of concave portions of the contour)\nsymmetry\nfractal dimension (“coastline approximation” - 1)\n\nThe mean, standard error and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\nAll feature values are recoded with four significant digits.\nMissing attribute values: none\nClass distribution: 357 benign, 212 malignant\n\n\nTask 1: Loading Libraries and Data\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns  # data visualization library  \nimport matplotlib.pyplot as plt\nimport time\n\n\nplt.style.use('ggplot')\nsns.set_style('white')\nsns.set_context('talk')\n\n\ndf = pd.read_csv('data/data.csv').drop(['Unnamed: 32'], axis = 1)\n\n\ndf.columns\n\nIndex(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n      dtype='object')\n\n\n\nExploratory Data Analysis\n\n\n\n\nTask 2: Separate Target from Features\n\nNote: If you are starting the notebook from this task, you can run cells from all the previous tasks in the kernel by going to the top menu and Kernel &gt; Restart and Run All ***\n\ntarget = df['diagnosis']\n\n\nfeatures = df.drop(['id', 'diagnosis'], axis = 1)\n\n\n\nTask 3: Plot Diagnosis Distributions\n\nNote: If you are starting the notebook from this task, you can run cells from all the previous tasks in the kernel by going to the top menu and Kernel &gt; Restart and Run All ***\n\n_ = target.value_counts().plot.bar()\n\n\n\n\n\n\n\n\n\nfeatures.describe()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\ncount\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n\n\nmean\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n0.062798\n...\n16.269190\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\n\n\nstd\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n0.007060\n...\n4.833242\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\n\n\nmin\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n0.049960\n...\n7.930000\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\n\n\n25%\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n0.057700\n...\n13.010000\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\n\n\n50%\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n0.061540\n...\n14.970000\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\n\n\n75%\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n0.066120\n...\n18.790000\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\n\n\nmax\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n0.097440\n...\n36.040000\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\n\n\n\n\n8 rows × 30 columns\n\n\n\n\nData Visualization\n\n\n\n\nTask 4: Visualizing Standardized Data with Seaborn\n\nNote: If you are starting the notebook from this task, you can run cells from all the previous tasks in the kernel by going to the top menu and Kernel &gt; Restart and Run All ***\n\nfeatures_std = features.subtract(features.mean()).div(features.std())\ndata = pd.concat([target, features_std], axis = 1)\n\n\ndata = pd.melt(data, id_vars = ['diagnosis'], var_name = 'features',\n               value_name = 'value')\n\n\nfeature_names = data.features.unique()\n\n\nfig = plt.figure(figsize = (11,7))\nax = sns.violinplot(\n    data = data.loc[data.features.isin(feature_names[:10])],\n    x = 'features',\n    y = 'value',\n    hue = 'diagnosis', \n    split = True,\n    inner = 'quart',\n    figure = fig,\n    zorder = 100\n) \nax.xaxis.set_tick_params(rotation = 90)\nax.set_frame_on(False)\nax.yaxis.set_visible(False)\nax.set_xlabel('')\nax.axhline(0, linestyle = '--', color = 'k', zorder = 1)\n_ = ax.annotate(xy = (-0.5,0.1), s = \"0\")\n_ = ax.set_title('Distribution of first 10 features conditioned on diagnosis')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nTask 5: Violin Plots and Box Plots\n\nfig = plt.figure(figsize = (11,7))\nax = sns.violinplot(\n    data = data.loc[data.features.isin(feature_names[10:20])],\n    x = 'features',\n    y = 'value',\n    hue = 'diagnosis', \n    split = True,\n    inner = 'quart',\n    figure = fig,\n    zorder = 100\n) \nax.xaxis.set_tick_params(rotation = 90)\nax.set_frame_on(False)\nax.yaxis.set_visible(False)\nax.set_xlabel('')\nax.axhline(0, linestyle = '--', color = 'k', zorder = 1)\n_ = ax.annotate(xy = (-0.5,0.1), s = \"0\")\n_ = ax.set_title('Distribution of features 10 to 20 conditioned on diagnosis')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize = (11,7))\nax = sns.violinplot(\n    data = data.loc[data.features.isin(feature_names[20:30])],\n    x = 'features',\n    y = 'value',\n    hue = 'diagnosis', \n    split = True,\n    inner = 'quart',\n    figure = fig,\n    zorder = 100\n) \nax.xaxis.set_tick_params(rotation = 90)\nax.set_frame_on(False)\nax.yaxis.set_visible(False)\nax.set_xlabel('')\nax.axhline(0, linestyle = '--', color = 'k', zorder = 1)\n_ = ax.annotate(xy = (-0.5,0.1), s = \"0\")\n_ = ax.set_title('Distribution of features 20 to 30 conditioned on diagnosis')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize = (11,9))\nax = sns.boxplot(\n    data = data.loc[data.features.isin(feature_names[:10])],\n    x = 'features',\n    y = 'value',\n    hue = 'diagnosis',\n) \nax.xaxis.set_tick_params(rotation = 90)\nax.set_frame_on(False)\nax.yaxis.set_visible(False)\nax.set_xlabel('')\nax.axhline(0, linestyle = '--', color = 'k', zorder = 1)\n_ = ax.annotate(xy = (-0.5,0.1), s = \"0\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nTask 6: Using Joint Plots for Feature Comparison\n\n_ = sns.jointplot(\n    features.loc[:, 'concavity_worst'],\n    features.loc[:, 'concave points_worst'],\n    kind = 'regg', scatter_kws = dict(alpha = 0.4, edgecolor = 'k')\n)\n\n\n\n\n\n\n\n\n\n\nTask 7: Observing the Distribution of Values and their Variance with Swarm Plots\n\nfig = plt.figure(figsize = (11,10))\nax = sns.swarmplot(\n    data = data.loc[data.features.isin(feature_names[:10])],\n    x = 'features',\n    y = 'value',\n    hue = 'diagnosis'\n) \nax.xaxis.set_tick_params(rotation = 90)\nax.set_frame_on(False)\nax.yaxis.set_visible(False)\nax.set_xlabel('')\nax.axhline(0, linestyle = '--', color = 'k', zorder = 1)\n_ = ax.set_title('Distribution of features 0 to 10 conditioned on diagnosis')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize = (11,10))\nax = sns.swarmplot(\n    data = data.loc[data.features.isin(feature_names[10:20])],\n    x = 'features',\n    y = 'value',\n    hue = 'diagnosis'\n) \nax.xaxis.set_tick_params(rotation = 90)\nax.set_frame_on(False)\nax.yaxis.set_visible(False)\nax.set_xlabel('')\nax.axhline(0, linestyle = '--', color = 'k', zorder = 1)\n_ = ax.set_title('Distribution of features 10 to 20 conditioned on diagnosis')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize = (11,10))\nax = sns.swarmplot(\n    data = data.loc[data.features.isin(feature_names[20:])],\n    x = 'features',\n    y = 'value',\n    hue = 'diagnosis'\n) \nax.xaxis.set_tick_params(rotation = 90)\nax.set_frame_on(False)\nax.yaxis.set_visible(False)\nax.set_xlabel('')\nax.axhline(0, linestyle = '--', color = 'k', zorder = 1)\n_ = ax.set_title('Distribution of features 20 to 30 conditioned on diagnosis')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nTask 8: Observing all Pair-wise Correlations\n\ncorr = features.corr()\n\n\nfig = plt.figure(figsize = (22,22))\nax = sns.heatmap(corr, square = True,\n                 annot = True, annot_kws = dict(fontsize = 12),\n                 linewidth = .5, fmt = '.1f')\nfig.tight_layout()",
    "crumbs": [
      "Projects",
      "Data Science",
      "EDA with seaborn"
    ]
  },
  {
    "objectID": "datacamp_courses/python/geospatial_data/markdown/chapter4.html",
    "href": "datacamp_courses/python/geospatial_data/markdown/chapter4.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "Joris Van den Bossche\nOpen source software developer and\n teacher, GeoPandas maintainer\nIPIS: International Peace Information Service\n\nImage: Connormah, CC BY-SA 3.0, from Wikimedia Commons\nIPIS: International Peace Information Service\n\nImage: G.A.O, public domain, from Wikimedia Commons\n\nMore analysis (re. social & security)\nReading files: geopandas.read_file(“path/to/file.geojson”)\nSupported formats:\n\nESRI Shapefile\nOne “file” consists of multiple files! ( .shp , .dbf, .shx , .prj , …)\nGeoJSON\nGeoPackage ( .gpkg )\n…\n\n(&) PostGIS databases!\nWriting a GeoDataFrame to a file with the to_file() method:\n# Writing a Shapefile file\ngeodataframe.to_file(“mydata.shp”, driver=‘ESRI Shapefile’)\n# Writing a GeoJSON file\ngeodataframe.to_file(“mydata.geojson”, driver=‘GeoJSON’)\n# Writing a GeoPackage file\ngeodataframe.to_file(“mydata.gpkg”, driver=‘GPKG’)\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nJoris Van den Bossche\nOpen source software developer and\n teacher, GeoPandas maintainer\nSpatial relationships:\n\nintersects\nwithin\ncontains\n\nJoin attributes based on spatial relation:\n\ngeopandas.sjoin Geometry operations:\nintersection\nunion\ndifference\n…\n\nCombine datasets based on geometry operation:\n\ngeopandas.overlay\n\nConvert a series of geometries to a single union geometry\n\n(GeoSeries of aeometries)\nConvert a series of geometries to a single union geometry:\n\n(GeoSeries of aeometries)\n\nafrica.unary_union\nl-inaln nonmotmu\npoint\n\n\nline.buffer(distance)\n\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nJoris Van den Bossche\nOpen source software developer and\n teacher, GeoPandas maintainer\n\nFor a single point ( cairo ):\narea = cairo.buffer(50000)\nrivers_within_area = rivers.intersection(area)\nprint(rivers_within_area.length.sum() / 1000)\n186.397219642\nSeries.apply() : call a function on each of the values of the Series\n\nfunction : the function being called on each value; the value is passed as the first argument\n**kwargs : additional arguments passed to the function\n\nFor a GeoSeries, the function is called as function(geom, **kwargs) for each geom in the GeoSeries\nThe function to apply:\ndef river_length(geom, rivers):\n    area = geom.buffer(50000)\n    rivers_within_area = rivers.intersection(area)\n    return rivers_within_area.length.sum() / 1000\nCall function on the single geometry:\nriver_length(cairo, rivers=rivers)\n186.3972196423455\nApplying on all cities:\ncities.geometry.apply(river_length, rivers=rivers)\n(\n\\[\\begin{array}{lr}0 & 0.000000 \\\\ 1 & 0.000000 \\\\ 2 & 106.072198\\end{array}\\]\n)\nApplying on all cities and assigning result to new column:\ncities['river_length'] = cities.geometry.apply(river_length, rivers=rivers)\ncities.head()\nname\ngeometry river_length\n0 Vatican City\nPOINT (1386304.6 5146502.5)\n0.000000\n1 San Marino\nPOINT (1385011.5 5455558.1)\n0.000000\n2\nVaduz\nPOINT (1059390.7 5963928.5) 106.072198\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nJoris Van den Bossche\nOpen source software developer and\n teacher, GeoPandas maintainer\nRaster\n\nImage source: QGIS documentation\n\n\nimport rasterio\n\n“Pythonic” bindings to GDAL\nReading and writing raster files\nProcessing tools (masking, reprojection, resampling, ..)\n\nhttps://rasterio.readthedocs.io/en/latest/\nimport rasterio\nsrc = rasterio.open(\"DEM_world.tif\")\nMetadata:\nsrc.count\nsrc.width, src.height\n1\n((4320,2160))\narray = src.read()\nStandard numpy array:\narray\narray([[[-4290, -4290, -4290, ..., -4290, -4290, -4290],\n    [-4278, -4278, -4278, ..., -4278, -4278, -4278],\n    [-4269, -4269, -4269, ..., -4269, -4269, -4269],\n    [ 2804, 2804, 2804, ..., 2804, 2804, 2804],\n    [ 2804, 2804, 2804, ..., 2804, 2804, 2804],\n    [ 2804, 2804, 2804, ..., 2804, 2804, 2804]]], dtype=int16)\nUsing the rasterio.plot.show() method:\nimport rasterio.plot\nrasterio.plot.show(src, cmap=‘terrain’)\n\n\nrasterstats : Summary statistics of geospatial raster datasets based on vector geometries (https://github.com/perrygeo/python-rasterstats)\n\nFor point vectors:\n\n[\n\\[\\begin{array}{r}\n\\text { rasterstats.point_query(geometries, \"path/to/raster\", } \\\\\n\\text { interpolation='nearest'|'bilinear') }\n\\end{array}\\]\n]\n\nFor polygon vectors:\n\nrasterstats.zonal_stats(geometries, “path/to/raster”,\n[ ]\nresult = rasterstats.zonal_stats(countries.geometry, \"DEM_gworld.tif\",\n    stats=['mean'])\ncountries['mean_elevation'] = pd.DataFrame(result)\ncountries.sort_values('mean_elevation', ascending=False).head()\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nInstructors\nJoris Van den Bossche & Dani Arribas-\n Bel\nMore on GeoPandas:\n\nGeoPandas docs and example gallery: https://geopandas.readthedocs.io/\nOther online sources, e.g.: https://automating-gis-processes.github.io/2018/\n\nLooking for spatial statistics? Check PySAL\nWorking with multi-dimensional gridded data? Check xarray\nWant to create interactive web maps? Check folium, ipyleaflet or geoviews\nMake matplotlib plots with projection support? Check cartopy\nWORKING WITH GEOSPATIAL DATA IN PYTHON"
  },
  {
    "objectID": "datacamp_courses/python/geospatial_data/markdown/chapter1.html",
    "href": "datacamp_courses/python/geospatial_data/markdown/chapter1.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "WORKING WITH GEOSPATIAL DATA IN PYTHON\nInstructors ()\nJoris Van den Bossche & Dani Arribas-\n Bel\nGeospatial data are data with location information\nGeospatial data are data with location information\n\n\n(12: 54) PM on Sunday, Oclober 2, 2016\nLunch Ride\nAdd a description\n\n(40.3 : 44: 53 )\nDistance Moving Time Elevation\n(173 w ,088 )\nEstimated Ai\nAvg Max\nSnow More\nSpeed 3.1 kmh 2:08:22\nView Flybys\n\n\n\nRaster\n\nVector\n“Discrete” representations that turn the world into:\nPoint(2, 10)\n“Discrete” representations that turn the world into:\n\nPoint(2, 10)\nLineString (([(1,2),(1,5), ]))\n“Discrete” representations that turn the world into:\n\nPoint(2, 10)\nLineString (([(1,2),(1,5), ]))\nPolygon([(13, 1), (14, 4), …])\nFeature consisting of multiple geometries: eg MultiPolygon\n\n\n\nVector features can have information associated that describe them: attributes\nTabular vector data:\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nJoris Van den Bossche\nOpen source software developer and\n teacher, GeoPandas maintainer\nrestaurants = pd.read_csv(“datasets/paris_restaurants.csv”)\nrestaurants.head()\n\nIn the rest of the course: - spatial file formats (Shapefiles, GeoJSON, GeoPackage, …) - GeoPandas: pandas dataframes with support for spatial data\nimport geopandas\ncountries = geopandas.read_file(“countries.geojson”)\ncountries.head()\ncountries.plot()\n\ncountries.head()\ntype(countries)\ngeopandas.geodataframe.GeoDataFrame\ncountries.head()\n\nA GeoDataFrame represents a tabular, geospatial vector dataset:\n\na ‘geometry’ column: that holds the geometry information\nother columns: attributes describe each of the geometries\n\ncountries.geometry\n0 POLYGON ((61.21 35.65, 62.23 35...\n1 MULTIPOLYGON (()23.90 -11.72, 2...\n175 POLYGON ((23.22 -17.52, 22.56 -...\n176 POLYGON ((29.43 -22.09, 28.79 -...\nName: geometry, Length: 176, dtype: object\ntype(countries.geometry)\ngeopandas.geoseries.GeoSeries\ncountries.geometry.area\n0 63.593500\n1 103.599439\n2 3.185163\n174 112.718524\n175 62.789498\n176 32.280371\nLength: 177, dtype: float64\nA GeoDataFrame is like a pandas DataFrame:\n\nall features of normal pandas DataFrames still work\n\nbut supercharged with spatial functionality:\n\nplot() method\ngeometry attribute (GeoSeries)\nspatial-specific attributes and methods (e.g. area )\n\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nJoris Van den Bossche\nOpen source software developer and teacher, GeoPandas maintainer\n\ncountries.head()\ncountries[‘continent’] == ‘Africa’\n(\n\\[\\begin{array}{ll}0 & \\text { False } \\\\ 1 & \\text { True } \\\\ & \\cdots \\\\ 175 & \\text { True } \\\\ 176 & \\text { True } \\\\ \\text { Name: continent, Length: 177, dtype: bool }\\end{array}\\]\n)\ncountries_africa = countries[countries[‘continent’] == ‘Africa’]\ncountries_africa.plot()\n\ncountries.plot()\n\ncountries.plot(color=“red”)\n\ncountries.plot(column='gdp_per_cap')\n\nfig, ax = plt.subplots(figsize=(12, 6))\ncountries.plot(ax=ax)\ncities.plot(ax=ax, color='red', markersize=10)\nax.set_axis_off()\n\nWORKING WITH GEOSPATIAL DATA IN PYTHON"
  },
  {
    "objectID": "datacamp_courses/dc_courses_python.html",
    "href": "datacamp_courses/dc_courses_python.html",
    "title": "Datacamp",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "stats_with_R/2_inference/2_inference.html",
    "href": "stats_with_R/2_inference/2_inference.html",
    "title": "Statistical inference using GSS data",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(statsr)\n\n\n\nload(\"gss.Rdata\")",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "2 Inference",
      "Statistical inference using GSS data"
    ]
  },
  {
    "objectID": "stats_with_R/2_inference/2_inference.html#load-packages",
    "href": "stats_with_R/2_inference/2_inference.html#load-packages",
    "title": "Statistical inference using GSS data",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(statsr)",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "2 Inference",
      "Statistical inference using GSS data"
    ]
  },
  {
    "objectID": "stats_with_R/2_inference/2_inference.html#load-data",
    "href": "stats_with_R/2_inference/2_inference.html#load-data",
    "title": "Statistical inference using GSS data",
    "section": "",
    "text": "load(\"gss.Rdata\")",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "2 Inference",
      "Statistical inference using GSS data"
    ]
  },
  {
    "objectID": "stats_with_R/2_inference/2_inference.html#intro",
    "href": "stats_with_R/2_inference/2_inference.html#intro",
    "title": "Statistical inference using GSS data",
    "section": "Intro",
    "text": "Intro\nThe General Social Survey is a US-nationwide sociological survey, nowadays conducted biannually since 1972 to “gather data to monitor and explain trends, changes, and constants in attitudes, behaviors, and attributes as well as examine the structure, development, and functioning of society in general as well as the role of various sub-groups.” (ref. GSS on Wikipedia).",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "2 Inference",
      "Statistical inference using GSS data"
    ]
  },
  {
    "objectID": "stats_with_R/2_inference/2_inference.html#sampling-strategy-and-generalizability-of-findings",
    "href": "stats_with_R/2_inference/2_inference.html#sampling-strategy-and-generalizability-of-findings",
    "title": "Statistical inference using GSS data",
    "section": "Sampling strategy and generalizability of findings",
    "text": "Sampling strategy and generalizability of findings\nThe sample population includes English-speaking (and since 2006, Spanish speaking) Americans over 18 living in “non-institutional arrangements” (excluding people living in college dorms, military quarters, nursing homes and long-term care facilities) within the US (GSS Introduction, p. 8).\nParts of the surveys of 1972-1974 and parts of the 1975 and 1976 surveys used block quota sampling, while all remaining surveys employed full probability sampling (GSS Introduction, p. 8; For detailed Sampling Design see Appendix A).\nIn quota sampling, a number of subjects with a certain characteristic which is to be sampled is defined beforehand. Sampling continues until the quota is fulfilled. It is a non-probabiblistic sampling strategy, therefore the results are not generalizable to the population at large.\nIn contrast, the data acquired with full probability sampling can be generalized from the sample to the population from which the sample was taken (1978 - today).\n\nConvenience bias\nA putative bias in surveys is sampling only subjects that can easily be reached, which in this sampling framework means e.g. people being at home. Measures have been taken to reduce this bias as described in (Appendix A, p. 2).\n\n\nNon-responder bias\nIf too many people do not respond to a survey, its generalizability might be limited (OpenIntro Statistics, p. 24). In the GSS, temporary non-respondents are sub-sampled and the cases in this sample where then pursued further to ensure a representative sample (Appendix A, p. 19 ff.)\n\n\nAge bias\nAs pointed out above, included subjects are older than 18. In addition, the condition to live in a household excludes a substantial part of those 18-24 (9.4% in 1980) and of those 75 and older (11.4% in 1980), see Appendix A, p. 1.\nTaken together, conclusions from this data are not applicable neither to children nor teenagers, while they can be generalized to the sampled population, if interpreted carefully for the age groups 18-24 and &gt;75, as these results may be skewed.\n\n\nCausality\nThe GSS is an observational study (no random assignment) and therefore does not allow for any causal conclusions on any observed associations between variables.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "2 Inference",
      "Statistical inference using GSS data"
    ]
  },
  {
    "objectID": "stats_with_R/2_inference/2_inference.html#workstatus-over-time",
    "href": "stats_with_R/2_inference/2_inference.html#workstatus-over-time",
    "title": "Statistical inference using GSS data",
    "section": "Workstatus over time",
    "text": "Workstatus over time\ndf = select(gss, year, sex, wrkstat) %&gt;% \n      na.omit()  %&gt;% group_by(year,sex) %&gt;% \n      count(wrkstat) %&gt;% mutate(p=100*n/sum(n))\ndf %&gt;%\n  ggplot(aes(x=year, y=p, fill=wrkstat)) + \n    geom_area() + facet_grid(~sex) + \n    geom_hline(yintercept = 50, alpha=0.5, linetype='dashed') +\n    geom_hline(yintercept = 25, alpha=0.5, linetype='dashed') +\n    geom_hline(yintercept = 75, alpha=0.5, linetype='dashed') +\n    geom_vline(xintercept = 1980, alpha = 0.5, linetype='dashed') +\n    geom_vline(xintercept = 2010, alpha = 0.5, linetype='dashed')\n\nThe plot shows the distribution of the workstatus over time conditioned by gender. Changes for men seem to be less dynamic than for women over this period of time.\ndf %&gt;% na.omit() %&gt;% \n  filter(sex == \"Female\" & wrkstat == \"Keeping House\" & (year==1980 | year==2010)) %&gt;%\n  summarise(n = n,p=round(p,2))\n## # A tibble: 2 x 4\n## # Groups:   year [2]\n##    year sex        n     p\n##   &lt;int&gt; &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n## 1  1980 Female   349  42.2\n## 2  2010 Female   204  17.7\nTable 1 shows the proportions of women which named “Keeping House” as their workstatus in the samples of 1980 and 2010, respectively. There is an observeddifference of -24.48 %.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "2 Inference",
      "Statistical inference using GSS data"
    ]
  },
  {
    "objectID": "stats_with_R/2_inference/2_inference.html#hypotheses",
    "href": "stats_with_R/2_inference/2_inference.html#hypotheses",
    "title": "Statistical inference using GSS data",
    "section": "Hypotheses",
    "text": "Hypotheses\nWe need to state two hypotheses \\(H_0\\) (null hypothesis) and \\(H_1\\) (alternative hypothesis). The null hypothesis states that the data do not provide convincing evidence that there is a significant difference between the samples with respect to the chosen test statistic. \\(H_0\\) instead states that there is convincing evidence for a significant difference. An important principle in inferential statistcs is to assume the null hypothesis to be true, until finding evidence (e.g. using an hypothesis test) that it is not.\n\nApplied within the framework of the research question, \\(H_0\\) would be :\nThere is no significant difference between the proportion of women keeping the house in the 1980 and 2010 samples. Any observed difference is likely due to chance.\n\\(H_0: p_1 - p_2 = 0\\)\n\n\\(H_1\\):\nThe data provide convincing evidence, that proportions of women keeping the house in the 1980 and 2010 samples are significantly differnt. The observed difference is very unlikely to be due to chance.\n\\(H_1: p_1 - p_2 \\neq 0\\)\n\nAs can be seen in the equations for \\(H_0\\) and \\(H_1\\), the parameter of interest in this case is the difference of the proportions of women keeping the house in the 1980 and 2010 samples.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "2 Inference",
      "Statistical inference using GSS data"
    ]
  },
  {
    "objectID": "stats_with_R/2_inference/2_inference.html#variable-preparation",
    "href": "stats_with_R/2_inference/2_inference.html#variable-preparation",
    "title": "Statistical inference using GSS data",
    "section": "Variable preparation",
    "text": "Variable preparation\nTo facilitate hypothesis testing and to be able to construct a confidence interval, the variable wrkstat with originally 8 levels will be transformed to a new variable at_home with two levels.\ntest &lt;- select(gss, year, sex, wrkstat) %&gt;% #select all important variables\n  na.omit() %&gt;%\n  filter((year == 1980 | year == 2010) & sex == \"Female\") %&gt;% #filter for female participants in 2010 and 1980\n  mutate(at_home=wrkstat==\"Keeping House\",\n         year=as.factor(year))",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "2 Inference",
      "Statistical inference using GSS data"
    ]
  },
  {
    "objectID": "stats_with_R/2_inference/2_inference.html#confidence-interval",
    "href": "stats_with_R/2_inference/2_inference.html#confidence-interval",
    "title": "Statistical inference using GSS data",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\nMethod description\nIn contrast to a point estimate of a statistic (mean, median, sample proportion), a confidence interval proposes a range of plausible values for the chosen statistic for a chosen confidence level (\\(1 - \\alpha\\))\nThe generic formula for a confidence interval:\n\\(CI_{1-\\alpha} = \\left[point\\ estimate \\pm z \\times SE\\right]\\)\nhas to be adapted for the parameter of interest (difference of two proportions) and the chosen confidence level (\\(95\\%\\)):\n\\(\\hat{p_1}-\\hat{p_2} \\pm z \\times \\sqrt{\\frac{p_1(1-p_1)}{n_1} +\n\\frac{p_2(1-p_2)}{n_2}}\\)\nThe formula for the standard error is based on the normal approximation and holds true, only if the Central Limit Theorem (CLT) can be invoked. Therefore certain conditions have to be checked, see below (conditions satisfied for construction of a confidence interval).\n\n\nConditions Check\nIndependence As for each turn of the GSS a new random sample is created, the observations for 1980 and 2010 can be assumed to be independent.\nSuccess-Failure-Condition\ntest %&gt;% \n  group_by(year) %&gt;% \n  count(at_home) %&gt;% \n  mutate(p=round(n/sum(n),2)) %&gt;%\n  kable()\n\n\n\nyear\nat_home\nn\np\n\n\n\n\n1980\nFALSE\n478\n0.58\n\n\n1980\nTRUE\n349\n0.42\n\n\n2010\nFALSE\n947\n0.82\n\n\n2010\nTRUE\n204\n0.18\n\n\n\nThe success-failure-condition requires at least 10 successes and at least 10 failures for each group in order to construct a confidence interval a confidence interval based on the CLT. As can be seen from columns n of the table above, this conditions is satified.\n\n\nCalculations\ninference(y = at_home, \n          x = year, \n          data = test, \n          statistic = \"proportion\", \n          type = \"ci\", \n          alternative='twosided', \n          method = \"theoretical\", \n          success = TRUE)\n## Response variable: categorical (2 levels, success: TRUE)\n## Explanatory variable: categorical (2 levels) \n## n_1980 = 827, p_hat_1980 = 0.422\n## n_2010 = 1151, p_hat_2010 = 0.1772\n## 95% CI (1980 - 2010): (0.2045 , 0.285)\n\n\n\nInterpretation\nThe plot gives the distribution of women keeping the house in the 1980 and 2010 samples. The text output gives a 95% confidence interval for difference on the two proportions from 20.45% - 28.50%. This interval has a 95% probability of containing the true population parameter. Note that the interval does not include zero, which indicates, that the two proportions are indeed significantly different.\n\n\nHypothesis test\n\nMethod description\nA hypothesis test for a difference of two proportions based on the CLT constructs the distribution of this difference under the assumptions that the null hypothesis is true. It then gives the probability of finding an equal or more extreme outcome with respect to the difference observed form the two samples. This probability is also called p-value. If the p-value is smaller than some chosen significance level (very often 5%), we can conclude that the data provides convincing evidence for rejection of the null hypothesis. We instead accept the alternative hypothesis.\n\n\nConditions Check\nAs for the construction of a condifidence interval, certain conditions have to be met in order to perform a hypothesis test on the difference of two proportions.\nIndependence As for each turn of the GSS a new random sample is created, the observations for 1980 and 2010 can be assumed to be independent.\nSuccess-Failure-Condition Checking the success-failure-condition for a hypothesis test on the difference of two proportions is slightly different compared to constructing a confidence interval. Instead of checking it invidually for both samples with \\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) as estimates for the population proportions, we will use the pooled proportion\n\\[\\hat{p}_{pooled} = \\frac{\\hat{p_1}n_1+\\hat{p_2}n_2}{n_1+n_2}\\]\nfor checking the success-failure-condition and to calculate the standard error. This provides the best estimate for the proportion of successes assuming that the null hypothesis is true.\ntest %&gt;% \n  group_by(year) %&gt;% \n  count(at_home) %&gt;% \n  mutate(p=round(n/sum(n),2)) %&gt;%\n  kable()\n\n\n\nyear\nat_home\nn\np\n\n\n\n\n1980\nFALSE\n478\n0.58\n\n\n1980\nTRUE\n349\n0.42\n\n\n2010\nFALSE\n947\n0.82\n\n\n2010\nTRUE\n204\n0.18\n\n\n\nIn our case:\n\\(\\hat{p}_{pooled} = \\frac{(349+204)}{349+478+204+947} = 0.2796\\)\n1980\n\\(\\hat{p}_{pooled} \\times n_{success} = 0.2796 \\times 349 = 97\\)\n\\((1-\\hat{p}_{pooled}) \\times n_{failure} = 0.7204 \\times 478 =\n344\\)\n2010\n\\(\\hat{p}_{pooled} \\times n_{success} = 0.2796 \\times 204 = 57\\)\n\\((1-\\hat{p}_{pooled}) \\times n_{failure} = 0.7204 \\times 947 =\n682\\)\nThe success-failure condition is satisfied since all values are at least 10.\n\n\nCalculations\ninference(y = at_home, \n          x = year, \n          data = test, \n          statistic = \"proportion\", \n          type = \"ht\", \n          alternative='twosided', \n          method = \"theoretical\",\n          null = 0,\n          success = TRUE)\n## Response variable: categorical (2 levels, success: TRUE)\n## Explanatory variable: categorical (2 levels) \n## n_1980 = 827, p_hat_1980 = 0.422\n## n_2010 = 1151, p_hat_2010 = 0.1772\n## H0: p_1980 =  p_2010\n## HA: p_1980 != p_2010\n## z = 11.9644\n## p_value = &lt; 0.0001\n\n\n\nInterpretation\nThe plots give the distribution of women keeping the house in the 1980 and 2010 samples and the distribution for the difference of the two proportions under the null hypothesis. The red line indicates the observed difference, the area under the curve in the red shaded areas give the probability of finding an equal or more extreme outcome (= p-value). Obviously, this area is very small. This is reflected in the text output which gives p-value &lt; 0.0001, we therefore can reject the null hypothesis: the data provide convincing evidence that the observed difference in the proportion of women keeping the house in 1980 and 2010 is statistically significant at the 5% significance level.\n\n\n\nConclusion\nBoth methods for inference on the difference of the proportions of women keeping the house in 1980 and 2010 agree. The 95% CI does not include zero, the p-value returned by the hypothesis test allows for rejection of the null hypothesis. We can therefore conclude that the data provides convincing evidence that the observed difference of -24.48% is not due to chance (at the 5% significance level).",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "2 Inference",
      "Statistical inference using GSS data"
    ]
  },
  {
    "objectID": "stats_with_R/4_bayesian/4_bayesian.html",
    "href": "stats_with_R/4_bayesian/4_bayesian.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(statsr)\nlibrary(BAS)\nlibrary(broom)\nlibrary(MASS)\nlibrary(ggpubr)\nlibrary(gridExtra)\nlibrary(knitr)\n\n\n\nMake sure your data and R Markdown files are in the same directory. When loaded your data file will be called movies. Delete this note when before you submit your work.\nload(\"movies.Rdata\")\nmovies = as.data.frame(movies)\n\n\n\n\n\nThe dataset contains data on 651 randomly chosen movies produced and release before 2016. These are observational data, therefore no causality can be inferred from an analysis. As a random sample was obtained, the findings can be generalized to the population at large. The target variable audience_score contains the audience score from Rotten Tomatoes. Rotten Tomatoes is a metacritic site for movies, i.e. it aggregates movie reviews over a variety of sources (e.g. IMDB and movie critic articles). Therefore, one important task will be to check for multicollinearity between explanatory variables and the response variable (see below). * * *\n\n\n\nFeature Films\nmovies = movies %&gt;%\n  dplyr::mutate(feature_film = ifelse(title_type == 'Feature Film', 'yes', 'no')) %&gt;%\n  dplyr::mutate(feature_film = as.factor(feature_film))\nGenre\nmovies = movies %&gt;%\n  dplyr::mutate(drama = ifelse(genre == 'Drama', 'yes', 'no')) %&gt;%\n  dplyr::mutate(drama = as.factor(drama))\nMPAA rating\nmovies = movies %&gt;%\n  dplyr::mutate(mpaa_rating_R = ifelse(mpaa_rating == 'R', 'yes', 'no')) %&gt;%\n  dplyr::mutate(mpaa_rating_R = as.factor(mpaa_rating_R))\nOscar season\noscar_months = c(11, 10, 12)\nmovies = movies %&gt;%\n  dplyr::mutate(oscar_season = ifelse(thtr_rel_month %in% oscar_months, 'yes', 'no')) %&gt;%\n  dplyr::mutate(oscar_season = as.factor(oscar_season))\nSummer\nsummer_months = c(5,6,7,8)\nmovies = movies %&gt;%\n  dplyr::mutate(summer_season = ifelse(thtr_rel_month %in% summer_months, 'yes', 'no')) %&gt;%\n  dplyr::mutate(summer_season = as.factor(summer_season))\n\n\n\n\nFeature Films\nffp &lt;- ggplot(movies, aes(y=audience_score, x=feature_film, fill=feature_film))  + \n  geom_violin() +\n  stat_summary(fun=median, geom=\"point\", shape=23, size=8, fill='white') + \n  theme(legend.position=\"none\") +\n  ggtitle('Feature Film')\nDrama\ndp &lt;- ggplot(movies, aes(y=audience_score, x=drama, fill=drama))  + \n  geom_violin() +\n  stat_summary(fun=median, geom=\"point\", shape=23, size=8, fill='white') +\n  theme(legend.position=\"none\") +\n  ggtitle('Drama')\nMPAA\nmpaap &lt;- ggplot(movies, aes(y=audience_score, x=mpaa_rating_R, fill=mpaa_rating_R))  + \n  geom_violin() +\n  stat_summary(fun=median, geom=\"point\", shape=23, size=8, fill='white') + \n  theme(legend.position=\"none\") +\n  ggtitle('MPAA Rating')\nOscar\noscarp &lt;- ggplot(movies, aes(y=audience_score, x=oscar_season, fill=oscar_season))  + \n  geom_violin() +\n  stat_summary(fun=median, geom=\"point\", shape=23, size=8, fill='white') + \n  theme(legend.position=\"none\") +\n  ggtitle('Oscar Season')\nSummer\nsummerp &lt;-ggplot(movies, aes(y=audience_score, x=summer_season, fill=summer_season)) + \n  geom_violin() +\n  stat_summary(fun=median, geom=\"point\", shape=23, size=8, fill='white') + \n  theme(legend.position=\"none\") +\n  ggtitle('Summer season')\ngrid.arrange(ffp, mpaap, oscarp, summerp, dp, ncol=4)\n\nThe figure above shows violinplots for of the audience_score variable with respect to all of the newly constructed variables with the median of each distribution indicated by a white diamond. The distribution of the audience score of movies with MPAA rating R has an additional high density area in the low range of audience scores compared to movies with a different rating. The variables oscar_season and summer_season are basically just the inverse of one another, it should be enough to include one of the variables (if at all).\nFor the last variable drama, distributions of the audience score are quite different, indicating a higher audience score attributed to films of the drama genre.\n\n\n\n\n\n\nThe dataset contains variables that are highly correlated with the response variable audience_score, as can be seen from the pairplot and the corresponding correlation matrix.\npairs(movies[,c('audience_score', 'critics_score', 'imdb_rating')])\n\ntidy(cor(movies[,c('audience_score', 'critics_score', 'imdb_rating')]))\n## Warning: 'tidy.matrix' is deprecated.\n## See help(\"Deprecated\")\n\n## # A tibble: 3 x 4\n##   .rownames      audience_score critics_score imdb_rating\n##   &lt;chr&gt;                   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n## 1 audience_score          1             0.704       0.865\n## 2 critics_score           0.704         1           0.765\n## 3 imdb_rating             0.865         0.765       1\nWe can learn from this plot that (maybe or maybe not surprisingly) critics and audience tend to agree in their ratings, as well as that there is a high correlation between the two measures from Rotten Tomatoes and the IMDB-rating. Information on how the scores are obtained can be found on the websites of Rotten Tomatoes and IMDB, respectively. Essentially, IMDB ratings and Audience Scores from Rotten Tomatoes are measuring the same thing - whether the movies was liked by the people who have seen it. So the high correlation is not surprising and there might as well be redundancy concerning the information in these two variables. It is probably a good idea to think about the original purpose of modeling this particular problem. Sure, if we assume that we model for modeling’s sake, we should include either IMDB rating or critics_score to predict audience_score (I actually ran a model selection using the stepAIC function and the final model retained included only imdb_rating - note very useful in my opinion). If we think about this from a possible scenario where we attempt to predict whether a movie will be liked by the audience based on some of its characteristics such as genre, director etc. In this case, we would avoid including imdb_rating as this information would not be available before the release of the movie. The situation could be judged a little different for critics_score. To me this seems like information that could be available beforehand - but maybe not at a timepoint where you could possible use information from modeling to change the movie in order to improve the outcome.\n\n\n\nAs there is no context given within the project description, modeling will be performed from the perspective of someone who wants to predict the popularity of a movie before its release, relying solely on characteristics of the movie known beforehand.\n# variables conatining information that is not available before a movies's release is commented out\nmovies_red &lt;- na.omit(movies[,c(\n  'feature_film',\n  'drama',\n  #'genre',\n  'runtime',\n  #'mpaa_rating_R',\n  'thtr_rel_year',\n  'oscar_season',\n  #'summer_season',\n  #'imdb_rating',\n  #'imdb_num_votes',\n  #'critics_score',\n  #'best_pic_nom',\n  #'best_pic_win',\n  'best_actor_win',\n  'best_actress_win',\n  'best_dir_win',\n  #'top200_box',\n  'audience_score'\n)])\n\n\n\nbma_red &lt;- bas.lm(audience_score ~ .,\n                  data = movies_red,\n                  prior = 'ZS-null',\n                  modelprior = uniform(),\n                  )\n\n\n\ncoef_bma &lt;- coef(bma_red)\nc &lt;- data.frame(coef_bma$probne0, coef_bma$postmean, coef_bma$postsd)\ncolnames(c) &lt;- c('post P(B != 0)', 'post. mean', 'post. SD')\nrownames(c) &lt;- coef_bma$namesx\nkable(round(c,3))\n\n\n\n\npost P(B != 0)\npost. mean\npost. SD\n\n\n\n\nIntercept\n1.000\n62.348\n0.725\n\n\nfeature_filmyes\n1.000\n-25.961\n2.641\n\n\ndramayes\n1.000\n7.635\n1.536\n\n\nruntime\n0.999\n0.179\n0.040\n\n\nthtr_rel_year\n0.447\n-0.065\n0.085\n\n\noscar_seasonyes\n0.073\n0.011\n0.445\n\n\nbest_actor_winyes\n0.079\n-0.068\n0.647\n\n\nbest_actress_winyes\n0.076\n-0.054\n0.680\n\n\nbest_dir_winyes\n0.322\n1.850\n3.183\n\n\n\npar(mfcol=c(3,4))\nplot(coef_bma, ask=FALSE, subset = 2:(length(coef_bma)-2))\n\nThe table and plot above give the marginal posterior inclusion probabilities and distributions for the explanatory variables. As can be seen feature_film, drama and runtime have inclusion probabilities of (or very close to) 1. Interpreting the posterior means of these parameters gives on average a ~26 point reduction for feature films, 7.6 points increase for dramas and 0.179 additional point per additional minute of runtime. The posterior means of the remaining variables could be interpreted in a similar manner, but are skipped due to their low inclusion probabilites.\n\n\n\nkable(round(summary(bma_red),3))\n\n\n\n\nP(B != 0 | Y)\nmodel 1\nmodel 2\nmodel 3\nmodel 4\nmodel 5\n\n\n\n\nIntercept\n1.000\n1.000\n1.000\n1.000\n1.000\n1.000\n\n\nfeature_filmyes\n1.000\n1.000\n1.000\n1.000\n1.000\n1.000\n\n\ndramayes\n1.000\n1.000\n1.000\n1.000\n1.000\n1.000\n\n\nruntime\n0.999\n1.000\n1.000\n1.000\n1.000\n1.000\n\n\nthtr_rel_year\n0.447\n0.000\n1.000\n0.000\n1.000\n0.000\n\n\noscar_seasonyes\n0.073\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nbest_actor_winyes\n0.079\n0.000\n0.000\n0.000\n0.000\n1.000\n\n\nbest_actress_winyes\n0.076\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nbest_dir_winyes\n0.322\n0.000\n0.000\n1.000\n1.000\n0.000\n\n\nBF\nNA\n1.000\n0.846\n0.508\n0.349\n0.077\n\n\nPostProbs\nNA\n0.292\n0.247\n0.148\n0.102\n0.022\n\n\nR2\nNA\n0.167\n0.173\n0.172\n0.177\n0.167\n\n\ndim\nNA\n4.000\n5.000\n5.000\n6.000\n5.000\n\n\nlogmarg\nNA\n49.175\n49.008\n48.498\n48.122\n46.610\n\n\n\nThe top 5 models all include feature_film, drama and runtime. Taken together these five models have posterior probability of 81.1% (vs. 5*(1/256) under the uniform model-prior) and all have and \\(R^2\\) of about 0.17, therefore accounting for about 17% of the variability in the data.\n\n\n\ny_pred = predict(bma_red,movies_red)$fit\nres = bma_red$Y - y_pred\nggplot() + geom_point(aes(x=movies_red$audience_score, y=y_pred)) + \n  stat_smooth(aes(x=movies_red$audience_score, y=y_pred),method = \"lm\", se = FALSE) +\n  ylim(ymin = 40, ymax=115) +\n  labs(x = \"True audience score\", y = \"Predicted audience score\")\n## `geom_smooth()` using formula 'y ~ x'\n\nPlotting true audience scores vs. the scores predicted by the BMA approach shows a positive correlation between the two. However, as points are widely spread around the regression line there is still room for improvement (for perfect prediction all points should lie on the line).\nggplot() + geom_point(aes(x = y_pred, y = res)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Fitted values\", y = \"Residuals\")\n\nThe residuals from the BMA predictions are scattered around 0. Some possible outliers and heteroscedascity might necessitate further modeling diagnostics and/or transformations of the response variable.\n\n\n\n\n\n\n\n\n\n\nRotten Tomates\nIMDB\nWikipedia\n\npred = data.frame(runtime = 142,\n                  audience_score = 86,\n                  critics_score = 52,\n                  thtr_rel_year = 2019,\n                  mpaa_rating_R = 'yes',\n                  best_pic_nom = 'no',\n                  summer_season = 'no',\n                  oscar_season = 'yes',\n                  feature_film = 'yes',\n                  drama = 'no',\n                  imdb_rating = 6.7,\n                  imdb_num_votes = 316581,\n                  best_pic_win = 'no',\n                  best_actor_win = 'no',\n                  best_actress_win = 'no',\n                  best_dir_win = 'no')\np &lt;- predict(bma_red, pred, se.fit = TRUE)\nThe audience score on Rotten Tomatoes for Star Wars: The Rise of Skywalker is\n\\[86\\]\nwhile the BMA model predicted\n\\[61\\]\nwith a 95% credible interval for the prediction\nset.seed(42)\ni &lt;- confint(p, nsim=100000)\n\\[25 - 98\\]\n\n\n\n\n\n\nPredicting audience_score of a movie only from information that would be available before its release yields models with rather low predictive power (\\(R^2\\) around 0.17, see above). If the cast or the director won an oscar is likely to have no effect, and if so, it is small compared to the other variables chosen for this modeling approach as discussed above. Therefore, a famous cast or director is no garanty for a popular movie. That the prediction for Star Wars: The Rise of Skywalker is off by 25 points illustrates the weaknesses of the model and the fact that the predictors chosen in this work are not sufficient to predict audience popularity before a movie’s release. The available data provide mainly technical information on the movies but leave out the most important parts - plot, mise-en-scène, music (composer of the film score could be easily incorporated in the dataset), just to give a few. In future work it might be interesting to look for ways to represent these in a form making these information accessible for modeling"
  },
  {
    "objectID": "stats_with_R/4_bayesian/4_bayesian.html#setup",
    "href": "stats_with_R/4_bayesian/4_bayesian.html#setup",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(statsr)\nlibrary(BAS)\nlibrary(broom)\nlibrary(MASS)\nlibrary(ggpubr)\nlibrary(gridExtra)\nlibrary(knitr)\n\n\n\nMake sure your data and R Markdown files are in the same directory. When loaded your data file will be called movies. Delete this note when before you submit your work.\nload(\"movies.Rdata\")\nmovies = as.data.frame(movies)"
  },
  {
    "objectID": "stats_with_R/4_bayesian/4_bayesian.html#part-1-data",
    "href": "stats_with_R/4_bayesian/4_bayesian.html#part-1-data",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "The dataset contains data on 651 randomly chosen movies produced and release before 2016. These are observational data, therefore no causality can be inferred from an analysis. As a random sample was obtained, the findings can be generalized to the population at large. The target variable audience_score contains the audience score from Rotten Tomatoes. Rotten Tomatoes is a metacritic site for movies, i.e. it aggregates movie reviews over a variety of sources (e.g. IMDB and movie critic articles). Therefore, one important task will be to check for multicollinearity between explanatory variables and the response variable (see below). * * *"
  },
  {
    "objectID": "stats_with_R/4_bayesian/4_bayesian.html#part-2-data-manipulation",
    "href": "stats_with_R/4_bayesian/4_bayesian.html#part-2-data-manipulation",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "Feature Films\nmovies = movies %&gt;%\n  dplyr::mutate(feature_film = ifelse(title_type == 'Feature Film', 'yes', 'no')) %&gt;%\n  dplyr::mutate(feature_film = as.factor(feature_film))\nGenre\nmovies = movies %&gt;%\n  dplyr::mutate(drama = ifelse(genre == 'Drama', 'yes', 'no')) %&gt;%\n  dplyr::mutate(drama = as.factor(drama))\nMPAA rating\nmovies = movies %&gt;%\n  dplyr::mutate(mpaa_rating_R = ifelse(mpaa_rating == 'R', 'yes', 'no')) %&gt;%\n  dplyr::mutate(mpaa_rating_R = as.factor(mpaa_rating_R))\nOscar season\noscar_months = c(11, 10, 12)\nmovies = movies %&gt;%\n  dplyr::mutate(oscar_season = ifelse(thtr_rel_month %in% oscar_months, 'yes', 'no')) %&gt;%\n  dplyr::mutate(oscar_season = as.factor(oscar_season))\nSummer\nsummer_months = c(5,6,7,8)\nmovies = movies %&gt;%\n  dplyr::mutate(summer_season = ifelse(thtr_rel_month %in% summer_months, 'yes', 'no')) %&gt;%\n  dplyr::mutate(summer_season = as.factor(summer_season))"
  },
  {
    "objectID": "stats_with_R/4_bayesian/4_bayesian.html#part-3-exploratory-data-analysis",
    "href": "stats_with_R/4_bayesian/4_bayesian.html#part-3-exploratory-data-analysis",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "Feature Films\nffp &lt;- ggplot(movies, aes(y=audience_score, x=feature_film, fill=feature_film))  + \n  geom_violin() +\n  stat_summary(fun=median, geom=\"point\", shape=23, size=8, fill='white') + \n  theme(legend.position=\"none\") +\n  ggtitle('Feature Film')\nDrama\ndp &lt;- ggplot(movies, aes(y=audience_score, x=drama, fill=drama))  + \n  geom_violin() +\n  stat_summary(fun=median, geom=\"point\", shape=23, size=8, fill='white') +\n  theme(legend.position=\"none\") +\n  ggtitle('Drama')\nMPAA\nmpaap &lt;- ggplot(movies, aes(y=audience_score, x=mpaa_rating_R, fill=mpaa_rating_R))  + \n  geom_violin() +\n  stat_summary(fun=median, geom=\"point\", shape=23, size=8, fill='white') + \n  theme(legend.position=\"none\") +\n  ggtitle('MPAA Rating')\nOscar\noscarp &lt;- ggplot(movies, aes(y=audience_score, x=oscar_season, fill=oscar_season))  + \n  geom_violin() +\n  stat_summary(fun=median, geom=\"point\", shape=23, size=8, fill='white') + \n  theme(legend.position=\"none\") +\n  ggtitle('Oscar Season')\nSummer\nsummerp &lt;-ggplot(movies, aes(y=audience_score, x=summer_season, fill=summer_season)) + \n  geom_violin() +\n  stat_summary(fun=median, geom=\"point\", shape=23, size=8, fill='white') + \n  theme(legend.position=\"none\") +\n  ggtitle('Summer season')\ngrid.arrange(ffp, mpaap, oscarp, summerp, dp, ncol=4)\n\nThe figure above shows violinplots for of the audience_score variable with respect to all of the newly constructed variables with the median of each distribution indicated by a white diamond. The distribution of the audience score of movies with MPAA rating R has an additional high density area in the low range of audience scores compared to movies with a different rating. The variables oscar_season and summer_season are basically just the inverse of one another, it should be enough to include one of the variables (if at all).\nFor the last variable drama, distributions of the audience score are quite different, indicating a higher audience score attributed to films of the drama genre."
  },
  {
    "objectID": "stats_with_R/4_bayesian/4_bayesian.html#part-4-modeling",
    "href": "stats_with_R/4_bayesian/4_bayesian.html#part-4-modeling",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "The dataset contains variables that are highly correlated with the response variable audience_score, as can be seen from the pairplot and the corresponding correlation matrix.\npairs(movies[,c('audience_score', 'critics_score', 'imdb_rating')])\n\ntidy(cor(movies[,c('audience_score', 'critics_score', 'imdb_rating')]))\n## Warning: 'tidy.matrix' is deprecated.\n## See help(\"Deprecated\")\n\n## # A tibble: 3 x 4\n##   .rownames      audience_score critics_score imdb_rating\n##   &lt;chr&gt;                   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n## 1 audience_score          1             0.704       0.865\n## 2 critics_score           0.704         1           0.765\n## 3 imdb_rating             0.865         0.765       1\nWe can learn from this plot that (maybe or maybe not surprisingly) critics and audience tend to agree in their ratings, as well as that there is a high correlation between the two measures from Rotten Tomatoes and the IMDB-rating. Information on how the scores are obtained can be found on the websites of Rotten Tomatoes and IMDB, respectively. Essentially, IMDB ratings and Audience Scores from Rotten Tomatoes are measuring the same thing - whether the movies was liked by the people who have seen it. So the high correlation is not surprising and there might as well be redundancy concerning the information in these two variables. It is probably a good idea to think about the original purpose of modeling this particular problem. Sure, if we assume that we model for modeling’s sake, we should include either IMDB rating or critics_score to predict audience_score (I actually ran a model selection using the stepAIC function and the final model retained included only imdb_rating - note very useful in my opinion). If we think about this from a possible scenario where we attempt to predict whether a movie will be liked by the audience based on some of its characteristics such as genre, director etc. In this case, we would avoid including imdb_rating as this information would not be available before the release of the movie. The situation could be judged a little different for critics_score. To me this seems like information that could be available beforehand - but maybe not at a timepoint where you could possible use information from modeling to change the movie in order to improve the outcome.\n\n\n\nAs there is no context given within the project description, modeling will be performed from the perspective of someone who wants to predict the popularity of a movie before its release, relying solely on characteristics of the movie known beforehand.\n# variables conatining information that is not available before a movies's release is commented out\nmovies_red &lt;- na.omit(movies[,c(\n  'feature_film',\n  'drama',\n  #'genre',\n  'runtime',\n  #'mpaa_rating_R',\n  'thtr_rel_year',\n  'oscar_season',\n  #'summer_season',\n  #'imdb_rating',\n  #'imdb_num_votes',\n  #'critics_score',\n  #'best_pic_nom',\n  #'best_pic_win',\n  'best_actor_win',\n  'best_actress_win',\n  'best_dir_win',\n  #'top200_box',\n  'audience_score'\n)])\n\n\n\nbma_red &lt;- bas.lm(audience_score ~ .,\n                  data = movies_red,\n                  prior = 'ZS-null',\n                  modelprior = uniform(),\n                  )\n\n\n\ncoef_bma &lt;- coef(bma_red)\nc &lt;- data.frame(coef_bma$probne0, coef_bma$postmean, coef_bma$postsd)\ncolnames(c) &lt;- c('post P(B != 0)', 'post. mean', 'post. SD')\nrownames(c) &lt;- coef_bma$namesx\nkable(round(c,3))\n\n\n\n\npost P(B != 0)\npost. mean\npost. SD\n\n\n\n\nIntercept\n1.000\n62.348\n0.725\n\n\nfeature_filmyes\n1.000\n-25.961\n2.641\n\n\ndramayes\n1.000\n7.635\n1.536\n\n\nruntime\n0.999\n0.179\n0.040\n\n\nthtr_rel_year\n0.447\n-0.065\n0.085\n\n\noscar_seasonyes\n0.073\n0.011\n0.445\n\n\nbest_actor_winyes\n0.079\n-0.068\n0.647\n\n\nbest_actress_winyes\n0.076\n-0.054\n0.680\n\n\nbest_dir_winyes\n0.322\n1.850\n3.183\n\n\n\npar(mfcol=c(3,4))\nplot(coef_bma, ask=FALSE, subset = 2:(length(coef_bma)-2))\n\nThe table and plot above give the marginal posterior inclusion probabilities and distributions for the explanatory variables. As can be seen feature_film, drama and runtime have inclusion probabilities of (or very close to) 1. Interpreting the posterior means of these parameters gives on average a ~26 point reduction for feature films, 7.6 points increase for dramas and 0.179 additional point per additional minute of runtime. The posterior means of the remaining variables could be interpreted in a similar manner, but are skipped due to their low inclusion probabilites.\n\n\n\nkable(round(summary(bma_red),3))\n\n\n\n\nP(B != 0 | Y)\nmodel 1\nmodel 2\nmodel 3\nmodel 4\nmodel 5\n\n\n\n\nIntercept\n1.000\n1.000\n1.000\n1.000\n1.000\n1.000\n\n\nfeature_filmyes\n1.000\n1.000\n1.000\n1.000\n1.000\n1.000\n\n\ndramayes\n1.000\n1.000\n1.000\n1.000\n1.000\n1.000\n\n\nruntime\n0.999\n1.000\n1.000\n1.000\n1.000\n1.000\n\n\nthtr_rel_year\n0.447\n0.000\n1.000\n0.000\n1.000\n0.000\n\n\noscar_seasonyes\n0.073\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nbest_actor_winyes\n0.079\n0.000\n0.000\n0.000\n0.000\n1.000\n\n\nbest_actress_winyes\n0.076\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nbest_dir_winyes\n0.322\n0.000\n0.000\n1.000\n1.000\n0.000\n\n\nBF\nNA\n1.000\n0.846\n0.508\n0.349\n0.077\n\n\nPostProbs\nNA\n0.292\n0.247\n0.148\n0.102\n0.022\n\n\nR2\nNA\n0.167\n0.173\n0.172\n0.177\n0.167\n\n\ndim\nNA\n4.000\n5.000\n5.000\n6.000\n5.000\n\n\nlogmarg\nNA\n49.175\n49.008\n48.498\n48.122\n46.610\n\n\n\nThe top 5 models all include feature_film, drama and runtime. Taken together these five models have posterior probability of 81.1% (vs. 5*(1/256) under the uniform model-prior) and all have and \\(R^2\\) of about 0.17, therefore accounting for about 17% of the variability in the data.\n\n\n\ny_pred = predict(bma_red,movies_red)$fit\nres = bma_red$Y - y_pred\nggplot() + geom_point(aes(x=movies_red$audience_score, y=y_pred)) + \n  stat_smooth(aes(x=movies_red$audience_score, y=y_pred),method = \"lm\", se = FALSE) +\n  ylim(ymin = 40, ymax=115) +\n  labs(x = \"True audience score\", y = \"Predicted audience score\")\n## `geom_smooth()` using formula 'y ~ x'\n\nPlotting true audience scores vs. the scores predicted by the BMA approach shows a positive correlation between the two. However, as points are widely spread around the regression line there is still room for improvement (for perfect prediction all points should lie on the line).\nggplot() + geom_point(aes(x = y_pred, y = res)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Fitted values\", y = \"Residuals\")\n\nThe residuals from the BMA predictions are scattered around 0. Some possible outliers and heteroscedascity might necessitate further modeling diagnostics and/or transformations of the response variable."
  },
  {
    "objectID": "stats_with_R/4_bayesian/4_bayesian.html#part-5-prediction",
    "href": "stats_with_R/4_bayesian/4_bayesian.html#part-5-prediction",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "Rotten Tomates\nIMDB\nWikipedia\n\npred = data.frame(runtime = 142,\n                  audience_score = 86,\n                  critics_score = 52,\n                  thtr_rel_year = 2019,\n                  mpaa_rating_R = 'yes',\n                  best_pic_nom = 'no',\n                  summer_season = 'no',\n                  oscar_season = 'yes',\n                  feature_film = 'yes',\n                  drama = 'no',\n                  imdb_rating = 6.7,\n                  imdb_num_votes = 316581,\n                  best_pic_win = 'no',\n                  best_actor_win = 'no',\n                  best_actress_win = 'no',\n                  best_dir_win = 'no')\np &lt;- predict(bma_red, pred, se.fit = TRUE)\nThe audience score on Rotten Tomatoes for Star Wars: The Rise of Skywalker is\n\\[86\\]\nwhile the BMA model predicted\n\\[61\\]\nwith a 95% credible interval for the prediction\nset.seed(42)\ni &lt;- confint(p, nsim=100000)\n\\[25 - 98\\]"
  },
  {
    "objectID": "stats_with_R/4_bayesian/4_bayesian.html#part-6-conclusion",
    "href": "stats_with_R/4_bayesian/4_bayesian.html#part-6-conclusion",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "Predicting audience_score of a movie only from information that would be available before its release yields models with rather low predictive power (\\(R^2\\) around 0.17, see above). If the cast or the director won an oscar is likely to have no effect, and if so, it is small compared to the other variables chosen for this modeling approach as discussed above. Therefore, a famous cast or director is no garanty for a popular movie. That the prediction for Star Wars: The Rise of Skywalker is off by 25 points illustrates the weaknesses of the model and the fact that the predictors chosen in this work are not sufficient to predict audience popularity before a movie’s release. The available data provide mainly technical information on the movies but leave out the most important parts - plot, mise-en-scène, music (composer of the film score could be easily incorporated in the dataset), just to give a few. In future work it might be interesting to look for ways to represent these in a form making these information accessible for modeling"
  },
  {
    "objectID": "duke_stats.html",
    "href": "duke_stats.html",
    "title": "Data analysis with R",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\n\n\nTitle\n\n\n\n\n\n\nApr 2, 2020\n\n\n\n\n\nExploring the BRFSS data\n\n\n\n\nApr 13, 2020\n\n\n\n\n\nStatistical inference using GSS data\n\n\n\n\nMay 15, 2020\n\n\n\n\n\nModeling and prediction of movie scores\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html",
    "href": "stats_with_R/3_regression/3_regression.html",
    "title": "Modeling and prediction of movie scores",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(statsr)\nlibrary(knitr)\nlibrary(broom)\nlibrary(leaps)\nlibrary(caret)\nlibrary(olsrr)\nlibrary(sjPlot)\n\n\n\nload(\"movies.Rdata\")\nmovies = as.data.frame(movies)",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#load-packages",
    "href": "stats_with_R/3_regression/3_regression.html#load-packages",
    "title": "Modeling and prediction of movie scores",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(statsr)\nlibrary(knitr)\nlibrary(broom)\nlibrary(leaps)\nlibrary(caret)\nlibrary(olsrr)\nlibrary(sjPlot)",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#load-data",
    "href": "stats_with_R/3_regression/3_regression.html#load-data",
    "title": "Modeling and prediction of movie scores",
    "section": "",
    "text": "load(\"movies.Rdata\")\nmovies = as.data.frame(movies)",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#some-functions",
    "href": "stats_with_R/3_regression/3_regression.html#some-functions",
    "title": "Modeling and prediction of movie scores",
    "section": "Some functions",
    "text": "Some functions\n#Check whether an actor is in among the main 5 actors\nget_actor &lt;- function(r, actor){\n  actor_cols = c('actor1', 'actor2', 'actor3', 'actor4', 'actor5')\n  s = as.data.frame(t(r), col.names = names(r), stringsAsFactors=FALSE)\n  if (actor %in% stack(s, actor_cols)$values){\n    return('yes')\n  } else {\n    return('no')\n  }\n}\nget_director &lt;- function(r){\n  s = as.data.frame(t(r), col.names = names(r), stringsAsFactors=FALSE)\n  if (s['director'] %in% directors) {\n    return(s['director'])\n  } else {\n    return('other')\n  }\n}\n\nget_studio &lt;- function(r){\n  s = as.data.frame(t(r), col.names = names(r), stringsAsFactors=FALSE)\n  if (s['studio'] %in% studios) {\n    return(s['studio'])\n  } else {\n    return('other')\n  }\n}",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#choice-of-target-variable",
    "href": "stats_with_R/3_regression/3_regression.html#choice-of-target-variable",
    "title": "Modeling and prediction of movie scores",
    "section": "Choice of target variable",
    "text": "Choice of target variable\nThe dataset contains numeric variables that are highly correlated with each other, as can be seen from the pairplot and the corresponding correlation matrix.\npairs(movies[,c('audience_score', 'critics_score', 'imdb_rating')])\n\ntidy(cor(movies[,c('audience_score', 'critics_score', 'imdb_rating')]))\n## Warning: 'tidy.matrix' is deprecated.\n## See help(\"Deprecated\")\n\n## # A tibble: 3 x 4\n##   .rownames      audience_score critics_score imdb_rating\n##   &lt;chr&gt;                   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n## 1 audience_score          1             0.704       0.865\n## 2 critics_score           0.704         1           0.765\n## 3 imdb_rating             0.865         0.765       1\nWe can learn from this plot that (maybe or maybe not surprisingly) critics and audience tend to agree in their ratings, as well as that there is a high correlation between the two measures from Rotten Tomatoes and the IMDB-rating. Information on how the scores are obtained can be found on the websites of Rotten Tomatoes and IMDB, respectively. Essentially, IMDB ratings and Audience Scores from Rotten Tomatoes are measuring the same thing - whether the movies was liked by the people who have seen it. So the high correlation is not surprising and there might as well be redundancy concerning the information in these two variables.\nFor this project I will choose audience_score as the response variable and drop critics_score and imdb_rating due to their high collinearity and redundant information.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#actors",
    "href": "stats_with_R/3_regression/3_regression.html#actors",
    "title": "Modeling and prediction of movie scores",
    "section": "Actors",
    "text": "Actors\nIn order to explore associations between the audience score and actors, we need to do some data manipulation. The columns containing the names of the five main actors are stacked, missing values are removed and finally appearances per actor are counted and stored.\nactor_cols = c('actor1', 'actor2', 'actor3', 'actor4', 'actor5')\n\n#count appearances of actors over the whole dataset\ndf = stack(movies[,actor_cols]) %&gt;%\n  na.omit() %&gt;% \n  group_by(values) %&gt;% \n  count()\n\n#get actors that appear in 5 or more movies\nactors = df[df$n&gt;=5,]$values\n\nactors\n##  [1] \"Al Pacino\"         \"Alan Alda\"         \"Andy Garcia\"      \n##  [4] \"Ben Affleck\"       \"Charlize Theron\"   \"Christian Bale\"   \n##  [7] \"Colin Firth\"       \"Dan Aykroyd\"       \"Danny Glover\"     \n## [10] \"Dennis Quaid\"      \"Diane Keaton\"      \"Eddie Murphy\"     \n## [13] \"Gene Hackman\"      \"Helen Hunt\"        \"Jamie Foxx\"       \n## [16] \"Jeremy Northam\"    \"John Cusack\"       \"John Hurt\"        \n## [19] \"John Travolta\"     \"Juliette Lewis\"    \"Keanu Reeves\"     \n## [22] \"Mel Gibson\"        \"Michael Caine\"     \"Minnie Driver\"    \n## [25] \"Naomi Watts\"       \"Nick Nolte\"        \"Nicolas Cage\"     \n## [28] \"Richard Gere\"      \"Robert De Niro\"    \"Samuel L. Jackson\"\n## [31] \"Sean Penn\"         \"Steve Martin\"      \"Steve Zahn\"       \n## [34] \"Tom Cruise\"        \"Tommy Lee Jones\"   \"Val Kilmer\"       \n## [37] \"Woody Harrelson\"\nThere are 37 actors that appear in 5 or more movies within this dataset. For visualisation we will extract, the mean and the standard deviation of audience_score for each of these actors.\neda_actors = data.frame()\ncolnames &lt;- c('min', 'mean', 'max', 'n', 'sd','se')\n\n#get summary statisics for actors with 5 or more appearances\nfor (a in actors) {\n  temp &lt;- movies %&gt;% \n    na.omit() %&gt;% \n    group_by(as.factor(apply(na.omit(movies), 1, get_actor, actor=a))) %&gt;% \n    summarise(min=min(audience_score),\n            mean=mean(audience_score),\n            max=max(audience_score),\n            n=n(),\n            sd = sd(audience_score),\n            se=sd(audience_score)/n())\n  colnames(temp) &lt;- c('isin', 'min', 'mean', 'max', 'n', 'sd','se')\n  eda_actors &lt;- rbind(eda_actors, as.data.frame(temp[temp$isin == 'yes',c('min', 'mean', 'max', 'n', 'sd','se')]))\n}\nrownames(eda_actors) &lt;- actors\neda_actors['actor'] = actors\nggplot(eda_actors[1:10,], aes(x=actor, y=mean, fill=actor, color=actor)) + \n  geom_point(size=5) + \n  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd)) + \n  geom_hline(yintercept = mean(movies$audience_score),\n             linetype='dashed',\n             show.legend = TRUE) +\n  theme(axis.text.x = element_text(color='white')) + \n  ylab('audience score') +\n  xlab('')\n\nThe plot above shows the \\(mean \\pm SD\\) of audience_score conditioned on the first 10 actors with 5 or more appearances, the dashed line indicates the mean of audience_score over all movies. For most of the actors, the movies they appear in have a rather large spread of audience score. Only the movies Christian Bale plays in have a consistent high rating different from the overall mean, which could prove useful for modeling.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#directors",
    "href": "stats_with_R/3_regression/3_regression.html#directors",
    "title": "Modeling and prediction of movie scores",
    "section": "Directors",
    "text": "Directors\nThe same analysis can be conducted for directors that appear four or more time in the dataset.\nreg = movies %&gt;%\n  group_by(director) %&gt;%\n  na.omit() %&gt;%\n  count()\n\ndirectors = reg[reg$n&gt;=4,]$director\ndirectors\n## [1] \"James Ivory\"     \"Martin Scorsese\" \"Oliver Stone\"    \"Renny Harlin\"   \n## [5] \"Woody Allen\"\n#refactoring the director variable\nmovies$director = as.factor(unlist(apply(movies, 1, get_director)))\n\neda_dir &lt;- movies %&gt;% \n  group_by(director) %&gt;%\n  summarise(min=min(audience_score),\n            mean=mean(audience_score),\n            max=max(audience_score),\n            n=n(),\n            sd = sd(audience_score),\n            se=sd(audience_score)/n())\ndir_p = eda_dir[eda_dir$director != 'other',] %&gt;%\n  ggplot(aes(x=director, y=mean, color = director, fill=director)) + \n  geom_point(size=5, shape=23, fill='white') + \n  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd)) + \n  geom_hline(yintercept = mean(movies$audience_score),\n             linetype='dashed',\n             show.legend = TRUE) +\n  theme(axis.text.x = element_text(color='white')) + \n  ylab('audience score') +\n  xlab('')\ndir_p + geom_jitter(aes(director, audience_score), data = movies[movies$director!='other',],\n                    position = position_jitter(0), color='black', show.legend = FALSE)\n\nData are \\(mean \\pm SD\\) with individual films indicated as black dots. As above the dashed line indicates the overall mean. From the five directors with 4 or more movies in the dataset, only Woody Allen’s movies have a consistently high score.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#studios",
    "href": "stats_with_R/3_regression/3_regression.html#studios",
    "title": "Modeling and prediction of movie scores",
    "section": "Studios",
    "text": "Studios\nstu = movies %&gt;%\n  group_by(studio) %&gt;%\n  na.omit() %&gt;%\n  count()\n## Warning: Factor `studio` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\nstudios = stu[stu$n &gt;= 10,]$studio\nstudios\n##  [1] 20th Century Fox                        \n##  [2] IFC Films                               \n##  [3] MCA Universal Home Video                \n##  [4] MGM                                     \n##  [5] Miramax Films                           \n##  [6] New Line Cinema                         \n##  [7] Paramount Home Video                    \n##  [8] Paramount Pictures                      \n##  [9] Sony Pictures                           \n## [10] Sony Pictures Home Entertainment        \n## [11] Twentieth Century Fox Home Entertainment\n## [12] Universal Pictures                      \n## [13] Warner Bros. Pictures                   \n## [14] Warner Home Video                       \n## 211 Levels: 20th Century Fox ... Zeitgeist Films\n#refactor the studio variable\nmovies$studio = as.factor(unlist(apply(movies, 1, get_studio)))\n\neda_studio &lt;- movies %&gt;% \n  group_by(studio) %&gt;%\n  summarise(min=min(audience_score),\n            mean=mean(audience_score),\n            max=max(audience_score),\n            n=n(),\n            sd = sd(audience_score),\n            se=sd(audience_score)/n())\nggplot() + \n  geom_jitter(aes(x=studio, y = audience_score),\n              data = movies[movies$studio!='other',],\n              position = position_jitter(0.1)) + \n  stat_summary(aes(x=studio, y = audience_score, color=studio), data = movies[movies$studio!='other',], \n               fun=mean, geom='point', shape=23, size=5, fill='white') + \n  geom_errorbar(aes(x=studio, ymin = mean-sd, ymax=mean+sd, color=studio),\n                data = eda_studio[eda_studio$studio!='other',]) + \n  geom_hline(yintercept = mean(movies$audience_score),\n             linetype='dashed',\n             show.legend = TRUE) +\n  theme(axis.text.x = element_text(color='white')) + \n  ylab('audience score') +\n  xlab('')\n\nData are \\(mean \\pm SD\\) with individual films indicated as black dots. When conditioned on the studio, there seem to be quite some differences in audience_score. Most of these major studios have a mean audience rating below the overall average.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#genre",
    "href": "stats_with_R/3_regression/3_regression.html#genre",
    "title": "Modeling and prediction of movie scores",
    "section": "Genre",
    "text": "Genre\nmovies %&gt;%\n  na.omit %&gt;%\n  group_by(genre) %&gt;%\n  ggplot(aes(genre, audience_score, fill=genre)) +\n  geom_violin() +\n  stat_summary(fun=mean, shape=23, size=2, fill='white') + \n  geom_hline(yintercept = mean(movies$audience_score),\n             linetype='dashed',\n             show.legend = TRUE) +\n  theme(axis.text.x = element_text(color='white')) + \n  ylab('audience score') +\n  xlab('')\n## Warning: Removed 11 rows containing missing values (geom_segment).\n\nThis distribution plot reveals that Documentaries and Muscial movies have scores above the overall average, while the inverse is the case for Horror movies.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#variable-encoding",
    "href": "stats_with_R/3_regression/3_regression.html#variable-encoding",
    "title": "Modeling and prediction of movie scores",
    "section": "Variable encoding",
    "text": "Variable encoding\nI will use One-Hot-Encoding for the categorical variables that will be included in the model. This allows to easily remove certain factor levels without releveling.\n\nActors\nfor (a in actors) {\n  movies[gsub(' ', '.', a)] = as.factor(apply(movies, 1, get_actor, actor = a))\n}\nmovies_red = movies[append(gsub(' ', '.', actors), c('director',\n                                                     'runtime',\n                                                     'genre',\n                                                     'studio',\n                                                     'best_dir_win',\n                                                     'best_actor_win',\n                                                     'best_actress_win',\n                                                     'audience_score'))] %&gt;% \n  na.omit() #reduced dataset with all variables for the full model\n\nmovies_red &lt;- within(movies_red,\n                     director &lt;- relevel(director, ref='other'))\n\nmovies_red &lt;- within(movies_red,\n                     studio &lt;- relevel(studio, ref='other'))\n\n\nDirectors and Studios\ndmy &lt;- dummyVars(\"~.\", data = data.frame(director = movies_red$director,\n                                         studio = movies_red$studio,\n                                         genre = movies_red$genre))\ndir_dummy &lt;- data.frame(predict(dmy,\n                                newdata = data.frame(director = movies_red$director,\n                                                     studio = movies_red$studio,\n                                                     genre = movies_red$genre))) %&gt;% \n  lapply(as.factor) %&gt;% \n  as.data.frame()\n\nmovies_red = data.frame(\n  append(\n    select(\n      movies_red, -c('director', 'studio', 'genre') #dropping the original columns\n      ), \n      select(\n        dir_dummy, -c('director.other', 'studio.other', 'genre.Other') #drop the reference levels of studio and director\n             )\n    )\n)",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#full-model",
    "href": "stats_with_R/3_regression/3_regression.html#full-model",
    "title": "Modeling and prediction of movie scores",
    "section": "Full model",
    "text": "Full model\nmd = lm(audience_score ~ ., data = movies_red)\nsmd = summary(md)\n#print(smd, digits=1)\ntab_model(md, p.style = 'asterisk')\n\n\nTable: Full Model coefficients, confidence intervals and p-values\n\n\n\n\n\n \n\n\naudience_score\n\n\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\n\n\n(Intercept)\n\n\n47.14 ***\n\n\n34.36 – 59.92\n\n\n\n\nAl.Pacino [yes]\n\n\n-2.70 \n\n\n-18.37 – 12.98\n\n\n\n\nAlan.Alda [yes]\n\n\n-22.24 *\n\n\n-39.36 – -5.12\n\n\n\n\nAndy.Garcia [yes]\n\n\n-3.87 \n\n\n-19.84 – 12.10\n\n\n\n\nBen.Affleck [yes]\n\n\n-18.90 *\n\n\n-35.43 – -2.36\n\n\n\n\nCharlize.Theron [yes]\n\n\n-12.62 \n\n\n-29.35 – 4.11\n\n\n\n\nChristian.Bale [yes]\n\n\n11.41 \n\n\n-5.52 – 28.34\n\n\n\n\nColin.Firth [yes]\n\n\n-4.21 \n\n\n-21.16 – 12.75\n\n\n\n\nDan.Aykroyd [yes]\n\n\n-3.80 \n\n\n-19.22 – 11.63\n\n\n\n\nDanny.Glover [yes]\n\n\n1.45 \n\n\n-15.62 – 18.53\n\n\n\n\nDennis.Quaid [yes]\n\n\n-8.03 \n\n\n-24.12 – 8.05\n\n\n\n\nDiane.Keaton [yes]\n\n\n-0.70 \n\n\n-16.88 – 15.47\n\n\n\n\nEddie.Murphy [yes]\n\n\n-6.10 \n\n\n-22.53 – 10.32\n\n\n\n\nGene.Hackman [yes]\n\n\n5.71 \n\n\n-10.14 – 21.55\n\n\n\n\nHelen.Hunt [yes]\n\n\n-2.74 \n\n\n-17.62 – 12.14\n\n\n\n\nJamie.Foxx [yes]\n\n\n-4.38 \n\n\n-19.01 – 10.26\n\n\n\n\nJeremy.Northam [yes]\n\n\n-3.62 \n\n\n-20.93 – 13.69\n\n\n\n\nJohn.Cusack [yes]\n\n\n8.69 \n\n\n-6.29 – 23.67\n\n\n\n\nJohn.Hurt [yes]\n\n\n-0.34 \n\n\n-14.89 – 14.21\n\n\n\n\nJohn.Travolta [yes]\n\n\n-2.27 \n\n\n-16.35 – 11.81\n\n\n\n\nJuliette.Lewis [yes]\n\n\n-1.40 \n\n\n-18.11 – 15.31\n\n\n\n\nKeanu.Reeves [yes]\n\n\n14.43 \n\n\n-0.65 – 29.51\n\n\n\n\nMel.Gibson [yes]\n\n\n12.99 \n\n\n-3.43 – 29.41\n\n\n\n\nMichael.Caine [yes]\n\n\n16.00 \n\n\n-0.91 – 32.91\n\n\n\n\nMinnie.Driver [yes]\n\n\n17.42 *\n\n\n0.38 – 34.46\n\n\n\n\nNaomi.Watts [yes]\n\n\n-3.02 \n\n\n-20.66 – 14.61\n\n\n\n\nNick.Nolte [yes]\n\n\n-7.15 \n\n\n-21.76 – 7.45\n\n\n\n\nNicolas.Cage [yes]\n\n\n-15.12 *\n\n\n-30.10 – -0.14\n\n\n\n\nRichard.Gere [yes]\n\n\n-15.77 *\n\n\n-28.81 – -2.73\n\n\n\n\nRobert.De.Niro [yes]\n\n\n-7.27 \n\n\n-26.57 – 12.03\n\n\n\n\nSamuel.L..Jackson [yes]\n\n\n11.84 \n\n\n-1.51 – 25.18\n\n\n\n\nSean.Penn [yes]\n\n\n0.06 \n\n\n-19.08 – 19.20\n\n\n\n\nSteve.Martin [yes]\n\n\n7.11 \n\n\n-9.56 – 23.78\n\n\n\n\nSteve.Zahn [yes]\n\n\n-1.36 \n\n\n-18.15 – 15.43\n\n\n\n\nTom.Cruise [yes]\n\n\n3.52 \n\n\n-13.77 – 20.80\n\n\n\n\nTommy.Lee.Jones [yes]\n\n\n-9.80 \n\n\n-26.86 – 7.25\n\n\n\n\nVal.Kilmer [yes]\n\n\n13.49 \n\n\n-1.82 – 28.80\n\n\n\n\nWoody.Harrelson [yes]\n\n\n-1.73 \n\n\n-16.93 – 13.48\n\n\n\n\nruntime\n\n\n0.16 ***\n\n\n0.08 – 0.25\n\n\n\n\nbest_dir_win [yes]\n\n\n2.61 \n\n\n-4.44 – 9.66\n\n\n\n\nbest_actor_win [yes]\n\n\n-0.46 \n\n\n-5.93 – 5.01\n\n\n\n\nbest_actress_win [yes]\n\n\n1.26 \n\n\n-4.27 – 6.80\n\n\n\n\ndirector.James.Ivory [1]\n\n\n-14.45 \n\n\n-32.88 – 3.98\n\n\n\n\ndirector.Martin.Scorsese[1]\n\n\n4.94 \n\n\n-17.04 – 26.93\n\n\n\n\ndirector.Oliver.Stone [1]\n\n\n3.72 \n\n\n-17.14 – 24.58\n\n\n\n\ndirector.Renny.Harlin [1]\n\n\n-9.06 \n\n\n-27.02 – 8.90\n\n\n\n\ndirector.Woody.Allen [1]\n\n\n26.49 *\n\n\n6.11 – 46.86\n\n\n\n\nstudio.20th.Century.Fox[1]\n\n\n-3.92 \n\n\n-12.79 – 4.95\n\n\n\n\nstudio.IFC.Films [1]\n\n\n-2.05 \n\n\n-12.02 – 7.92\n\n\n\n\nstudio.MCA.Universal.Home.Video[1]\n\n\n2.52 \n\n\n-8.04 – 13.08\n\n\n\n\nstudio.MGM [1]\n\n\n-13.60 **\n\n\n-22.74 – -4.46\n\n\n\n\nstudio.Miramax.Films [1]\n\n\n3.48 \n\n\n-6.20 – 13.16\n\n\n\n\nstudio.New.Line.Cinema[1]\n\n\n-12.76 *\n\n\n-24.33 – -1.18\n\n\n\n\nstudio.Paramount.Home.Video[1]\n\n\n-12.71 *\n\n\n-23.49 – -1.94\n\n\n\n\nstudio.Paramount.Pictures[1]\n\n\n2.88 \n\n\n-3.83 – 9.59\n\n\n\n\nstudio.Sony.Pictures [1]\n\n\n-1.74 \n\n\n-13.65 – 10.18\n\n\n\n\nstudio.Sony.Pictures.Home.Entertainment[1]\n\n\n-6.71 \n\n\n-14.18 – 0.75\n\n\n\n\nstudio.Twentieth.Century.Fox.Home.Entertainment[1]\n\n\n-2.63 \n\n\n-12.51 – 7.25\n\n\n\n\nstudio.Universal.Pictures[1]\n\n\n9.52 *\n\n\n1.43 – 17.62\n\n\n\n\nstudio.Warner.Bros..Pictures[1]\n\n\n-1.55 \n\n\n-8.72 – 5.62\n\n\n\n\nstudio.Warner.Home.Video[1]\n\n\n-3.12 \n\n\n-12.02 – 5.78\n\n\n\n\ngenre.Action…Adventure[1]\n\n\n-9.71 \n\n\n-20.01 – 0.59\n\n\n\n\ngenre.Animation [1]\n\n\n2.57 \n\n\n-12.62 – 17.75\n\n\n\n\ngenre.Art.House…International[1]\n\n\n0.98 \n\n\n-12.31 – 14.27\n\n\n\n\ngenre.Comedy [1]\n\n\n-11.15 *\n\n\n-21.37 – -0.92\n\n\n\n\ngenre.Documentary [1]\n\n\n20.07 ***\n\n\n9.54 – 30.60\n\n\n\n\ngenre.Drama [1]\n\n\n1.43 \n\n\n-8.03 – 10.88\n\n\n\n\ngenre.Horror [1]\n\n\n-13.46 *\n\n\n-25.43 – -1.49\n\n\n\n\ngenre.Musical…Performing.Arts[1]\n\n\n16.12 *\n\n\n2.44 – 29.80\n\n\n\n\ngenre.Mystery…Suspense[1]\n\n\n-9.02 \n\n\n-19.58 – 1.53\n\n\n\n\ngenre.Science.Fiction…Fantasy[1]\n\n\n-13.33 \n\n\n-28.35 – 1.69\n\n\n\n\nObservations\n\n\n650\n\n\n\n\nR2 / R2 adjusted\n\n\n0.324 / 0.242\n\n\n\n\n\n\np&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\nThe full model has an \\(\\text{adjusted }R^2\\) of 0.245 and accounts for 32.3% of the variance in the data (\\(R^2\\) = 0.323). The majority of the variables has a small impact on the response and/or is not statiscally significant, so we should be able to remove some (a lot) of the predictors without the model getting worse.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#reduced-model-variable-selection-based-on-p-values",
    "href": "stats_with_R/3_regression/3_regression.html#reduced-model-variable-selection-based-on-p-values",
    "title": "Modeling and prediction of movie scores",
    "section": "Reduced model: variable selection based on p-values",
    "text": "Reduced model: variable selection based on p-values\nAs I am rather interested in significant and important predictors for audience_score, model selection will be based on p-values. Given the large number of variables, the ols_step_both_p function from the olsrr package will be used. This function performs both forward and backward selection and finally returns a model containing only statistically significant predictors.\npm = ols_step_both_p(md, prem=0.05)\npar(mfrow=c(1,3))\nplot(pm$model, which=c(1,2), add.smooth=FALSE)\nhist(pm$model$residuals)\n\nAt a brief look at the final model diagnostics plots, there is no visible pattern in the residuals. They are distributed somewhat normally around zero with a slight left skew. All conditions seem to be satisfied.\ntab_model(pm$model, p.style = 'asterisk')\n\n\n\n\n \n\n\naudience_score\n\n\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\n\n\n(Intercept)\n\n\n34.81 ***\n\n\n26.99 – 42.63\n\n\n\n\ngenre.Documentary [1]\n\n\n30.79 ***\n\n\n25.43 – 36.14\n\n\n\n\ngenre.Drama [1]\n\n\n10.24 ***\n\n\n7.23 – 13.26\n\n\n\n\nruntime\n\n\n0.18 ***\n\n\n0.11 – 0.25\n\n\n\n\ngenre.Musical…Performing.Arts[1]\n\n\n24.73 ***\n\n\n14.35 – 35.11\n\n\n\n\ndirector.Woody.Allen [1]\n\n\n30.53 ***\n\n\n13.10 – 47.96\n\n\n\n\nstudio.MGM [1]\n\n\n-11.76 **\n\n\n-20.62 – -2.90\n\n\n\n\nRichard.Gere [yes]\n\n\n-16.27 *\n\n\n-28.82 – -3.73\n\n\n\n\nChristian.Bale [yes]\n\n\n18.21 *\n\n\n2.26 – 34.17\n\n\n\n\nstudio.Universal.Pictures[1]\n\n\n8.95 *\n\n\n1.52 – 16.39\n\n\n\n\ngenre.Art.House…International[1]\n\n\n10.81 *\n\n\n1.29 – 20.33\n\n\n\n\nMinnie.Driver [yes]\n\n\n16.90 *\n\n\n1.32 – 32.49\n\n\n\n\nKeanu.Reeves [yes]\n\n\n14.66 *\n\n\n0.43 – 28.88\n\n\n\n\ngenre.Animation [1]\n\n\n11.94 *\n\n\n0.13 – 23.75\n\n\n\n\nObservations\n\n\n650\n\n\n\n\nR2 / R2 adjusted\n\n\n0.256 / 0.240\n\n\n\n\n\n\np&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n  The final model contains considerably less variables, all being statistically significant. Note that this smaller model has comparable values for \\(\\text{adjusted }R^2 = 0.240\\) and \\(R^2 = 0.256\\).",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#interpretation",
    "href": "stats_with_R/3_regression/3_regression.html#interpretation",
    "title": "Modeling and prediction of movie scores",
    "section": "Interpretation",
    "text": "Interpretation\nRegression coefficients have to be interpreted as average change of the response when everthing else is held constant. With this in mind, we can see that there are several genres (Documentary, Drama, Animation, Art House, Musical) that have significantly higher scores. Longer films have higher scores on average (0.18 points per minute runtime). Also some actors have been identified to have a significant impact on the audience score - not always for the better. An example: verything else held constant, having Richard Gere in the cast leads to a decrease of -16.27 on average.\nCAVEAT These are observational data, we cannot infere causality. So we do not know whether the appearance of Richard Gere causes the movie to be bad or whether there are other factors causing the bad rating. Simply put - it could just be bad luck of Richard Gere to get casted for inherently bad movies.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/3_regression/3_regression.html#café-society",
    "href": "stats_with_R/3_regression/3_regression.html#café-society",
    "title": "Modeling and prediction of movie scores",
    "section": "Café Society",
    "text": "Café Society\n\nSources\n\nRotten Tomates\nIMDB\nWikipedia\n\n\n\nBase Data\nmovie &lt;- data.frame(actor1='Jeannie Berlin',\n                    actor2='Steve Carell',\n                    actor3='Jesse Eisenberg',\n                    actor4='Blake Lively',\n                    actor5='Parker Posey',\n                    director='Woody Allen',\n                    genre='Drama',\n                    runtime=96,\n                    studio='Amazon Studios',\n                    audience_score=56)\n\n\nData Manipulation\nThe variables of the movie whose audience_score we want to predict, have to match the variables that were used to build the model. Therefore, the same data preprocessing steps have to be applied.\nfor (a in actors) {\n  movie[gsub(' ', '.', a)] = as.factor(apply(movie, 1, get_actor, actor = a))\n}\n\nmovie$studio = as.factor(unlist(apply(movie, 1, get_studio)))\nmovie$director = as.factor(unlist(apply(movie, 1, get_director)))\n\nmovie = movie[append(gsub(' ', '.', actors), c('director',\n                                               'runtime',\n                                               'genre',\n                                               'studio',\n                                               'audience_score'))]\n\ndir_dummy &lt;- data.frame(predict(dmy,\n                                newdata = data.frame(director = movie$director,\n                                                     studio = movie$studio,\n                                                     genre = movie$genre))) %&gt;% \n  lapply(as.factor) %&gt;% \n  as.data.frame()\n\nmovie = data.frame(\n  append(\n    select(\n      movie, -c('director', 'studio', 'genre') #dropping the original columns\n      ), \n    select(\n      dir_dummy, -c('director.other', 'studio.other', 'genre.Other') #drop the reference levels of studio, director and Genre\n      )\n    )\n)\n\n\nFinal Prediction (and Prediction Interval)\npred = predict(pm$model, movie, interval = 'prediction', level = 0.95)\nThe audience score on Rotten Tomatoes for Café Society is\n\\[56\\]\nwhile the final model predicts\n\\[93\\]\nwith a prediction interval at the 95% confidence level of\n\\[54 - 132\\]\nInterpretation of the prediction interval We are 95% confident that this interval contains the specific response audicence_score corresponding to \\(x(Cafe\\ Society)\\).",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "3 Regression",
      "Modeling and prediction of movie scores"
    ]
  },
  {
    "objectID": "stats_with_R/1_intro_data/1_intro_data.html",
    "href": "stats_with_R/1_intro_data/1_intro_data.html",
    "title": "Exploring the BRFSS data",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(plyr)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(reshape2)\n\n\n\nload(\"brfss2013.RData\")",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "1 Intro Data",
      "Exploring the BRFSS data"
    ]
  },
  {
    "objectID": "stats_with_R/1_intro_data/1_intro_data.html#load-packages",
    "href": "stats_with_R/1_intro_data/1_intro_data.html#load-packages",
    "title": "Exploring the BRFSS data",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(plyr)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(reshape2)",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "1 Intro Data",
      "Exploring the BRFSS data"
    ]
  },
  {
    "objectID": "stats_with_R/1_intro_data/1_intro_data.html#load-data",
    "href": "stats_with_R/1_intro_data/1_intro_data.html#load-data",
    "title": "Exploring the BRFSS data",
    "section": "",
    "text": "load(\"brfss2013.RData\")",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "1 Intro Data",
      "Exploring the BRFSS data"
    ]
  },
  {
    "objectID": "stats_with_R/1_intro_data/1_intro_data.html#introduction",
    "href": "stats_with_R/1_intro_data/1_intro_data.html#introduction",
    "title": "Exploring the BRFSS data",
    "section": "Introduction",
    "text": "Introduction\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a US nationwide survey regarding health-related risk behaviors, chronic health conditions, and use of preventive services which is conducted annually since 1984. The data for this survey is collected via telephone interviews over landline or mobile phones on a randomly chosen sample. The BRFSS is an observational study (as no treatment is either given or withheld), therefore no causal inference concerning the collected data can be draw. However, it is a means to screen for associations between the sampled variables, which in turn might be the foundation for further investigations in randomized experiments.\nSampling strategy and generalizability of findings Sampling for landline and mobile surveys differed (see BRFSS 2013 Overview p.6). While stratified sampling was applied to obtain landline numbers, mobile phone numbers were obtained by dividing the complete set of mobile phone numbers into a number of clusters equivalent to the desired sample size, and subsequent sampling of one number from each cluster.\nNon-response bias While both methods sample phone numbers randomly, great caution has to be taken, when generalizing findings to the population at large due to the fact that non-responder rates are high (50.4% and 62.2% for landline and mobile surveys, respectively, see BRFSS 2013 Quality Report, p.4).\nConvenience sample Additionaly, the sample might be biased towards people that can be reached by phone. Parts of the population that do not have (or want) acces to a phone or are less likely to answer (low income, illegal immigrants, children) might be missed during sampling.\n\n\n\nLecture Slides - Week 1, Random Sample Assignment",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "1 Intro Data",
      "Exploring the BRFSS data"
    ]
  },
  {
    "objectID": "stats_with_R/1_intro_data/1_intro_data.html#visual-eda",
    "href": "stats_with_R/1_intro_data/1_intro_data.html#visual-eda",
    "title": "Exploring the BRFSS data",
    "section": "Visual EDA",
    "text": "Visual EDA\n#plots for each variable\neda_p1 &lt;- \n  na.omit(brfss2013[,c('income2','sex')]) %&gt;% dplyr::group_by(sex) %&gt;% dplyr::count() %&gt;%\n  ggplot(aes(x=sex, y = n, fill=sex)) +\n  geom_bar(stat='identity')\n\neda_p2 &lt;- \n  na.omit(brfss2013[,c('income2','sex')]) %&gt;% dplyr::group_by(income2) %&gt;% dplyr::count() %&gt;%\n  ggplot(aes(x=income2, y = n, fill=income2)) +\n  geom_bar(stat='identity')\n\neda_p3 &lt;- \n  na.omit(brfss2013[,c('sex','genhlth')]) %&gt;% dplyr::group_by(genhlth) %&gt;% dplyr::count() %&gt;%\n  ggplot(aes(x=genhlth, y = n, fill=genhlth)) +\n  geom_bar(stat='identity')\n\neda_p4 &lt;- \n  na.omit(brfss2013[,c('sex','X_mrace1')]) %&gt;% dplyr::group_by(X_mrace1) %&gt;% dplyr::count() %&gt;%\n  filter(X_mrace1 == 'White' |\n           X_mrace1 == 'Black or African American' |\n           X_mrace1 == 'Asian') %&gt;%\n  ggplot(aes(x=X_mrace1, y = n, fill=X_mrace1)) +\n  geom_bar(stat='identity')\n\n#arranging plots in a grid\ngrid.arrange(\n  eda_p1 + ggtitle(\"Gender distribution\") + theme(legend.position = \"none\"),\n  eda_p2 + ggtitle(\"Income distribution\")\n    + scale_x_discrete(labels=c('&lt;$10k', '&lt;$15k', '&lt;$20k', '&lt;$25k','&lt;$35k', '&lt;$50k', '&lt;$75k', '&gt;$75k')) \n    + theme(legend.position = 'none', axis.text.x = element_text(angle = 45, hjust=1)) \n    + xlab(\"\"),\n  eda_p3 + ggtitle(\"Dist. of Health status\") + theme(legend.position = \"none\"),\n  eda_p4 + ggtitle(\"Dist.  of ethnicity\") + theme(legend.position = \"none\")\n         + xlab(\"\") + scale_x_discrete(labels=c('White', 'Black', 'Asian')),\n  nrow = 2\n) \n All distributions of variables that will be used in the subsequent analysis are skewed.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "1 Intro Data",
      "Exploring the BRFSS data"
    ]
  },
  {
    "objectID": "stats_with_R/1_intro_data/1_intro_data.html#summary-statistics",
    "href": "stats_with_R/1_intro_data/1_intro_data.html#summary-statistics",
    "title": "Exploring the BRFSS data",
    "section": "Summary statistics",
    "text": "Summary statistics\nNote on chosen summary statistics As all variables used to address the research questions are categorical, summary statistics will be counts and proportions calculated for each variable.\n\n\nGender\nkable(\n  data.frame(\n    na.omit(\n      brfss2013[,c('income2','sex')]) %&gt;%\n      dplyr::count(sex) %&gt;% \n      dplyr::mutate(p = round(100*n/sum(n),2))\n    ),\n  col.names = c('Sex', 'count', '%'),\n  format='html',\n  table.attr = \"class=\\\"table table-condensed\\\" width=50%\"\n  )\n\n\n\nSex\ncount\n%\n\n\n\n\nMale\n178186\n42.39\n\n\nFemale\n242162\n57.61\n\n\n\nWomen make up a total of 57.61% of participants, while they made up about only 50.94% of the US population in 2013. In the 2013 BRFSS, there were 1.36 times more female than male participants.\n\n\n\nIncome Level\nkable(\n  na.omit(\n    brfss2013[,c('income2','sex')]) %&gt;%\n    dplyr::count(income2) %&gt;% \n    dplyr::mutate(p = round((n/sum(n))*100,2)),\n  col.names = c(\"Income\", \"count\", \"%\"),\n  format='html',\n  table.attr = \"class=\\\"table table-condensed\\\" width=50%\"\n)\n\n\n\nIncome\ncount\n%\n\n\n\n\nLess than $10,000\n25441\n6.05\n\n\nLess than $15,000\n26793\n6.37\n\n\nLess than $20,000\n34873\n8.30\n\n\nLess than $25,000\n41732\n9.93\n\n\nLess than $35,000\n48867\n11.63\n\n\nLess than $50,000\n61509\n14.63\n\n\nLess than $75,000\n65231\n15.52\n\n\n$75,000 or more\n115902\n27.57\n\n\n\n27.6% of participants earn more are in the highest income group, that is approx. the same proportion as within the 4 lowest income groups taken together.\n\n\n\nHealth Status\nkable(\n na.omit(\n   brfss2013[,c('sex','genhlth')]) %&gt;% \n   dplyr::count(genhlth) %&gt;% \n   dplyr::mutate(p = round((n/sum(n))*100,2)),\n col.names = c('Gen. Health', 'count', '%'),\n  format='html',\n  table.attr = \"class=\\\"table table-condensed\\\" width=50%\"\n)\n\n\n\nGen. Health\ncount\n%\n\n\n\n\nExcellent\n85481\n17.45\n\n\nVery good\n159075\n32.48\n\n\nGood\n150555\n30.74\n\n\nFair\n66726\n13.62\n\n\nPoor\n27951\n5.71\n\n\n\nPariticipants with Good or Very Good general health status make up over 60% of all responders.\n\n\n\nEthnicity\nkable(\n  na.omit(brfss2013[,c('sex','X_mrace1')]) %&gt;% \n  filter(X_mrace1 == 'White' |\n         X_mrace1 == 'Black or African American' |\n         X_mrace1 == 'Asian') %&gt;% dplyr::count(X_mrace1) %&gt;% \n  dplyr::mutate(p = round((n/sum(n))*100,2)),\n  col.names = c('Ethnicity', 'count', '%'),\n  format='html',\n  table.attr = \"class=\\\"table table-condensed\\\" width=50%\"\n)\n\n\n\nEthnicity\ncount\n%\n\n\n\n\nWhite\n400421\n88.69\n\n\nBlack or African American\n41221\n9.13\n\n\nAsian\n9850\n2.18\n\n\n\nIn a subsample only considering White, Black/African American and Asian ethinicity, the vast majority (88.7%) of the participants belongs to the White ethnicity.\n\nConclusion EDA of chosen variables revealed, that all of them are skewed, i.e. are not uniformly distributed across factor levels. While this might actually reflect the true distribution in the population, care has to be taken, when conclusions based from these data are generalized without any further checks, as it is possible that this sample is biased (this has been discussed in Part 1: Data)\n\n\nComment on the significance of observed differences All research questions seek to investigate putative associations between variables. Assuming no association, we would expect roughly equal distributions for the factors of the response variable regardles of the chosen explanatory variable. Any difference that might befound, reflects first and foremost the difference within the given sample. Whether findings can be generalized to the population at large is largely dependent on the sampling strategy, as discussed above. In order to judge the significance of any observed difference, tools of statistical inference are required, which will not be used in this project. Therefore, I tried to draw rather reluctant conclusions on the three research questions. And still, all of them should be taken with a grain of salt and/or a more thorough analysis.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "1 Intro Data",
      "Exploring the BRFSS data"
    ]
  },
  {
    "objectID": "stats_with_R/1_intro_data/1_intro_data.html#is-there-an-association-between-gender-and-income-level",
    "href": "stats_with_R/1_intro_data/1_intro_data.html#is-there-an-association-between-gender-and-income-level",
    "title": "Exploring the BRFSS data",
    "section": "Is there an association between gender and income level?",
    "text": "Is there an association between gender and income level?\n\nIntro and objective\nWhy would this be an interesting question?\nGender equality is of great concern and a major topic in public discussion. One of the major questions deals with justice when it comes to salaries. Therefore exploring a putative association between gender and income level is of particular interest.\nTable - Difference (%) between genders in income levels\n#extract income and sex variables\ninc_gender &lt;-\n  na.omit(brfss2013[,c('income2','sex')])\n\n#remap factor labels for easier plotting\ninc_gender$income2 =\n  mapvalues(inc_gender$income2,\n            from=levels(inc_gender$income2),\n            to = c('&lt;$10k', '&lt;$15k', '&lt;$20k', '&lt;$25k',\n                   '&lt;$35k', '&lt;$50k', '&lt;$75k', '&gt;$75k'))\n\n#calculate proportions of income levels conditioned by gender\nperc = inc_gender %&gt;%\n  dplyr::group_by(sex) %&gt;%\n  dplyr::count(income2) %&gt;%\n  dplyr::mutate(Freq = (n/sum(n))*100)\n\ndiff = data.frame(perc[perc$sex=='Male',]$Freq - perc[perc$sex=='Female',]$Freq)\ncolnames(diff) &lt;- 'Difference (%)'\ndiff = t(diff)\ncolnames(diff) &lt;- c(\"&lt;$10k\", \"&lt;$15k\", \"&lt;$20k\", \"&lt;$25k\", \"&lt;$35k\", \"&lt;$50k\", \"&lt;$75k\", \"&gt;$75k\")\nkable(round(diff,2),format = 'markdown')\n\n\n\n\n&lt;$10k\n&lt;$15k\n&lt;$20k\n&lt;$25k\n&lt;$35k\n&lt;$50k\n&lt;$75k\n&gt;$75k\n\n\n\n\nDifference (%)\n-2.42\n-2.1\n-2.16\n-1.91\n-1.06\n0.72\n1.71\n7.21\n\n\n\nThis table shows the percentage of male participants minus the percentage of female participants for each income level. Therefore, a negative value indicates more women within a given group. The gap decreases with increasing income and is actually inversed for incomes greater than $50,000 (positive value indicates more men than women). For addtiional interpretation see below.\n#general plot layout\np_p &lt;- \n  perc %&gt;%\n  ggplot(aes(x=sex, y = Freq, fill=sex)) +\n  geom_bar(stat='identity') +\n  facet_grid(~income2)\n\n#plot fine tuning\np_p +\n  ggtitle(\"Gender distribution within income levels\") +\n  #xlab(\"Income level\") +\n  ylab(\"Percentage\") +\n  theme(axis.text.x = element_text(angle = 45, hjust=1))\n\n\n\nInterpretation and conclusions\nInterpretation The plot shows the percentage of participants in each income group, conditioned by gender. Women seem to be overrepresented in income groups &lt;35,000$. A striking difference is found in the highest income group (&gt;75,000$), where we find 7.21% more men than women (see table above). If there was no association between income level and gender, we would expect numbers for men in women in each group to be roughly equal. This might be the case for all groups except &gt;$75k, where the difference appears more obvious.\nCAVEAT In order to draw any substantial conclusion on the observed differences, more research is needed, as well as tools from statistical inference, which is out the scope of this project. The causal conclusion, that women are paid less solely because of their gender, would be invalid, if based only on this analysis, as there was no control for confounding variables. Please refer to additional sources on Gender Pay Gap.\nConclusion Question 1 The trends inferred from the plot indicate an association between gender and income level within this sample.\nReminder on generalizability of the findings As discussed above, findings from the analysis of this sample cannot be generalized to the population and therefore can (if at all) only be indicative with respect to the overall population. In order to draw any inference on the relationship of these variables further research is needed.",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "1 Intro Data",
      "Exploring the BRFSS data"
    ]
  },
  {
    "objectID": "stats_with_R/1_intro_data/1_intro_data.html#is-there-an-association-between-gender-income-level-and-general-health-status",
    "href": "stats_with_R/1_intro_data/1_intro_data.html#is-there-an-association-between-gender-income-level-and-general-health-status",
    "title": "Exploring the BRFSS data",
    "section": "Is there an association between gender, income level and general health status?",
    "text": "Is there an association between gender, income level and general health status?\n\nIntro and objective\nWhy would this be an interesting question?\nAs an addon to question no. 1, this questions seeks to explore putative associations between gender, income and general health.\nPlot 1\n#extract income, sex and genhlth variables\ninc_health_gender &lt;-\n  na.omit(brfss2013[,c('income2','sex', 'genhlth')])\n\n#remap factor labels for easier plotting\ninc_health_gender$income2 &lt;-\n  mapvalues(inc_health_gender$income2,\n            from=levels(inc_health_gender$income2),\n            to = c('&lt;$10,000', '&lt;$15,000', '&lt;$20,000', '&lt;$25,000',\n                   '&lt;$35,000', '&lt;$50,000', '&lt;$75,000', '&gt;$75,000'))\n\n#calculate proportions of gender in each health status group\nperc_2 = inc_health_gender %&gt;%\n  dplyr::group_by(sex) %&gt;%\n  dplyr::count(genhlth) %&gt;%\n  dplyr::mutate(Freq = n*100/sum(n))\n\n#general plot layout\np_p &lt;- \n  perc_2 %&gt;%\n  ggplot(aes(x=sex, y = Freq, fill=sex)) +\n  geom_bar(stat='identity') +\n  facet_grid(~genhlth)\n\np_p\n\n\n\nInterpretation and conclusions\nInterpretation A first analysis of the gender health status distributions, reveals equal proportions between men and women for the Excellent and Very good factor values. The difference is a little more pronounced in the remaining groups, with women being underrepresented in the Good group, while the remainder is divided between the Fair and the Poor group. Taken together, within this sample women are more likely to have a Fair or Poor health status.\nCAVEAT As pointed out above, women were more likely to respond to this survey. This might provide an explanation for women being overrepresented in the groups with general health status: If people with lower general health status have a lower probability to respond to this survey, sampling more subjects could possibly include more subjects with less likely attributes. For this reason, there might be more women with lower general health status in the survey than men.\n#calculate proportions of health status in income levels conditioned by gender\nperc_2 = inc_health_gender %&gt;%\n  dplyr::group_by(sex,income2) %&gt;%\n  dplyr::count(genhlth) %&gt;%\n  dplyr::mutate(Freq = n*100/sum(n))\n\n#general plot layout\np_p2 &lt;- \n  perc_2 %&gt;%\n  ggplot(aes(x=sex, y = Freq, fill=genhlth)) +\n  geom_bar(stat='identity') +\n  facet_grid(~income2)\n\n#plot fine tuning\np_p2 +\n  ggtitle(\"Distribution of health status within income level conditioned by gender\") +\n  xlab(\"\") +\n  ylab(\"Cumulative percentage\") +\n  theme(axis.text.x = element_text(angle = 45, hjust=1)) +\n  scale_fill_discrete(name='Health status')\n\nInterpretation An overall trend is easily visible, higher income means better general health status for both men and women. This can be inferred from the rising proportion of people with Excellent or Good health status in higher income groups or the decreasing proportions of people with health status Poor. When taking into account the distribution of health status within income groups, it can be found that for income levels &gt;$20,000 women seem to be more likely to be in good health (Good + Very good + Excellent). For the &lt;$15,000 group, distributions of health status are roughly equal between men and women, while at very low incomes the effect seen in the higher income groups is inversed.\nConclusion Question 2 With the trends visible in Plot 2, gender, general health status and income level seem to be associated within this sample. Notably, the extent to which income level impacts health status seems to be different for men and women.\nReminder on generalizability of the findings Same as for question 1",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "1 Intro Data",
      "Exploring the BRFSS data"
    ]
  },
  {
    "objectID": "stats_with_R/1_intro_data/1_intro_data.html#is-there-and-association-between-race-white-black-or-african-american-asian-income-level-and-general-health-status",
    "href": "stats_with_R/1_intro_data/1_intro_data.html#is-there-and-association-between-race-white-black-or-african-american-asian-income-level-and-general-health-status",
    "title": "Exploring the BRFSS data",
    "section": "Is there and association between race (White, Black or African American, Asian), income level and general health status?",
    "text": "Is there and association between race (White, Black or African American, Asian), income level and general health status?\n\nIntro and objective\nWhy would this be an interesting question?\nRace has been described to possbily have an impact on income and health.\nPlot\n#extract income and sex variables, filter for specific races\ninc_health_eth &lt;-\n  na.omit(brfss2013[,c('income2','X_mrace1', 'genhlth')]) %&gt;%\n  filter(X_mrace1 == 'White' |\n           X_mrace1 == 'Black or African American' |\n           X_mrace1 == 'Asian')\n\n#remap factor labels for easier plotting\ninc_health_eth$income2 = mapvalues(inc_health_eth$income2,\n                                   from=levels(inc_health_eth$income2),\n                                   to = c('&lt;$10,000', '&lt;$15,000', '&lt;$20,000', '&lt;$25,000',\n                                          '&lt;$35,000', '&lt;$50,000', '&lt;$75,000', '&gt;$75,000'))\n\n#calculate proportions of income levels conditioned by ethnicity\nperc_3 = inc_health_eth %&gt;%\n  dplyr::group_by(X_mrace1,income2) %&gt;%\n  dplyr::count(genhlth) %&gt;%\n  dplyr::mutate(Freq = (n/sum(n))*100)\n\n#general plot layout\np_p3 &lt;-perc_3 %&gt;% ggplot(aes(x=X_mrace1, y = Freq, fill=genhlth)) +\n  geom_bar(stat='identity') +\n  facet_grid(~income2)\n\n#plot fine tuning\np_p3 +\n  ggtitle(\"Distribution of general health status within income levels conditioned by ethnicity\") +\n  xlab(\"Ethnicity\") +\n  ylab(\"Percentage\") +\n  theme(axis.text.x = element_text(angle = 45, hjust=1)) +\n  scale_fill_discrete(name='Health status') +\n  geom_hline(yintercept = c(25,50,75), linetype=\"dashed\", alpha=0.5)\n\n\n\nInterpretation and conclusions\nInterpretation The overall trend of general health status is the same across income classes when conditioned for ethnicity: Higher income - better overall health. Two interesting observations can be made: 1) Asians seem to have a better general health status compared to Whites and Black/African Americans especially in the lower income levels. 2) While general health status of Whites and Black/African Americans are comparable within the low income levels, Black/African Americans seem to profit less from their higher income when it comes to their overall health (smaller percentage with excellent health and higher percentage of Fair + Poor health status compared to Whites and Asians).\nConclusion Question 3 With the trends visible in the plot above, ethnicity, general health status and income level seem to be associated within this sample. As we have seen for gender, race may influence the impact of income level on general health status.\nReminder on generalizability of the findings Same as for question 1",
    "crumbs": [
      "Projects",
      "<img src='/cv/pictos/r.svg' width=15px> Data Analysis with R",
      "1 Intro Data",
      "Exploring the BRFSS data"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "datacamp_courses/python/geospatial_data/markdown/chapter2.html",
    "href": "datacamp_courses/python/geospatial_data/markdown/chapter2.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "WORKING WITH GEOSPATIAL DATA IN PYTHON\n\nGeographic Data Science Lab (University of Liverpool)\ncities = geopandas.read_file(\"ne_110m_populated_places.shp\")\ncities.head()\nbrussels = cities.loc[170, 'geometry']\nprint(brussels)\nPOINT (4.33137074969045 50.83526293533032)\nbrussels = cities.loc[170, ‘geometry’]\nprint(brussels)\nPOINT (4.33137074969045 50.83526293533032)\ntype(brussels)\nshapely.geometry.point.Point\ntype(brussels)\nshapely.geometry.point.Point\n\nPython Package for the manipulation and analysis of geometric objects\nProvides the Point, LineString and Polygon objects\nGeoSeries (GeoDataFrame ‘geometry’ column) consists of shapely objects\n\nAccessing from a GeoDataFrame:\nbrussels = cities.loc[170, 'geometry']\nparis = cities.loc[235, 'geometry']\nbelgium = countries.loc[countries['name'] == 'Belgium', 'geometry'].squeeze()\nfrance = countries.loc[countries['name'] == 'France', 'geometry'].squeeze()\nuk = countries.loc[countries['name'] == 'United Kingdom', 'geometry'].squeeze()\nCreating manually:\nfrom shapely.geometry import Point\n(=) Point ((1,2))\nprint(p)\nPOINT (1 2)\nThe area of a geometry:\nbelgium.area\nThe distance between 2 geometries:\nbrussels.distance(paris)\n2.8049127723186214\nAnd many more! (e.g. centroid, simplify , …)\ngeopandas.GeoSeries([belgium, france, uk, paris, brussels, line]).plot()\n\n\nfrance.contains(brussels)\nbrussels.within(belgium)\nTrue\nTrue\nline.intersects(france)\nTrue\nline.intersects(uk)\nFalse\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nGeographic Data Science Lab\n (University of Liverpool)\nbrussels.within(france)\nFalse\nparis.within(france)\nTrue\nbrussels.within(france)\nFalse\nFor full GeoDataFrame?\ncities.head()\nname\ngeometry\n0 Vatican City POINT (12.45338654497177 41.90328217996012)\n1 San Marino POINT (12.44177015780014 43.936095834768)\n2 Vaduz POINT (9.516669472907267 47.13372377429357)\n3 Lobamba POINT (31.19999710971274 -26.46666746135247)\nThe within() operation for each geometry in cities :\ncities.within(france)\ncities[‘geometry’][0].within(france)\n\nFalse\ncities[‘geometry’][1].within(france)\ncities[‘geometry’][2].within(france)\nFalse\n…\nFilter cities depending on the within() operation:\ncities[cities.within(france)]\nWhich countries does the Amazon flow through?\nrivers = geopandas.read_file(“ne_50m_rivers_lake_centerlines.shp”)\nrivers.head()\n\namazon = rivers[rivers['name'] == 'Amazonas'].geometry.squeeze()\nmask = countries.intersects(amazon)\ncountries[mask]\n\nwithin\ncontains\nintersects\n\nMore at https://shapely.readthedocs.io/en/latest/\nparis.within(france)\nTrue\nfrance.intersects(amazon)\nFalse\ncities.within(france)\n\ncountries.intersects(amazon)\n(\n\\[\\begin{array}{ll}0 & \\text { False } \\\\ 1 & \\text { False } \\\\ 2 & \\text { False }\\end{array}\\]\n)\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nGeographic Data Science Lab\n (University of Liverpool)\n\nWhich cities are located within Brazil?\nbrazil = countries.loc[22, 'geometry']\ncities[cities.within(brazil)]\n\nBut what if we want to know for each city in which country it is located?\n\njoined = geopandas.sjoin(cities,\n    countries[['name', 'geometry']],\n    op=\"within\")\njoined.head()\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\n\nGeographic Data Science Lab (University of Liverpool)\ncountries.plot(column=‘gdp_per_cap’, legend=True)\n\nSpecifying a column:\nlocations.plot(column=‘variable’)\nChoropleth with classification scheme:\nlocations.plot(column=‘variable’, scheme=‘quantiles’, k=7, cmap=‘viridis’)\nKey choices:\n\nNumber of classes ( k )\nClassification algorithm ( scheme )\nColor palette ( cmap )\n\nlocations.plot(column=‘variable’, scheme=‘Quantiles’, k=7, cmap=‘viridis’)\nChoropleths necessarily imply information loss (but that’s OK)\nTension between:\n\nMaintaining detail and granularity from original values (higher k )\nAbstracting information so it is easier to process and interpret (lower k)\n\nRule of thumb: 3 to 12 classes or “bins”\nlocations.plot(column=‘variable’, scheme=‘quantiles’, k=7, cmap=‘viridis’)\nHow do we allocate every value in our variable into one of the (k) groups?\nTwo (common) approaches for continuous variables:\n\nEqual Intervals (‘equal_interval’)\nQuantiles ( ‘quantiles’)\n\nlocations.plot(column=‘variable’, scheme=‘equal_interval’, k=7, cmap=‘Purples’)\n\nlocations.plot(column=‘variable’, scheme=‘quantiles’, k=7, cmap=‘Purples’)\nquantiles\n\nGeographical distribution\n\nCategories, non-ordered\nlocations.plot(column='variable',\n    categorical=True, cmap='Purples')\nGraduated, sequential\nlocations.plot(column=‘variable’, (=5, =) ‘RdPu’)\nGraduated, divergent\nlocations.plot(column=‘variable’, (k=5), cmap=‘RdY̌Gn’)\nIMPORTANT: Align with your purpose\nWORKING WITH GEOSPATIAL DATA IN PYTHON"
  },
  {
    "objectID": "datacamp_courses/python/geospatial_data/markdown/chapter3.html",
    "href": "datacamp_courses/python/geospatial_data/markdown/chapter3.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "Joris Van den Bossche\nOpen source software developer and\n teacher, GeoPandas maintainer\nLocation of the Eiffel Tower:\nPOINT (2.2945 48.8584)\n() The Coordinate Reference System (CRS) relates the coordinates to a specific location on earth.\n\nDegrees of latitude and longitude.\nE.g. (48^{} 51^{} N, 2^{} 17^{} )\nUsed in GPS, web mapping applications…\nin Python we use (lon, lat) and not (lat, long) - Longitude: ([-180,180]) - Latitude: ([-90,90])\n\n\n((x, y))\n((x, y)) coordinates are usually in meters or feet\nAlbers Equal Area projection\n\nMercator projection\n\nProjected size vs actual size (Mercator projection)\n\nproj4 string\nExample: +proj=longlat +datum=WGS84 +no_defs\nDict representation:\n{‘proj’: ‘longlat’, ‘datum’: ‘WGS84’, ‘no_defs’: True}\nEPSG code\nExample:\nEPSG:4326 = WGS84 geographic CRS (longitude, latitude)\nThe .crs attribute of a GeoDataFrame/GeoSeries:\nimport geopandas\ngdf = geopandas.read_file(“countries.shp”)\nprint(gdf.crs)\n{‘init’: ‘epsg:4326’}\n\n“geographic” (long, lat) versus “projected” ( (x, y) ) coordinates\nCoordinates Reference System (CRS) in GeoPandas: .crs attribute\nMost used geographic CRS: WGS84 or EPSG:4326\n\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nJoris Van den Bossche\nOpen source software developer and\n teacher, GeoPandas maintainer\nThe .crs attribute of a GeoDataFrame/GeoSeries:\nimport geopandas\ngdf = geopandas.read_file(“countries.shp”)\nprint(gdf.crs)\n{‘init’: ‘epsg:4326’}\ngdf_noCRS = geopandas.read_file(“countries_noCRS.shp”)\nprint(gdf_noCRS.crs)\n{}\nAdd CRS information to crs :\n# Option 1\ngdf.crs = {'init': 'epsg:4326'}\n# Option 2\ngdf.crs = {'proj': 'longlat', 'datum': 'WGS84', 'no_defs': True}\nimport geopandas\ngdf = geopandas.read_file(“countries_web_mercator.shp”)\nprint(gdf.crs)\n{‘init’: ‘epsg:3857’, ‘no_defs’: True}\nThe to_crs() method:\n# Option 1\ngdf2 = gdf.to_crs({'proj': 'longlat', 'datum': 'WGS84', 'no_defs': True})\n# Option 2\ngdf2 = gdf.to_crs(epsg=4326)\n\nSources with a different CRS\n\ndf1 = geopandas.read_file(...)\ndf2 = geopandas.read_file(...)\ndf2 = df2.to_crs(df1.crs)\n\nSources with a different CRS\nMapping (distortion of shape and distances)\n\n\n\nSources with a different CRS\nMapping (distortion of shape and distances)\nDistance / area based calculations\n\nTips:\n\nUse projection specific to the area of your data\nMost countries have a standard CRS\n\nUseful sites:\n\nhttp://spatialreference.org/\nhttps://epsg.io/\n\n\nTo convert to another CRS: the to_crs() method\nMake sure different datasets have the same CRS\nWhen calculating distance, area, … -&gt; use a projected CRS\n\nUseful sites:\n\nhttp://spatialreference.org/\nhttps://epsg.io/\n\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nJoris Van den Bossche\n\nOpen source software developer and teacher, GeoPandas maintainer\na\nb\n\n\n[ ]\n\n[ ]\nafrica.head()\n\n\nprint(box)\nPOLYGON ((60 10, (60-10,-20-10,-2010))\n\nafrica.intersection(box) \nafrica.head()\nafrica.intersection(box)\n0 (POLYGON ((13.22332255001795 -10, 13.120987583…\n1 POLYGON ((29.33999759290035 -4.499983412294092…\n2\n()\n. .\ndtype: object\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nWORKING WITH GEOSPATIAL DATA IN PYTHON\nJoris Van den Bossche\nOpen source software developer and\n teacher, GeoPandas maintainer\n\ncountries.intersection(circle)\nLimitations of countries.intersection(circle) : - Only intersecting a GeoSeries with a single polygon\n\nDoes not preserve attribute information\n\n\ncountries.plot()\n\n[ ]\n\ngeopandas.overlay(countries, geologic_regions, how=‘intersection’)\nIntersection method (with single polygon)\ncountries.intersection(geologic_region_A)\ngeopandas.overlay(countries, geologic_regions, how=‘intersection’)\n\nWORKING WITH GEOSPATIAL DATA IN PYTHON"
  },
  {
    "objectID": "uncategorized_projects/nypd_shooting/nyc_shooting.html",
    "href": "uncategorized_projects/nypd_shooting/nyc_shooting.html",
    "title": " DS as a Field - NYPD shooting analysis report",
    "section": "",
    "text": "Dataset used in this report : NYPD Shooting Incident Data (Historic)\nThe data set is a manually curated list of all shooting incidents in NYC from 2006 through the end of the previous calender year. It contains the coordinates of the shooting and the corresponding police precinct, demographic information about victim and perpetrator (if known) and information about the location of the incident.\n\n\n\nIn a short analysis I will have a look a the trend of gun violence in the period 2006 - 2022 and its spatial distribution among the 77 NYPD precincts.\n\n\n\n\n\nnypd_link_csv &lt;- \"https://data.cityofnewyork.us/api/views/833y-fsy8/rows.csv?accessType=DOWNLOAD\"\nnypd_link_json &lt;- \"https://data.cityofnewyork.us/api/views/833y-fsy8/rows.json?accessType=DOWNLOAD\"\n\njson &lt;- fromJSON(nypd_link_json)\nmeta &lt;- json$meta\ndata &lt;- read_csv(nypd_link_csv) %&gt;%\n  mutate(OCCUR_DATE = mdy(OCCUR_DATE))\n\n\n\nThe metadata contains a short description of each column in the dataset.\ncol_meta &lt;-\n  data.frame(col = meta$view$columns$name,\n             desc = meta$view$columns$description)\nkable(col_meta)\n\n\n\n\n\n\n\ncol\ndesc\n\n\n\n\nsid\nNA\n\n\nid\nNA\n\n\nposition\nNA\n\n\ncreated_at\nNA\n\n\ncreated_meta\nNA\n\n\nupdated_at\nNA\n\n\nupdated_meta\nNA\n\n\nmeta\nNA\n\n\nINCIDENT_KEY\nRandomly generated persistent ID for each arrest\n\n\nOCCUR_DATE\nExact date of the shooting incident\n\n\nOCCUR_TIME\nExact time of the shooting incident\n\n\nBORO\nBorough where the shooting incident occurred\n\n\nLOC_OF_OCCUR_DESC\n\n\n\nPRECINCT\nPrecinct where the shooting incident occurred\n\n\nJURISDICTION_CODE\nJurisdiction where the shooting incident occurred. Jurisdiction codes 0(Patrol), 1(Transit) and 2(Housing) represent NYPD whilst codes 3 and more represent non NYPD jurisdictions\n\n\nLOC_CLASSFCTN_DESC\n\n\n\nLOCATION_DESC\nLocation of the shooting incident\n\n\nSTATISTICAL_MURDER_FLAG\nShooting resulted in the victim’s death which would be counted as a murder\n\n\nPERP_AGE_GROUP\nPerpetrator’s age within a category\n\n\nPERP_SEX\nPerpetrator’s sex description\n\n\nPERP_RACE\nPerpetrator’s race description\n\n\nVIC_AGE_GROUP\nVictim’s age within a category\n\n\nVIC_SEX\nVictim’s sex description\n\n\nVIC_RACE\nVictim’s race description\n\n\nX_COORD_CD\nMidblock X-coordinate for New York State Plane Coordinate System, Long Island Zone, NAD 83, units feet (FIPS 3104)\n\n\nY_COORD_CD\nMidblock Y-coordinate for New York State Plane Coordinate System, Long Island Zone, NAD 83, units feet (FIPS 3104)\n\n\nLatitude\nLatitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326)\n\n\nLongitude\nLongitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326)\n\n\nLon_Lat\nLongitude and Latitude Coordinates for mapping\n\n\nBorough Boundaries\n\n\n\nCity Council Districts\n\n\n\nPolice Precincts\n\n\n\nZip Codes\n\n\n\nCommunity Districts\n\n\n\n\nFor my analysis I will only keep two columns:\n1.) PRECINCT and 2.) OCCUR_DATE\ndata &lt;- data %&gt;%\n  select(c(PRECINCT, OCCUR_DATE))\n\n\n\n\n\ntotal_n &lt;- data %&gt;%\n  group_by(year(OCCUR_DATE)) %&gt;%\n  count() %&gt;%\n  rename(year = `year(OCCUR_DATE)`)\nkable(total_n)\n\n\n\nyear\nn\n\n\n\n\n2006\n2055\n\n\n2007\n1887\n\n\n2008\n1959\n\n\n2009\n1828\n\n\n2010\n1912\n\n\n2011\n1939\n\n\n2012\n1717\n\n\n2013\n1339\n\n\n2014\n1464\n\n\n2015\n1434\n\n\n2016\n1208\n\n\n2017\n970\n\n\n2018\n958\n\n\n2019\n967\n\n\n2020\n1948\n\n\n2021\n2011\n\n\n2022\n1716\n\n\n\nggplot(total_n) +\n  aes(x = year, y = n) + geom_line() + geom_point()\n\nFrom 2006 the number shooting incidents continuously declined until it bottomed around 2018. Gun violence then surged in 2020 due to generally difficult economic conditions, which aggravated in the aftermath of the coronavirus epidemic.\n\n\n\n#calculate no. of incidents per police precinct\ntotal &lt;- data %&gt;% group_by(PRECINCT) %&gt;% count()\n#create a map\nprec_geo_url = 'https://opendata.arcgis.com/datasets/c35786feb0ac4d1b964f41f874f151c1_0.geojson'\n\n#download geojson of NYPD police precincts\ngeo &lt;- geojson_read(prec_geo_url, what = 'sp')\n\n#get geojson dataframe\ngeo_tidy &lt;- tidy(geo)\nThe polygons corresponding to the police precincts are numbered 1:77 in the geojson file. Therefore, we need to map the actual precinct codes in the data to this range.\ntotal$id &lt;- 1:77\ntotal$id &lt;- as.character(total$id)\nNow the number of incidents per precinct can be joined with the geodata.\ngeo_tidy_all &lt;- geo_tidy %&gt;%\n  left_join(total, by = 'id')\nThe geo_tidy dataframe is now ready to be used for the creation of the map.\nggplot(geo_tidy_all) + \n  geom_polygon(aes(x = long, y = lat, group = group, fill = n), color = 'black', size = 0.2) +\n  scale_fill_viridis_b(trans = \"log\", breaks = c(1, 5, 10, 50, 100, 500, 1000)) +\n  theme_void() +\n  labs(title = \"Cumulative spatial distribution of shooting incidents \\ngrouped by NYPD precinct\",\n       subtitle = \"2006 - 2022\") +\n  coord_map()\n\nkable(total %&gt;% arrange(desc(n)) %&gt;% select(c(PRECINCT, n)) %&gt;% head())\n\n\n\nPRECINCT\nn\n\n\n\n\n75\n1557\n\n\n73\n1452\n\n\n67\n1216\n\n\n44\n1020\n\n\n79\n1012\n\n\n47\n953\n\n\n\nGun violence is clearly spread over the whole city, with hot-spots in the Bronx and in East New York. The 75th and 73th precinct counted the highest number of shooting incidents over this period of time. The 75th precinct was once known as New York’s “Killing Ground”.\n\n\n\nnypp_id = read_csv(\"https://data.cityofnewyork.us/resource/kmub-vria.csv\")$precinct\n\n#calculate no. of incidents per police precinct\ntotal2006 &lt;- data %&gt;% \n  group_by(PRECINCT, year(OCCUR_DATE)) %&gt;% \n  rename(year = \"year(OCCUR_DATE)\") %&gt;%\n  count() %&gt;%\n  rename(Precinct = PRECINCT) %&gt;%\n  filter(year == 2006) %&gt;%\n  ungroup() %&gt;%\n  select(c(Precinct, n))\n\n#left join as there 0 incidents in some precincts\ntotal2006 &lt;- left_join(data.frame(Precinct = nypp_id),\n                       total2006,\n                       by = c('Precinct')) %&gt;%\n  mutate(n = ifelse(is.na(n), 0, n))\n\n#aligned with polygons\ntotal2006$id = as.character(1:77)\ngeo_tidy_2006 &lt;- geo_tidy %&gt;%\n  left_join(total2006, by = 'id')\nggplot(geo_tidy_2006) + \n  geom_polygon(aes(x = long, y = lat, group = group, fill = n), color = 'black', size = 0.2) +\n  scale_fill_viridis_b(trans = \"log\", breaks = c(1, 3, 5, 10, 30, 50, 100)) +\n  theme_void() +\n  labs(title = \"Spatial distribution of shootings incident grouped by NYPD precinct\",\n       subtitle = \"2006\") +\n  coord_map()\n\n\n\n\n#calculate no. of incidents per police precinct\ntotal2018 &lt;- data %&gt;% \n  group_by(PRECINCT, year(OCCUR_DATE)) %&gt;% \n  rename(year = \"year(OCCUR_DATE)\") %&gt;%\n  count() %&gt;%\n  rename(Precinct = PRECINCT) %&gt;%\n  filter(year == 2018) %&gt;%\n  ungroup() %&gt;%\n  select(c(Precinct, n))\n\n#left join as there 0 incidents in some precincts\ntotal2018 &lt;- left_join(data.frame(Precinct = nypp_id),\n                       total2018,\n                       by = c('Precinct')) %&gt;%\n  mutate(n = ifelse(is.na(n), 0, n))\n\n#aligned with polygons\ntotal2018$id = as.character(1:77)\ngeo_tidy_2018 &lt;- geo_tidy %&gt;%\n  left_join(total2018, by = 'id')\nggplot(geo_tidy_2018) + \n  geom_polygon(aes(x = long, y = lat, group = group, fill = n), color = 'black', size = 0.2) +\n  scale_fill_viridis_b(trans = \"log\", breaks = c(1, 3, 5, 10, 30, 50, 100)) +\n  theme_void() +\n  labs(title = \"Spatial distribution of shootings incident grouped by NYPD precinct\",\n       subtitle = \"2018\") +\n  coord_map()\n\nComparing the two maps from 2006 and 2018, clearly shows that there was a significant reduction in gun violence, also in and around the infamous 75th precinct.\n\n\n\n\nThe above findings are merely descriptive but do not investigate potential causes for the observed distribution and surge of gun violence in New York City. Further investigations should link the shooting data to socioeconomic data.\n\n\n\n\nThe data set contains only shooting incidents that were recorded by the police and is a convenience sample in the sense that unreported shootings are registered. As a consequence analyses of this data set are potentially underestimating the number of shootings.\n\n\n\n\nMaps in R https://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html\nChloropleth map https://r-graph-gallery.com/327-chloropleth-map-from-geojson-with-ggplot2.html",
    "crumbs": [
      "Projects",
      "Data Science",
      "NY Gun shootings"
    ]
  },
  {
    "objectID": "uncategorized_projects/nypd_shooting/nyc_shooting.html#dataset-description",
    "href": "uncategorized_projects/nypd_shooting/nyc_shooting.html#dataset-description",
    "title": " DS as a Field - NYPD shooting analysis report",
    "section": "",
    "text": "Dataset used in this report : NYPD Shooting Incident Data (Historic)\nThe data set is a manually curated list of all shooting incidents in NYC from 2006 through the end of the previous calender year. It contains the coordinates of the shooting and the corresponding police precinct, demographic information about victim and perpetrator (if known) and information about the location of the incident.",
    "crumbs": [
      "Projects",
      "Data Science",
      "NY Gun shootings"
    ]
  },
  {
    "objectID": "uncategorized_projects/nypd_shooting/nyc_shooting.html#analysis-objective",
    "href": "uncategorized_projects/nypd_shooting/nyc_shooting.html#analysis-objective",
    "title": " DS as a Field - NYPD shooting analysis report",
    "section": "",
    "text": "In a short analysis I will have a look a the trend of gun violence in the period 2006 - 2022 and its spatial distribution among the 77 NYPD precincts.",
    "crumbs": [
      "Projects",
      "Data Science",
      "NY Gun shootings"
    ]
  },
  {
    "objectID": "uncategorized_projects/nypd_shooting/nyc_shooting.html#data-import-and-cleaning",
    "href": "uncategorized_projects/nypd_shooting/nyc_shooting.html#data-import-and-cleaning",
    "title": " DS as a Field - NYPD shooting analysis report",
    "section": "",
    "text": "nypd_link_csv &lt;- \"https://data.cityofnewyork.us/api/views/833y-fsy8/rows.csv?accessType=DOWNLOAD\"\nnypd_link_json &lt;- \"https://data.cityofnewyork.us/api/views/833y-fsy8/rows.json?accessType=DOWNLOAD\"\n\njson &lt;- fromJSON(nypd_link_json)\nmeta &lt;- json$meta\ndata &lt;- read_csv(nypd_link_csv) %&gt;%\n  mutate(OCCUR_DATE = mdy(OCCUR_DATE))\n\n\n\nThe metadata contains a short description of each column in the dataset.\ncol_meta &lt;-\n  data.frame(col = meta$view$columns$name,\n             desc = meta$view$columns$description)\nkable(col_meta)\n\n\n\n\n\n\n\ncol\ndesc\n\n\n\n\nsid\nNA\n\n\nid\nNA\n\n\nposition\nNA\n\n\ncreated_at\nNA\n\n\ncreated_meta\nNA\n\n\nupdated_at\nNA\n\n\nupdated_meta\nNA\n\n\nmeta\nNA\n\n\nINCIDENT_KEY\nRandomly generated persistent ID for each arrest\n\n\nOCCUR_DATE\nExact date of the shooting incident\n\n\nOCCUR_TIME\nExact time of the shooting incident\n\n\nBORO\nBorough where the shooting incident occurred\n\n\nLOC_OF_OCCUR_DESC\n\n\n\nPRECINCT\nPrecinct where the shooting incident occurred\n\n\nJURISDICTION_CODE\nJurisdiction where the shooting incident occurred. Jurisdiction codes 0(Patrol), 1(Transit) and 2(Housing) represent NYPD whilst codes 3 and more represent non NYPD jurisdictions\n\n\nLOC_CLASSFCTN_DESC\n\n\n\nLOCATION_DESC\nLocation of the shooting incident\n\n\nSTATISTICAL_MURDER_FLAG\nShooting resulted in the victim’s death which would be counted as a murder\n\n\nPERP_AGE_GROUP\nPerpetrator’s age within a category\n\n\nPERP_SEX\nPerpetrator’s sex description\n\n\nPERP_RACE\nPerpetrator’s race description\n\n\nVIC_AGE_GROUP\nVictim’s age within a category\n\n\nVIC_SEX\nVictim’s sex description\n\n\nVIC_RACE\nVictim’s race description\n\n\nX_COORD_CD\nMidblock X-coordinate for New York State Plane Coordinate System, Long Island Zone, NAD 83, units feet (FIPS 3104)\n\n\nY_COORD_CD\nMidblock Y-coordinate for New York State Plane Coordinate System, Long Island Zone, NAD 83, units feet (FIPS 3104)\n\n\nLatitude\nLatitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326)\n\n\nLongitude\nLongitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326)\n\n\nLon_Lat\nLongitude and Latitude Coordinates for mapping\n\n\nBorough Boundaries\n\n\n\nCity Council Districts\n\n\n\nPolice Precincts\n\n\n\nZip Codes\n\n\n\nCommunity Districts\n\n\n\n\nFor my analysis I will only keep two columns:\n1.) PRECINCT and 2.) OCCUR_DATE\ndata &lt;- data %&gt;%\n  select(c(PRECINCT, OCCUR_DATE))\n\n\n\n\n\ntotal_n &lt;- data %&gt;%\n  group_by(year(OCCUR_DATE)) %&gt;%\n  count() %&gt;%\n  rename(year = `year(OCCUR_DATE)`)\nkable(total_n)\n\n\n\nyear\nn\n\n\n\n\n2006\n2055\n\n\n2007\n1887\n\n\n2008\n1959\n\n\n2009\n1828\n\n\n2010\n1912\n\n\n2011\n1939\n\n\n2012\n1717\n\n\n2013\n1339\n\n\n2014\n1464\n\n\n2015\n1434\n\n\n2016\n1208\n\n\n2017\n970\n\n\n2018\n958\n\n\n2019\n967\n\n\n2020\n1948\n\n\n2021\n2011\n\n\n2022\n1716\n\n\n\nggplot(total_n) +\n  aes(x = year, y = n) + geom_line() + geom_point()\n\nFrom 2006 the number shooting incidents continuously declined until it bottomed around 2018. Gun violence then surged in 2020 due to generally difficult economic conditions, which aggravated in the aftermath of the coronavirus epidemic.\n\n\n\n#calculate no. of incidents per police precinct\ntotal &lt;- data %&gt;% group_by(PRECINCT) %&gt;% count()\n#create a map\nprec_geo_url = 'https://opendata.arcgis.com/datasets/c35786feb0ac4d1b964f41f874f151c1_0.geojson'\n\n#download geojson of NYPD police precincts\ngeo &lt;- geojson_read(prec_geo_url, what = 'sp')\n\n#get geojson dataframe\ngeo_tidy &lt;- tidy(geo)\nThe polygons corresponding to the police precincts are numbered 1:77 in the geojson file. Therefore, we need to map the actual precinct codes in the data to this range.\ntotal$id &lt;- 1:77\ntotal$id &lt;- as.character(total$id)\nNow the number of incidents per precinct can be joined with the geodata.\ngeo_tidy_all &lt;- geo_tidy %&gt;%\n  left_join(total, by = 'id')\nThe geo_tidy dataframe is now ready to be used for the creation of the map.\nggplot(geo_tidy_all) + \n  geom_polygon(aes(x = long, y = lat, group = group, fill = n), color = 'black', size = 0.2) +\n  scale_fill_viridis_b(trans = \"log\", breaks = c(1, 5, 10, 50, 100, 500, 1000)) +\n  theme_void() +\n  labs(title = \"Cumulative spatial distribution of shooting incidents \\ngrouped by NYPD precinct\",\n       subtitle = \"2006 - 2022\") +\n  coord_map()\n\nkable(total %&gt;% arrange(desc(n)) %&gt;% select(c(PRECINCT, n)) %&gt;% head())\n\n\n\nPRECINCT\nn\n\n\n\n\n75\n1557\n\n\n73\n1452\n\n\n67\n1216\n\n\n44\n1020\n\n\n79\n1012\n\n\n47\n953\n\n\n\nGun violence is clearly spread over the whole city, with hot-spots in the Bronx and in East New York. The 75th and 73th precinct counted the highest number of shooting incidents over this period of time. The 75th precinct was once known as New York’s “Killing Ground”.\n\n\n\nnypp_id = read_csv(\"https://data.cityofnewyork.us/resource/kmub-vria.csv\")$precinct\n\n#calculate no. of incidents per police precinct\ntotal2006 &lt;- data %&gt;% \n  group_by(PRECINCT, year(OCCUR_DATE)) %&gt;% \n  rename(year = \"year(OCCUR_DATE)\") %&gt;%\n  count() %&gt;%\n  rename(Precinct = PRECINCT) %&gt;%\n  filter(year == 2006) %&gt;%\n  ungroup() %&gt;%\n  select(c(Precinct, n))\n\n#left join as there 0 incidents in some precincts\ntotal2006 &lt;- left_join(data.frame(Precinct = nypp_id),\n                       total2006,\n                       by = c('Precinct')) %&gt;%\n  mutate(n = ifelse(is.na(n), 0, n))\n\n#aligned with polygons\ntotal2006$id = as.character(1:77)\ngeo_tidy_2006 &lt;- geo_tidy %&gt;%\n  left_join(total2006, by = 'id')\nggplot(geo_tidy_2006) + \n  geom_polygon(aes(x = long, y = lat, group = group, fill = n), color = 'black', size = 0.2) +\n  scale_fill_viridis_b(trans = \"log\", breaks = c(1, 3, 5, 10, 30, 50, 100)) +\n  theme_void() +\n  labs(title = \"Spatial distribution of shootings incident grouped by NYPD precinct\",\n       subtitle = \"2006\") +\n  coord_map()\n\n\n\n\n#calculate no. of incidents per police precinct\ntotal2018 &lt;- data %&gt;% \n  group_by(PRECINCT, year(OCCUR_DATE)) %&gt;% \n  rename(year = \"year(OCCUR_DATE)\") %&gt;%\n  count() %&gt;%\n  rename(Precinct = PRECINCT) %&gt;%\n  filter(year == 2018) %&gt;%\n  ungroup() %&gt;%\n  select(c(Precinct, n))\n\n#left join as there 0 incidents in some precincts\ntotal2018 &lt;- left_join(data.frame(Precinct = nypp_id),\n                       total2018,\n                       by = c('Precinct')) %&gt;%\n  mutate(n = ifelse(is.na(n), 0, n))\n\n#aligned with polygons\ntotal2018$id = as.character(1:77)\ngeo_tidy_2018 &lt;- geo_tidy %&gt;%\n  left_join(total2018, by = 'id')\nggplot(geo_tidy_2018) + \n  geom_polygon(aes(x = long, y = lat, group = group, fill = n), color = 'black', size = 0.2) +\n  scale_fill_viridis_b(trans = \"log\", breaks = c(1, 3, 5, 10, 30, 50, 100)) +\n  theme_void() +\n  labs(title = \"Spatial distribution of shootings incident grouped by NYPD precinct\",\n       subtitle = \"2018\") +\n  coord_map()\n\nComparing the two maps from 2006 and 2018, clearly shows that there was a significant reduction in gun violence, also in and around the infamous 75th precinct.\n\n\n\n\nThe above findings are merely descriptive but do not investigate potential causes for the observed distribution and surge of gun violence in New York City. Further investigations should link the shooting data to socioeconomic data.",
    "crumbs": [
      "Projects",
      "Data Science",
      "NY Gun shootings"
    ]
  },
  {
    "objectID": "uncategorized_projects/nypd_shooting/nyc_shooting.html#potential-source-of-bias",
    "href": "uncategorized_projects/nypd_shooting/nyc_shooting.html#potential-source-of-bias",
    "title": " DS as a Field - NYPD shooting analysis report",
    "section": "",
    "text": "The data set contains only shooting incidents that were recorded by the police and is a convenience sample in the sense that unreported shootings are registered. As a consequence analyses of this data set are potentially underestimating the number of shootings.",
    "crumbs": [
      "Projects",
      "Data Science",
      "NY Gun shootings"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/web_scraping/web_scraping.html",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/web_scraping/web_scraping.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "This code won’t run in wowrkspace, but you can download it locally if you are interested in learning about how we obtained the URLs of the professors used in this project.\n\n!pip install selenium!pip install webdriver-manager\n\n\nfrom selenium import webdriverfrom selenium.webdriver.chrome.service import Servicefrom webdriver_manager.chrome import ChromeDriverManagerfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECimport requestsfrom bs4 import BeautifulSoupfrom lxml import etreefrom urllib.request import urlopen\n\n\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))driver.get(\"https://www.ratemyprofessors.com/search/teachers?query=?\")# Wait for initialize, in secondswait = WebDriverWait(driver, 8)wait.until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[5]/div/div/button')))button = driver.find_element('xpath','/html/body/div[5]/div/div/button')button.click()clicks = 0while clicks &lt; 140:    if clicks%10==0:        print(f'Clicked {clicks} times.')    wait = WebDriverWait(driver, 7)    time.sleep(3)    wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[4]/button')))    show_more = driver.find_element('xpath','//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[4]/button')    show_more.click()    clicks += 1cards = driver.find_elements(By.XPATH,'//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[3]/a[*]')driver.quit()profs = [i.get_attribute('href') for i in cards]"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html",
    "title": "Do students describe professors differently based on gender?",
    "section": "",
    "text": "Note: You can consult the solution of this live training in the file browser as notebook-solution.ipynb\nLanguage plays a crucial role in shaping our perceptions and attitudes towards gender in the workplace, in classrooms, and personal relationships. Studies have shown that gender bias in language can have a significant impact on the way people are perceived and treated.\nFor example, research has found that job advertisements that use masculine-coded language tend to attract more male applicants, while those that use feminine-coded language tend to attract more female applicants. Similarly, gendered language can perpetuate differences in the classroom.\nIn this project, we’ll using scraped student reviews from ratemyprofessors.com to identify differences in language commonly used for male vs. female professors, and explore subtleties in how language in the classroom can be gendered.\nThis excellent tool created by Ben Schmidt allows us to enter the words and phrases that we find in our analysis and explore them in more depth. We’ll do this at the end.\nCatalyst also does some incredible work on decoding gendered language.",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "Python",
      "NLP Gender Reviews",
      "Do students describe professors differently based on gender?"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "title": "Do students describe professors differently based on gender?",
    "section": "3a. What additional package imports are required for data visualization and NLP?",
    "text": "3a. What additional package imports are required for data visualization and NLP?\n\nimport numpy as np # For manipulating matrices during NLP\n\nimport nltk # Natural language toolkit\nfrom nltk.tokenize import word_tokenize # Used for breaking up strings of text (e.g. sentences) into words\nfrom nltk.stem.porter import PorterStemmer # Used to return the dictionary base of a word\nfrom nltk.tokenize import WhitespaceTokenizer # Used for breaking up strings of text (e.g. sentences) into words based on white space\n\nnltk.download('punkt')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # Used to count the occurences of words and phrases\nfrom sklearn.feature_extraction import text as sktext# Using to extrat features from text\n\n# For plotting\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set(style='white')\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /home/kantundpeterpan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n3b. How can we assign gender labels to professors?\nLet’s write a custom function that assigns a gender label to professors based on the pronouns most commontly used for him. Specifically: - If any of ['she', 'her', 'herself', 'shes'] occur more than 5 times across all reviews for that professor, we label the professor “F”. - If any of ['him', 'he', 'his', 'himself'] occur more than 5 times across all reviews for that professor, we label the professor “F”.\n\nfrom collections import Counter\n\n\ndef assign_pronoun(review_list):\n    \n    she_ps = ['she', 'her', 'herself', 'shes']\n    he_ps = ['him', 'he', 'his', 'himself']\n    \n    counters = [Counter(word_tokenize(r.lower())) for r in review_list]\n    \n    ### FEMALE\n    she_ps_counter = dict()\n    \n    for sp in she_ps:\n        she_ps_counter[sp] = 0\n        she_ps_counter[sp] = sum([c[sp] if sp in c.keys() else 0 for c in counters])\n        \n    she_ps_counts = np.array([she_ps_counter[sp] for sp in she_ps])\n    \n    if np.sum(she_ps_counts) &gt; 5:\n        return \"F\"\n    \n    ### MALE\n    he_ps_counter = dict()\n    \n    for hp in he_ps:\n        he_ps_counter[hp] = 0\n        he_ps_counter[hp] = sum([c[hp] if hp in c.keys() else 0 for c in counters])\n        \n    he_ps_counts = np.array([he_ps_counter[hp] for hp in he_ps])\n    \n    if np.sum(he_ps_counts) &gt; 5:\n        return \"M\"\n\n\nassign_pronoun(df.review.iloc[6])\n\n'M'\n\n\n\ndf['pronouns'] = df.review.apply(assign_pronoun)\n\n\ndf.pronouns.value_counts()\n\npronouns\nM    417\nF    139\nName: count, dtype: int64\n\n\n\n\n3c. Are there any initial differences between male and female professors based on their overall ratings?\nLet’s start with a barplot.\n\nplt.figure(figsize=(4,4))\nsns.barplot(data = df, x = 'pronouns', y = 'rating', estimator = 'median',\n            palette = 'magma')\nplt.show()\n\n/tmp/ipykernel_832826/2203816405.py:2: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data = df, x = 'pronouns', y = 'rating', estimator = 'median',\n\n\n\n\n\n\n\n\n\nA boxplot overlaid with a stripplot will give us a better sense of the distribution of the data.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(df, x = 'pronouns', y = 'rating', palette = 'magma')\nsns.stripplot(data = df, x = 'pronouns', y = 'rating', jitter = 0.2, color = 'lightblue',\n              edgecolor = 'k', linewidth=1)\nplt.show()\n\n/tmp/ipykernel_832826/1838852018.py:2: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(df, x = 'pronouns', y = 'rating', palette = 'magma')",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "Python",
      "NLP Gender Reviews",
      "Do students describe professors differently based on gender?"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "title": "Do students describe professors differently based on gender?",
    "section": "Task 3d. What are the most important words being used to describe professors in reviews?",
    "text": "Task 3d. What are the most important words being used to describe professors in reviews?\nLet’s write a custom function that tokenizes and lemmatizes our list of words. - Word tokenization: process of splitting text into individual words, called tokens. A common preprocessing step in natural language processing (NLP) so that text can be analyzed and processed more easily. Methods include whitespace tokenization, regular expression-based tokenization, and rule-based tokenization. We’ll be using the word_tokenize tokenizer from nltk, with all its defaults. - Lemmatization: process of reducing words to their base or dictionary form, called the lemma. Also a common pre-processing step in NLP, so that words with a common base form are treated the same way. For example, the lemma of “am” is “be”, of “running” is “run”, and of “mice” is “mouse”.\n\nPorterStemmer().stem(\"she\\'s\")\n\n\"she'\"\n\n\n\nword_tokenize('she\\'s a girl')\n\n['she', \"'s\", 'a', 'girl']\n\n\n\nimport string\n\n\ndef tokenize(text):\n    tk = WhitespaceTokenizer()\n    tokens = tk.tokenize(text)\n    stems = []\n    for item in tokens:\n        stems.append(PorterStemmer().stem(item).strip(string.punctuation))\n    return stems\n\nLet’s import a list of stop words, which are common English words that we will be ignoring in our analysis. sklearn provides a common list of stop words, and we can append additional words to this list. Below, we append pronouns, along with the words “class” and “student”. Feel free to add any additional words you’d like to ignore to this list later on as you try to build upon this analysis!\n\nmy_stop_words = sktext.ENGLISH_STOP_WORDS.union([\"he\",\"she\",\"his\",\"her\",\n                                              \"himself\",\"herself\", \"hers\",\"shes\"\n                                              \"class\",\"student\", 'man', 'woman', 'girl',\n                                                 'guy', 'lady', 'mr', 'mrs', 'ms'])\nmy_stop_words = my_stop_words.union([tokenize(word)[0] for word in my_stop_words])\n\nFor the purpose of analyzing review texts, we want to move from having one row for each professor to one row for each review. Lets do this with .explode() from pandas.\n\ndf_quality = df[(df['review'].apply(len) == df['quality'].apply(len))]\nq = df_quality[['pronouns','review','quality']].explode(['review','quality'], ignore_index=True).dropna()\nq['quality'] = q['quality'].astype(float)\n\n\nq.head(5)\n\n\n\n\n\n\n\n\npronouns\nreview\nquality\n\n\n\n\n0\nF\nGood experience for a class online. It was unc...\n4.0\n\n\n1\nF\nHonestly she didnt teach good at all and she w...\n2.0\n\n\n2\nF\nI think if you go by word for word in the modu...\n1.0\n\n\n3\nF\nTook her online class CSS64. We started buildi...\n1.0\n\n\n4\nF\nTook her for a late start hybrid class (Bus43)...\n3.0\n\n\n\n\n\n\n\nTFIDF vectorization is the process of assigning scores to each review in a document based on how frequently the word occurs, normalized by how frequently the word occurs in the dataset overall.\nWe’ll use TfidfVectorizer() to generate these scores. This will return a matrix, with as many rows as reviews, and as many columns as words in our dataset.\n\nvec = TfidfVectorizer(\n    tokenizer = tokenize,\n    stop_words = list(my_stop_words),\n    ngram_range = (1,4)\n)\nX = vec.fit_transform(q.review)\nfeature_names = vec.get_feature_names_out()\n\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anywh', 'becau', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh'] not in stop_words.\n  warnings.warn(\n\n\n\nfeature_names.shape\n\n(365516,)\n\n\n\nnp.random.choice(feature_names, size = 10)\n\narray([\"sense you'v\", 'work easy week class',\n       'technology(blackboard),noon lac know', 'just read quizz',\n       \"can't drop day\", 'lot question understand materi', 'class ad',\n       'languag pain', 'style rel scatter unorganized',\n       \"prof l'heureux help\"], dtype=object)\n\n\nX is a sparse matrix. We’ll now move into filtering X for: - Rows with male professors and reviews of high quality - Rows with female professors and reviews of high quality - Rows with male professors and reviews of low quality - Rows with female professors and reviews of low quality\nWe can explore feature importance in each of these to get a sense of which words and phrases are coming up most often in the data.\n\nm_pos = X[q.pronouns.eq('M') & q.quality.ge(4.5)]\n\n\nm_pos = X[q.pronouns.eq('M') & q.quality.ge(4.5)]\nf_pos = X[q.pronouns.eq('F') & q.quality.ge(4.5)]\nm_neg = X[q.pronouns.eq('M') & q.quality.le(2.5)]\nf_neg = X[q.pronouns.eq('F') & q.quality.le(2.5)]\n\n\nnp.unique(np.array(m_pos[0,:].todense()))\n\narray([0.        , 0.04840186, 0.05048048, 0.05840671, 0.0588915 ,\n       0.06440657, 0.07216968, 0.07743822, 0.08628339, 0.0890904 ,\n       0.09454997, 0.09609708, 0.0982585 , 0.09872669, 0.10101388,\n       0.10570444, 0.11337177, 0.12547472, 0.13281775, 0.13699447,\n       0.146295  , 0.15107613, 0.15781475])\n\n\nLet’s have a look at what language students are using to describe male professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(m_pos.sum(axis = 0)))[0,::-1]\nm_pos_features = feature_names[importance[:300]]\n\nPrint out the 25 most important features\n\nm_pos_features[:25]\n\narray(['comment', 'great', 'class', 'teacher', 'best', 'professor',\n       'prof', 'good', 'help', 'realli', 'make', 'easi', 'love',\n       'great teacher', 'awesom', 'know', 'learn', '', 'work', 'lot',\n       'lectur', 'amaz', 'cours', 'nice', 'test'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively.\n\nimportance = np.argsort(np.asarray(f_pos.sum(axis = 0)))[0,::-1]\nf_pos_features = feature_names[importance[:300]]\nf_pos_features[:25]\n\narray(['comment', 'great', 'class', 'teacher', 'prof', 'help',\n       'professor', 'best', 'good', 'easi', 'realli', 'work', 'nice',\n       'lot', 'love', 'make', 'learn', 'helpful', 'great teacher', '',\n       'amaz', 'great prof', 'cours', 'lectur', 'hard'], dtype=object)\n\n\nIt should be interesting if there are words exclusively used for one gender\n\n#male\nonly_m_pos = ~np.in1d(m_pos_features, f_pos_features)\nm_pos_features[only_m_pos][:25]\n\n/tmp/ipykernel_832826/3886134522.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_m_pos = ~np.in1d(m_pos_features, f_pos_features)\n\n\narray(['fantast', 'excel teacher', 'topic', 'awesom professor',\n       'excel professor', 'u', 'brilliant', 'realli enjoy', '2', 'old',\n       'amaz professor', \"professor i'v\", \"teacher i'v\", 'mark', 'b',\n       'hot', 'review', 'genuin', 'bore', \"he'll\", 'reason', 'hours',\n       'want learn', 'overal', 'feel'], dtype=object)\n\n\n\n#female\nonly_f_pos = ~np.isin(f_pos_features, m_pos_features)\nf_pos_features[only_f_pos][:25]\n\narray(['extra credit', 'especi', 'spanish', \"she'll\", 'assignments',\n       'realli nice', 'offer', 'help prof', 'class nice', 'succeed',\n       'realli want', 'alot', 'onlin class', 'knowledgable', 'group',\n       'good lectur', 'attention', 'nice help', \"i'd\", 'real world',\n       'easi grade', 'awsom', 'account', 'guid', 'semester'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe male professors negatively.\n\nimportance = np.argsort(np.asarray(m_neg.sum(axis = 0)))[0,::-1]\nm_neg_features = feature_names[importance[:300]]\nm_neg_features[:25]\n\narray(['comment', 'class', 'teach', 'hard', 'test', 'worst', 'professor',\n       'teacher', 'lectur', '', 'time', 'know', 'like', 'grade', \"don't\",\n       \"doesn't\", 'just', 'prof', 'avoid', 'bore', 'question', 'good',\n       'make', 'doe', 'read'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors negatively.\n\nimportance = np.argsort(np.asarray(f_neg.sum(axis = 0)))[0,::-1]\nf_neg_features = feature_names[importance[:300]]\nf_neg_features[:25]\n\narray(['comment', 'class', 'worst', 'grade', 'teacher', 'hard', 'teach',\n       \"don't\", \"doesn't\", 'help', 'like', 'test', '', 'just', 'time',\n       'work', 'professor', 'good', 'doe', 'question', 'make', 'know',\n       'horribl', 'learn', 'unclear'], dtype=object)\n\n\nSame analysis for exclusive words:\n\n#male\nonly_m_neg = ~np.in1d(m_neg_features, f_neg_features)\nm_neg_features[only_m_neg][:25]\n\n/tmp/ipykernel_832826/728473888.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_m_neg = ~np.in1d(m_neg_features, f_neg_features)\n\n\narray(['speak', 'hard understand', 'great', 'arrog', 'hear', 'costs',\n       'smart', 'hardest', 'taught', 'listen', 'rambl', 'english',\n       'exampl', 'let', 'sit', 'incred', 'wrote', 'knowledg', 'possible',\n       'probabl', 'avoid costs', 'gpa', 'offic', '1', \"can't teach\"],\n      dtype=object)\n\n\n\n#female\nonly_f_neg = ~np.in1d(f_neg_features, m_neg_features)\nf_neg_features[only_f_neg][:25]\n\n/tmp/ipykernel_832826/1584707764.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_f_neg = ~np.in1d(f_neg_features, m_neg_features)\n\n\narray(['late', 'disorgan', 'gave', 'advis', 'annoy', 'feedback', 'cost',\n       'agre', 'helpful', 'disorganized', 'nice person', 'avoid cost',\n       'unorganized', 'opinion', 'slow', 'quit', 'colleg', 'honestli',\n       'fan', 'papers', 'spanish', 'easi class', 'favorites', 'instead',\n       '5'], dtype=object)",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "Python",
      "NLP Gender Reviews",
      "Do students describe professors differently based on gender?"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#congratulations-on-making-it-to-the-end",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#congratulations-on-making-it-to-the-end",
    "title": "Do students describe professors differently based on gender?",
    "section": "Congratulations on making it to the end!",
    "text": "Congratulations on making it to the end!\n\nWhere to from here?\n\nWe can feed these words into Ben Schmidt’s tool to derive insights by field.\nIf you’re interested in learning more about web scraping, take our courses on Web Scraping in Python\nIf you’re intersted in diving in to the world of Natural Language Processing, explore our skill track.",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "Python",
      "NLP Gender Reviews",
      "Do students describe professors differently based on gender?"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#parse-lists-generated-by-gemini",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#parse-lists-generated-by-gemini",
    "title": "Do students describe professors differently based on gender?",
    "section": "Parse lists generated by GEMINI",
    "text": "Parse lists generated by GEMINI\n\nFunction parse_gemini_list\n\ndef parse_gemini_list(s: str) -&gt; list:\n    \"\"\"parse comma separated list of words remove annotations in parentheses\"\"\"\n    new = [w.strip() for w in s.split(',')]\n    for i,n in enumerate(new):\n        if '(' in n:\n            new[i] = n.split('(')[0]\n    \n    new = np.unique([x for word in new for x in tokenize(word)])\n            \n    return new\n\n\n\nRead and parse files\n\nwith open('w_pos_lex', 'r') as f:\n    w_pos_lex = parse_gemini_list(f.read())\n\n\nwith open('w_neg_lex', 'r') as f:\n    w_neg_lex = parse_gemini_list(f.read())\n\n\nwith open('m_pos_lex', 'r') as f:\n    m_pos_lex = parse_gemini_list(f.read())\n\n\nwith open('m_neg_lex', 'r') as f:\n    m_neg_lex = parse_gemini_list(f.read())\n\n\n\nCast unique lexicons for each gender and review sentiment\n\nw_pos_lex_uni = w_pos_lex[~np.isin(w_pos_lex, m_pos_lex)]\n\n\nm_pos_lex_uni = m_pos_lex[~np.isin(m_pos_lex, w_pos_lex)]\n\n\nw_neg_lex_uni = w_neg_lex[~np.isin(w_neg_lex, m_neg_lex)]\n\n\nm_neg_lex_uni = m_neg_lex[~np.isin(m_neg_lex, w_neg_lex)]\n\n\nstereo_count_w_pos = q.loc[q.pronouns.eq('F') & q.quality.ge(4.5)].review.apply(tokenize).apply(\n    np.isin, args = (w_pos_lex_uni,)).apply(np.sum)\n\n\ndist_w_pos = stereo_count_w_pos.value_counts()\ndist_w_pos.plot(kind = 'bar')\n\n\n\n\n\n\n\n\n\nstereo_count_m_pos = q.loc[q.pronouns.eq('M') & q.quality.ge(4.5)].review.apply(tokenize).apply(\n    np.isin, args = (m_pos_lex_uni,)).apply(np.sum)\n\n\ndist_m_pos = stereo_count_m_pos.value_counts()\n\n\ndist_m_pos.plot(kind = 'bar')\n\n\n\n\n\n\n\n\n\nstereo_count_w_neg = q.loc[q.pronouns.eq('F') & q.quality.le(2.5)].review.apply(tokenize).apply(\n    np.isin, args = (w_neg_lex_uni,)).apply(np.sum)\n\n\ndist_w_neg = stereo_count_w_neg.value_counts().sort_index()\n\n\ndist_w_neg.plot(kind = 'bar')\n\n\n\n\n\n\n\n\n\nstereo_count_m_neg = q.loc[q.pronouns.eq('M') & q.quality.le(2.5)].review.apply(tokenize).apply(\n    np.isin, args = (m_neg_lex_uni,)).apply(np.sum)\n\n\ndist_m_neg = stereo_count_m_neg.value_counts()\n\n\ndist_m_neg.plot(kind = 'bar')\n\n\n\n\n\n\n\n\n\ndist_m_neg\n\nreview\n0    2218\n1     240\n2      37\n3      10\n4       9\n5       1\nName: count, dtype: int64\n\n\n\nfrom scipy.stats import chi2_contingency\n\n\ntable = np.zeros((2,  max(dist_w_neg.shape[0], dist_m_neg.shape[0])), dtype = int)\ntable[0, :dist_w_neg.shape[0]] = dist_w_neg.values\ntable[1, :dist_m_neg.shape[0]] = dist_m_neg.values\n\n\ntable\n\narray([[ 278,  229,  146,   86,   37,   20,    3,    1,    1],\n       [2218,  240,   37,   10,    9,    1,    0,    0,    0]])\n\n\n\nchi2_table = pd.DataFrame(table, index = ['F', 'M'])\n\n\nchi2_table\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nF\n278\n229\n146\n86\n37\n20\n3\n1\n1\n\n\nM\n2218\n240\n37\n10\n9\n1\n0\n0\n0\n\n\n\n\n\n\n\n\nchi2_contingency(chi2_table)\n\nChi2ContingencyResult(statistic=np.float64(1073.2259818314606), pvalue=np.float64(2.3184709261716882e-226), dof=8, expected_freq=array([[6.02924005e+02, 1.13289807e+02, 4.42047648e+01, 2.31893848e+01,\n        1.11115802e+01, 5.07267793e+00, 7.24668275e-01, 2.41556092e-01,\n        2.41556092e-01],\n       [1.89307600e+03, 3.55710193e+02, 1.38795235e+02, 7.28106152e+01,\n        3.48884198e+01, 1.59273221e+01, 2.27533172e+00, 7.58443908e-01,\n        7.58443908e-01]]))\n\n\n\ntable = np.zeros((2, max(dist_w_pos.shape[0], dist_m_pos.shape[0])), dtype = int)\ntable[0, :dist_w_pos.shape[0]] = dist_w_pos.values\ntable[1, :dist_m_pos.shape[0]] = dist_m_pos.values\n\n\nchi2_table = pd.DataFrame(table, index = ['F', 'M'])\n\n\nchi2_contingency(chi2_table)\n\nChi2ContingencyResult(statistic=np.float64(35.680653072523995), pvalue=np.float64(1.1002520334386392e-06), dof=5, expected_freq=array([[6.76990876e+02, 2.24479927e+02, 6.08759124e+01, 9.13138686e+00,\n        1.26824818e+00, 2.53649635e-01],\n       [1.99200912e+03, 6.60520073e+02, 1.79124088e+02, 2.68686131e+01,\n        3.73175182e+00, 7.46350365e-01]]))",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "Python",
      "NLP Gender Reviews",
      "Do students describe professors differently based on gender?"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/london_transport/notebook.html",
    "href": "datacamp_projects/workspace/SQL/london_transport/notebook.html",
    "title": "SQL - London transportation dataset",
    "section": "",
    "text": "tower bridge\n\n\nLondon, or as the Romans called it “Londonium”! Home to over 8.5 million residents who speak over 300 languages. While the City of London is a little over one square mile (hence its nickname “The Square Mile”), Greater London has grown to encompass 32 boroughs spanning a total area of 606 square miles!\n\n\n\nunderground train leaving a platform\n\n\nGiven the city’s roads were originally designed for horse and cart, this area and population growth has required the development of an efficient public transport system! Since the year 2000, this has been through the local government body called Transport for London, or TfL, which is managed by the London Mayor’s office. Their remit covers the London Underground, Overground, Docklands Light Railway (DLR), buses, trams, river services (clipper and Emirates Airline cable car), roads, and even taxis.\nThe Mayor of London’s office make their data available to the public here. In this project, you will work with a slightly modified version of a dataset containing information about public transport journey volume by transport type.\nThe data has been loaded into a Google BigQuery database called TFL with a single table called JOURNEYS, including the following data:",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "London Transport",
      "SQL - London transportation dataset"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/london_transport/notebook.html#most-popular-transport-types-by-total-journeys-over-whole-dataset",
    "href": "datacamp_projects/workspace/SQL/london_transport/notebook.html#most-popular-transport-types-by-total-journeys-over-whole-dataset",
    "title": "SQL - London transportation dataset",
    "section": "Most popular transport types by total journeys over whole dataset",
    "text": "Most popular transport types by total journeys over whole dataset\nWhat are the most popular transport types, measured by the total number of journeys?\nThe output should contain two columns, 1) journey_type and 2) total_journeys_millions, and be sorted by the second column in descending order. Save the query as most_popular_transport_types.\n\n-- most_popular_transport_types\nSELECT JOURNEY_TYPE, SUM(JOURNEYS_MILLIONS) as TOTAL_JOURNEYS_MILLIONS\nFROM TFL.JOURNEYS\nGROUP BY JOURNEY_TYPE\nORDER BY TOTAL_JOURNEYS_MILLIONS DESC;\n\n\n\n\n\n\n\n\nJOURNEY_TYPE\nTOTAL_JOURNEYS_MILLIONS\n\n\n\n\n0\nBus\n24905.193947\n\n\n1\nUnderground & DLR\n15020.466544\n\n\n2\nOverground\n1666.845666\n\n\n3\nTfL Rail\n411.313421\n\n\n4\nTram\n314.689875\n\n\n5\nEmirates Airline\n14.583718",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "London Transport",
      "SQL - London transportation dataset"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/london_transport/notebook.html#best-five-months-for-emirates-airlines",
    "href": "datacamp_projects/workspace/SQL/london_transport/notebook.html#best-five-months-for-emirates-airlines",
    "title": "SQL - London transportation dataset",
    "section": "Best five months for Emirates Airlines",
    "text": "Best five months for Emirates Airlines\nWhich five months and years were the most popular for the Emirates Airline?\nReturn an output containing month, year, and journeys_millions, with the latter rounded to two decimal places and aliased as rounded_journeys_millions. Exclude null values and order the results by 1) rounded_journeys_millions in descending order and 2) year in ascending order, saving the result as emirates_airline_popularity.\n\n-- emirates_airline_popularity\nSELECT YEAR, MONTH, ROUND(SUM(JOURNEYS_MILLIONS),2) as ROUNDED_JOURNEYS_MILLIONS\nFROM TFL.JOURNEYS\nWHERE JOURNEY_TYPE = 'Emirates Airline' AND JOURNEY_TYPE IS NOT NULL\nGROUP BY YEAR, MONTH, JOURNEY_TYPE\nORDER BY ROUNDED_JOURNEYS_MILLIONS DESC, YEAR ASC\nLIMIT 5\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nROUNDED_JOURNEYS_MILLIONS\n\n\n\n\n0\n2012\n5\n0.53\n\n\n1\n2012\n6\n0.38\n\n\n2\n2012\n4\n0.24\n\n\n3\n2013\n5\n0.19\n\n\n4\n2015\n5\n0.19",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "London Transport",
      "SQL - London transportation dataset"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/london_transport/notebook.html#five-worst-years-for-london-underground",
    "href": "datacamp_projects/workspace/SQL/london_transport/notebook.html#five-worst-years-for-london-underground",
    "title": "SQL - London transportation dataset",
    "section": "Five worst years for London Underground",
    "text": "Five worst years for London Underground\nFind the five years with the lowest volume of Underground & DLR journeys, saving as least_popular_years_tube. The results should contain the columns year, journey_type, and total_journeys_millions.\n\n-- least_popular_years_tube\nSELECT YEAR, JOURNEY_TYPE, SUM(JOURNEYS_MILLIONS) as TOTAL_JOURNEYS_MILLIONS\nFROM TFL.JOURNEYS\nWHERE JOURNEY_TYPE = 'Underground & DLR'\nGROUP BY YEAR, JOURNEY_TYPE\nORDER BY TOTAL_JOURNEYS_MILLIONS ASC\nLIMIT 5\n\n\n\n\n\n\n\n\nYEAR\nJOURNEY_TYPE\nTOTAL_JOURNEYS_MILLIONS\n\n\n\n\n0\n2020\nUnderground & DLR\n310.179316\n\n\n1\n2021\nUnderground & DLR\n748.452544\n\n\n2\n2022\nUnderground & DLR\n1064.859009\n\n\n3\n2010\nUnderground & DLR\n1096.145588\n\n\n4\n2011\nUnderground & DLR\n1156.647654",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "London Transport",
      "SQL - London transportation dataset"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/international_debt/notebook.html",
    "href": "datacamp_projects/workspace/SQL/international_debt/notebook.html",
    "title": "SQL - International debt statistics",
    "section": "",
    "text": "Humans not only take debts to manage necessities. A country may also take debt to manage its economy. For example, infrastructure spending is one costly ingredient required for a country’s citizens to lead comfortable lives. The World Bank is the organization that provides debt to countries.\nIn this project, you are going to analyze international debt data collected by The World Bank. The dataset contains information about the amount of debt (in USD) owed by developing countries across several categories. You are going to find the answers to the following questions:\n\nWhat is the number of distinct countries present in the database?\nWhat country has the highest amount of debt?\nWhat country has the lowest amount of repayments?\n\nBelow is a description of the table you will be working with:",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "International Debt",
      "SQL - International debt statistics"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/international_debt/notebook.html#no.-of-distinct-countries",
    "href": "datacamp_projects/workspace/SQL/international_debt/notebook.html#no.-of-distinct-countries",
    "title": "SQL - International debt statistics",
    "section": "No. of distinct countries",
    "text": "No. of distinct countries\nWhat is the number of distinct countries present in the database? The output should be single row and column aliased as total_distinct_countries. Save the query as num_distinct_countries.\n\n-- num_distinct_countries \nSELECT COUNT(DISTINCT country_name) as total_distinct_countries\nFROM international_debt\n\n\n\n\n\n\n\n\ntotal_distinct_countries\n\n\n\n\n0\n124",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "International Debt",
      "SQL - International debt statistics"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/international_debt/notebook.html#country-with-highest-debt",
    "href": "datacamp_projects/workspace/SQL/international_debt/notebook.html#country-with-highest-debt",
    "title": "SQL - International debt statistics",
    "section": "Country with highest debt",
    "text": "Country with highest debt\nWhat country has the highest amount of debt? Your output should contain two columns: country_name and total_debt and one row. Save the query as highest_debt_country.\n\n-- highest_debt_country \nSELECT country_name, SUM(debt) as total_debt\nFROM international_debt\nGROUP BY country_name\nORDER BY total_debt DESC\nLIMIT 1\n\n\n\n\n\n\n\n\ncountry_name\ntotal_debt\n\n\n\n\n0\nChina\n2.857935e+11",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "International Debt",
      "SQL - International debt statistics"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/international_debt/notebook.html#county-with-lowest-amount-of-principal-repayments",
    "href": "datacamp_projects/workspace/SQL/international_debt/notebook.html#county-with-lowest-amount-of-principal-repayments",
    "title": "SQL - International debt statistics",
    "section": "County with lowest amount of principal repayments",
    "text": "County with lowest amount of principal repayments\nWhat country has the lowest amount of principal repayments (indicated by the “DT.AMT.DLXF.CD” indicator code)? The output table should contain three columns: country_name, indicator_name, and lowest_repayment and one row, saved in the query lowest_principal_repayment.\n\n-- lowest_principal_repayment \nSELECT country_name, indicator_name, MAX(debt) as lowest_repayment\nFROM international_debt\nWHERE indicator_code = 'DT.AMT.DLXF.CD'\nGROUP BY country_name, indicator_name\nORDER BY lowest_repayment ASC\nLIMIT 1\n\n\n\n\n\n\n\n\ncountry_name\nindicator_name\nlowest_repayment\n\n\n\n\n0\nTimor-Leste\nPrincipal repayments on external debt, long-te...\n825000",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Workspace",
      "SQL",
      "International Debt",
      "SQL - International debt statistics"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "",
    "text": "So you think you can classify text? How about tweets? In this notebook, we’ll take a dive into the world of social media text classification by investigating how to properly classify tweets from two prominent North American politicians: Donald Trump and Justin Trudeau.\n\n\n\nPhoto Credit: Executive Office of the President of the United States\n\n\nTweets pose specific problems to NLP, including the fact they are shorter texts. There are also plenty of platform-specific conventions to give you hassles: mentions, #hashtags, emoji, links and short-hand phrases (ikr?). Can we overcome those challenges and build a useful classifier for these two tweeters? Yes! Let’s get started.\n\n\nTo begin, we will import all the tools we need from scikit-learn. We will need to properly vectorize our data (CountVectorizer and TfidfVectorizer). And we will also want to import some models, including MultinomialNB from the naive_bayes module, LinearSVC from the svm module and PassiveAggressiveClassifier from the linear_model module. Finally, we’ll need sklearn.metrics and train_test_split and GridSearchCV from the model_selection module to evaluate and optimize our model.\n\n\n# Set seed for reproducibility\nimport random; random.seed(53)\n\n# Import all we need from sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Trump Trudeau Tweets",
      "Trump vs. Trudeau: Tweet classification"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introduction-and-imports",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introduction-and-imports",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "",
    "text": "So you think you can classify text? How about tweets? In this notebook, we’ll take a dive into the world of social media text classification by investigating how to properly classify tweets from two prominent North American politicians: Donald Trump and Justin Trudeau.\n\n\n\nPhoto Credit: Executive Office of the President of the United States\n\n\nTweets pose specific problems to NLP, including the fact they are shorter texts. There are also plenty of platform-specific conventions to give you hassles: mentions, #hashtags, emoji, links and short-hand phrases (ikr?). Can we overcome those challenges and build a useful classifier for these two tweeters? Yes! Let’s get started.\n\n\nTo begin, we will import all the tools we need from scikit-learn. We will need to properly vectorize our data (CountVectorizer and TfidfVectorizer). And we will also want to import some models, including MultinomialNB from the naive_bayes module, LinearSVC from the svm module and PassiveAggressiveClassifier from the linear_model module. Finally, we’ll need sklearn.metrics and train_test_split and GridSearchCV from the model_selection module to evaluate and optimize our model.\n\n\n# Set seed for reproducibility\nimport random; random.seed(53)\n\n# Import all we need from sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Trump Trudeau Tweets",
      "Trump vs. Trudeau: Tweet classification"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#transforming-our-collected-data",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#transforming-our-collected-data",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "2. Transforming our collected data",
    "text": "2. Transforming our collected data\n\nTo begin, let’s start with a corpus of tweets which were collected in November 2017. They are available in CSV format. We’ll use a Pandas DataFrame to help import the data and pass it to scikit-learn for further processing.\n\n\nSince the data has been collected via the Twitter API and not split into test and training sets, we’ll need to do this. Let’s use train_test_split() with random_state=53 and a test size of 0.33, just as we did in the DataCamp course. This will ensure we have enough test data and we’ll get the same results no matter where or when we run this code.\n\n\nimport pandas as pd\n\n# Load data\ntweet_df = pd.read_csv('datasets/tweets.csv')\n\n# Create target\ny = tweet_df['author']\nX = tweet_df.drop('author', axis = 1)\n\n# Split training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 53, test_size = 0.33)",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Trump Trudeau Tweets",
      "Trump vs. Trudeau: Tweet classification"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#vectorize-the-tweets",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#vectorize-the-tweets",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "3. Vectorize the tweets",
    "text": "3. Vectorize the tweets\n\nWe have the training and testing data all set up, but we need to create vectorized representations of the tweets in order to apply machine learning.\n\n\nTo do so, we will utilize the CountVectorizer and TfidfVectorizer classes which we will first need to fit to the data.\n\n\nOnce this is complete, we can start modeling with the new vectorized tweets!\n\n\n# Initialize count vectorizer\ncount_vectorizer = CountVectorizer(max_df = 0.9, min_df = 0.05, stop_words = 'english')\ncount_vectorizer.fit(X_train['status'])\n\n# Create count train and test variables\ncount_train = count_vectorizer.transform(X_train['status'])\ncount_test = count_vectorizer.transform(X_test['status'])\n\n# Initialize tfidf vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.9, min_df = 0.05)\\\n                    .fit(X_train['status'])\n\n# Create tfidf train and test variables\ntfidf_train = tfidf_vectorizer.transform(X_train['status'])\ntfidf_test = tfidf_vectorizer.transform(X_test['status'])",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Trump Trudeau Tweets",
      "Trump vs. Trudeau: Tweet classification"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#training-a-multinomial-naive-bayes-model",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#training-a-multinomial-naive-bayes-model",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "4. Training a multinomial naive Bayes model",
    "text": "4. Training a multinomial naive Bayes model\n\nNow that we have the data in vectorized form, we can train the first model. Investigate using the Multinomial Naive Bayes model with both the CountVectorizer and TfidfVectorizer data. Which do will perform better? How come?\n\n\nTo assess the accuracies, we will print the test sets accuracy scores for both models.\n\n\n# Create a MulitnomialNB model\ntfidf_nb = MultinomialNB().fit(tfidf_train, y_train)\n\n# ... Train your model here ...\n\n# Run predict on your TF-IDF test data to get your predictions\ntfidf_nb_pred = tfidf_nb.predict(tfidf_test)\n\n# Calculate the accuracy of your predictions\ntfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n\n# Create a MulitnomialNB model\ncount_nb = MultinomialNB().fit(count_train, y_train)\n# ... Train your model here ...\n\n# Run predict on your count test data to get your predictions\ncount_nb_pred = count_nb.predict(count_test)\n\n# Calculate the accuracy of your predictions\ncount_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\n\nprint('NaiveBayes Tfidf Score: ', tfidf_nb_score)\nprint('NaiveBayes Count Score: ', count_nb_score)\n\nNaiveBayes Tfidf Score:  0.803030303030303\nNaiveBayes Count Score:  0.7954545454545454",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Trump Trudeau Tweets",
      "Trump vs. Trudeau: Tweet classification"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#evaluating-our-model-using-a-confusion-matrix",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#evaluating-our-model-using-a-confusion-matrix",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "5. Evaluating our model using a confusion matrix",
    "text": "5. Evaluating our model using a confusion matrix\n\nWe see that the TF-IDF model performs better than the count-based approach. Based on what we know from the NLP fundamentals course, why might that be? We know that TF-IDF allows unique tokens to have a greater weight - perhaps tweeters are using specific important words that identify them! Let’s continue the investigation.\n\n\nFor classification tasks, an accuracy score doesn’t tell the whole picture. A better evaluation can be made if we look at the confusion matrix, which shows the number correct and incorrect classifications based on each class. We can use the metrics, True Positives, False Positives, False Negatives, and True Negatives, to determine how well the model performed on a given class. How many times was Trump misclassified as Trudeau?\n\n\n%matplotlib inline\n\nfrom datasets.helper_functions import plot_confusion_matrix\n\n# Calculate the confusion matrices for the tfidf_nb model and count_nb models\ntfidf_nb_cm = metrics.confusion_matrix(y_test, tfidf_nb_pred)\ncount_nb_cm = metrics.confusion_matrix(y_test, count_nb_pred)\n\n# Plot the tfidf_nb_cm confusion matrix\nplot_confusion_matrix(tfidf_nb_cm, classes=['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"TF-IDF NB Confusion Matrix\")\n\n# Plot the count_nb_cm confusion matrix without overwriting the first plot \nplot_confusion_matrix(count_nb_cm, classes=['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"Count NB Confusion matrix\", figure=1)\n\nConfusion matrix, without normalization\nConfusion matrix, without normalization",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Trump Trudeau Tweets",
      "Trump vs. Trudeau: Tweet classification"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#trying-out-another-classifier-linear-svc",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#trying-out-another-classifier-linear-svc",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "6. Trying out another classifier: Linear SVC",
    "text": "6. Trying out another classifier: Linear SVC\n\nSo the Bayesian model only has one prediction difference between the TF-IDF and count vectorizers – fairly impressive! Interestingly, there is some confusion when the predicted label is Trump but the actual tweeter is Trudeau. If we were going to use this model, we would want to investigate what tokens are causing the confusion in order to improve the model.\n\n\nNow that we’ve seen what the Bayesian model can do, how about trying a different approach? LinearSVC is another popular choice for text classification. Let’s see if using it with the TF-IDF vectors improves the accuracy of the classifier!\n\n\n# Create a LinearSVM model\ntfidf_svc = LinearSVC().fit(tfidf_train, y_train)\n\n# ... Train your model here ...\n\n# Run predict on your tfidf test data to get your predictions\ntfidf_svc_pred = tfidf_svc.predict(tfidf_test)\n\n# Calculate your accuracy using the metrics module\ntfidf_svc_score = metrics.accuracy_score(y_test, tfidf_svc_pred)\n\nprint(\"LinearSVC Score:   %0.3f\" % tfidf_svc_score)\n\n# Calculate the confusion matrices for the tfidf_svc model\nsvc_cm = metrics.confusion_matrix(y_test, tfidf_svc_pred)\n\n# Plot the confusion matrix using the plot_confusion_matrix function\nplot_confusion_matrix(svc_cm, classes = ['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"TF-IDF LinearSVC Confusion Matrix\")\n\nLinearSVC Score:   0.841\nConfusion matrix, without normalization",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Trump Trudeau Tweets",
      "Trump vs. Trudeau: Tweet classification"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introspecting-our-top-model",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introspecting-our-top-model",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "7. Introspecting our top model",
    "text": "7. Introspecting our top model\n\nWow, the LinearSVC model is even better than the Multinomial Bayesian one. Nice work! Via the confusion matrix we can see that, although there is still some confusion where Trudeau’s tweets are classified as Trump’s, the False Positive rate is better than the previous model. So, we have a performant model, right?\n\n\nWe might be able to continue tweaking and improving all of the previous models by learning more about parameter optimization or applying some better preprocessing of the tweets.\n\n\nNow let’s see what the model has learned. Using the LinearSVC Classifier with two classes (Trump and Trudeau) we can sort the features (tokens), by their weight and see the most important tokens for both Trump and Trudeau. What are the most Trump-like or Trudeau-like words? Did the model learn something useful to distinguish between these two men?\n\n\nfrom datasets.helper_functions import plot_and_return_top_features\n\n# Import pprint from pprint\nfrom pprint import pprint\n\n# Get the top features using the plot_and_return_top_features function and your top model and tfidf vectorizer\ntop_features = plot_and_return_top_features(tfidf_svc, tfidf_vectorizer)\n\n# pprint the top features\npprint(top_features)\n\n\n\n\n\n\n\n\n[(-0.3959834966911922, 'great'),\n (-0.24645580091925237, 'thank'),\n (0.06257998949180026, 'president'),\n (0.48211745246750215, 'https'),\n (0.5960555762649068, 'vietnam'),\n (0.6155609686456073, 'amp'),\n (0.7725857577713344, 'le'),\n (0.8213735137856691, 'les'),\n (0.8286549508433744, 'today'),\n (1.1869092357816051, 'du'),\n (1.3143518952322126, 'pour'),\n (1.4122560793508427, 'nous'),\n (1.4612710235935042, 'rt'),\n (1.4991808273544363, 'et'),\n (1.50564270245237, 'la'),\n (1.6567934485943738, 'canada')]",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Trump Trudeau Tweets",
      "Trump vs. Trudeau: Tweet classification"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#bonus-can-you-write-a-trump-or-trudeau-tweet",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#bonus-can-you-write-a-trump-or-trudeau-tweet",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "8. Bonus: can you write a Trump or Trudeau tweet?",
    "text": "8. Bonus: can you write a Trump or Trudeau tweet?\n\nSo, what did our model learn? It seems like it learned that Trudeau tweets in French!\n\n\nI challenge you to write your own tweet using the knowledge gained to trick the model! Use the printed list or plot above to make some inferences about what words will classify your text as Trump or Trudeau. Can you fool the model into thinking you are Trump or Trudeau?\n\n\nIf you can write French, feel free to make your Trudeau-impersonation tweet in French! As you may have noticed, these French words are common words, or, “stop words”. You could remove both English and French stop words from the tweets as a preprocessing step, but that might decrease the accuracy of the model because Trudeau is the only French-speaker in the group. If you had a dataset with more than one French speaker, this would be a useful preprocessing step.\n\n\nFuture work on this dataset could involve:\n\n\n\nAdd extra preprocessing (such as removing URLs or French stop words) and see the effects\n\n\nUse GridSearchCV to improve both your Bayesian and LinearSVC models by finding the optimal parameters\n\n\nIntrospect your Bayesian model to determine what words are more Trump- or Trudeau- like\n\n\nAdd more recent tweets to your dataset using tweepy and retrain\n\n\n\nGood luck writing your impersonation tweets – feel free to share them on Twitter!\n\n\n# Write two tweets as strings, one which you want to classify as Trump and one as Trudeau\ntrump_tweet = \"Covfeve\"\ntrudeau_tweet = \"Make Canada great again!\"\n\n# Vectorize each tweet using the TF-IDF vectorizer's transform method\n# Note: `transform` needs the string in a list object (i.e. [trump_tweet])\ntrump_tweet_vectorized = tfidf_vectorizer.transform([trump_tweet])\ntrudeau_tweet_vectorized = tfidf_vectorizer.transform([trudeau_tweet])\n\n# Call the predict method on your vectorized tweets\ntrump_tweet_pred = tfidf_svc.predict(trump_tweet_vectorized)\ntrudeau_tweet_pred = tfidf_svc.predict(trudeau_tweet_vectorized)\n\nprint(\"Predicted Trump tweet\", trump_tweet_pred)\nprint(\"Predicted Trudeau tweet\", trudeau_tweet_pred)\n\nPredicted Trump tweet ['Donald J. Trump']\nPredicted Trudeau tweet ['Justin Trudeau']",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "Python",
      "Trump Trudeau Tweets",
      "Trump vs. Trudeau: Tweet classification"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/r/programming_lang/notebook.html",
    "href": "datacamp_projects/guided/r/programming_lang/notebook.html",
    "title": " - Rise and Fall of Programming Languages",
    "section": "",
    "text": "How can we tell what programming languages and technologies are used by the most people? How about what languages are growing and which are shrinking, so that we can tell which are most worth investing time in?\n\n\nOne excellent source of data is Stack Overflow, a programming question and answer site with more than 16 million questions on programming topics. By measuring the number of questions about each technology, we can get an approximate sense of how many people are using it. We’re going to use open data from the Stack Exchange Data Explorer to examine the relative popularity of languages like R, Python, Java and Javascript have changed over time.\n\n\nEach Stack Overflow question has a tag, which marks a question to describe its topic or technology. For instance, there’s a tag for languages like R or Python, and for packages like ggplot2 or pandas.\n\n\n\nWe’ll be working with a dataset with one observation for each tag in each year. The dataset includes both the number of questions asked in that tag in that year, and the total number of questions asked in that year.\n\n\n# Load libraries\nlibrary('readr')\nlibrary('dplyr')\n\n# Load dataset\nby_tag_year &lt;- read_csv('datasets//by_tag_year.csv')\n\n# Inspect the dataset\nprint(by_tag_year)\n\nParsed with column specification:\ncols(\n  year = col_double(),\n  tag = col_character(),\n  number = col_double(),\n  year_total = col_double()\n)\n\n\n# A tibble: 40,518 x 4\n    year tag           number year_total\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1  2008 .htaccess         54      58390\n 2  2008 .net            5910      58390\n 3  2008 .net-2.0         289      58390\n 4  2008 .net-3.5         319      58390\n 5  2008 .net-4.0           6      58390\n 6  2008 .net-assembly      3      58390\n 7  2008 .net-core          1      58390\n 8  2008 2d                42      58390\n 9  2008 32-bit            19      58390\n10  2008 32bit-64bit        4      58390\n# ... with 40,508 more rows",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "R",
      "Programming Lang",
      "{{< fa brands r-project >}} - Rise and Fall of Programming Languages"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/r/programming_lang/notebook.html#data-on-tags-over-time",
    "href": "datacamp_projects/guided/r/programming_lang/notebook.html#data-on-tags-over-time",
    "title": " - Rise and Fall of Programming Languages",
    "section": "",
    "text": "How can we tell what programming languages and technologies are used by the most people? How about what languages are growing and which are shrinking, so that we can tell which are most worth investing time in?\n\n\nOne excellent source of data is Stack Overflow, a programming question and answer site with more than 16 million questions on programming topics. By measuring the number of questions about each technology, we can get an approximate sense of how many people are using it. We’re going to use open data from the Stack Exchange Data Explorer to examine the relative popularity of languages like R, Python, Java and Javascript have changed over time.\n\n\nEach Stack Overflow question has a tag, which marks a question to describe its topic or technology. For instance, there’s a tag for languages like R or Python, and for packages like ggplot2 or pandas.\n\n\n\nWe’ll be working with a dataset with one observation for each tag in each year. The dataset includes both the number of questions asked in that tag in that year, and the total number of questions asked in that year.\n\n\n# Load libraries\nlibrary('readr')\nlibrary('dplyr')\n\n# Load dataset\nby_tag_year &lt;- read_csv('datasets//by_tag_year.csv')\n\n# Inspect the dataset\nprint(by_tag_year)\n\nParsed with column specification:\ncols(\n  year = col_double(),\n  tag = col_character(),\n  number = col_double(),\n  year_total = col_double()\n)\n\n\n# A tibble: 40,518 x 4\n    year tag           number year_total\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1  2008 .htaccess         54      58390\n 2  2008 .net            5910      58390\n 3  2008 .net-2.0         289      58390\n 4  2008 .net-3.5         319      58390\n 5  2008 .net-4.0           6      58390\n 6  2008 .net-assembly      3      58390\n 7  2008 .net-core          1      58390\n 8  2008 2d                42      58390\n 9  2008 32-bit            19      58390\n10  2008 32bit-64bit        4      58390\n# ... with 40,508 more rows",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "R",
      "Programming Lang",
      "{{< fa brands r-project >}} - Rise and Fall of Programming Languages"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/r/programming_lang/notebook.html#now-in-fraction-format",
    "href": "datacamp_projects/guided/r/programming_lang/notebook.html#now-in-fraction-format",
    "title": " - Rise and Fall of Programming Languages",
    "section": "2. Now in fraction format",
    "text": "2. Now in fraction format\n\nThis data has one observation for each pair of a tag and a year, showing the number of questions asked in that tag in that year and the total number of questions asked in that year. For instance, there were 54 questions asked about the .htaccess tag in 2008, out of a total of 58390 questions in that year.\n\n\nRather than just the counts, we’re probably interested in a percentage: the fraction of questions that year that have that tag. So let’s add that to the table.\n\n\n# Add fraction column\nby_tag_year_fraction &lt;- by_tag_year %&gt;% mutate(fraction = number/year_total)\n\n# Print the new table\nprint(by_tag_year_fraction)\n\n# A tibble: 40,518 x 5\n    year tag           number year_total  fraction\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1  2008 .htaccess         54      58390 0.000925 \n 2  2008 .net            5910      58390 0.101    \n 3  2008 .net-2.0         289      58390 0.00495  \n 4  2008 .net-3.5         319      58390 0.00546  \n 5  2008 .net-4.0           6      58390 0.000103 \n 6  2008 .net-assembly      3      58390 0.0000514\n 7  2008 .net-core          1      58390 0.0000171\n 8  2008 2d                42      58390 0.000719 \n 9  2008 32-bit            19      58390 0.000325 \n10  2008 32bit-64bit        4      58390 0.0000685\n# ... with 40,508 more rows",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "R",
      "Programming Lang",
      "{{< fa brands r-project >}} - Rise and Fall of Programming Languages"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/r/programming_lang/notebook.html#has-r-been-growing-or-shrinking",
    "href": "datacamp_projects/guided/r/programming_lang/notebook.html#has-r-been-growing-or-shrinking",
    "title": " - Rise and Fall of Programming Languages",
    "section": "3. Has R been growing or shrinking?",
    "text": "3. Has R been growing or shrinking?\n\nSo far we’ve been learning and using the R programming language. Wouldn’t we like to be sure it’s a good investment for the future? Has it been keeping pace with other languages, or have people been switching out of it?\n\n\nLet’s look at whether the fraction of Stack Overflow questions that are about R has been increasing or decreasing over time.\n\n\n# Filter for R tags\nr_over_time &lt;- by_tag_year_fraction %&gt;% filter(tag == 'r')\n\n# Print the new table\nprint(r_over_time)\n\n# A tibble: 11 x 5\n    year tag   number year_total fraction\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1  2008 r          8      58390 0.000137\n 2  2009 r        524     343868 0.00152 \n 3  2010 r       2270     694391 0.00327 \n 4  2011 r       5845    1200551 0.00487 \n 5  2012 r      12221    1645404 0.00743 \n 6  2013 r      22329    2060473 0.0108  \n 7  2014 r      31011    2164701 0.0143  \n 8  2015 r      40844    2219527 0.0184  \n 9  2016 r      44611    2226072 0.0200  \n10  2017 r      54415    2305207 0.0236  \n11  2018 r      28938    1085170 0.0267",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "R",
      "Programming Lang",
      "{{< fa brands r-project >}} - Rise and Fall of Programming Languages"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/r/programming_lang/notebook.html#visualizing-change-over-time",
    "href": "datacamp_projects/guided/r/programming_lang/notebook.html#visualizing-change-over-time",
    "title": " - Rise and Fall of Programming Languages",
    "section": "4. Visualizing change over time",
    "text": "4. Visualizing change over time\n\nRather than looking at the results in a table, we often want to create a visualization. Change over time is usually visualized with a line plot.\n\n\n# Load ggplot2\nlibrary('ggplot2')\n\n# Create a line plot of fraction over time\nggplot(r_over_time, aes(x = year, y = fraction)) +\n    geom_line()",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "R",
      "Programming Lang",
      "{{< fa brands r-project >}} - Rise and Fall of Programming Languages"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/r/programming_lang/notebook.html#how-about-dplyr-and-ggplot2",
    "href": "datacamp_projects/guided/r/programming_lang/notebook.html#how-about-dplyr-and-ggplot2",
    "title": " - Rise and Fall of Programming Languages",
    "section": "5. How about dplyr and ggplot2?",
    "text": "5. How about dplyr and ggplot2?\n\nBased on that graph, it looks like R has been growing pretty fast in the last decade. Good thing we’re practicing it now!\n\n\nBesides R, two other interesting tags are dplyr and ggplot2, which we’ve already used in this analysis. They both also have Stack Overflow tags!\n\n\nInstead of just looking at R, let’s look at all three tags and their change over time. Are each of those tags increasing as a fraction of overall questions? Are any of them decreasing?\n\n\n# A vector of selected tags\nselected_tags &lt;- c('r', 'dplyr', 'ggplot2')\n\n# Filter for those tags\nselected_tags_over_time &lt;- by_tag_year_fraction %&gt;% \n                filter(tag %in% selected_tags)\n\n# Plot tags over time on a line plot using color to represent tag\nggplot(selected_tags_over_time, aes(x = year, y = fraction)) + \n        geom_line(aes(color = tag))",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "R",
      "Programming Lang",
      "{{< fa brands r-project >}} - Rise and Fall of Programming Languages"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/r/programming_lang/notebook.html#what-are-the-most-asked-about-tags",
    "href": "datacamp_projects/guided/r/programming_lang/notebook.html#what-are-the-most-asked-about-tags",
    "title": " - Rise and Fall of Programming Languages",
    "section": "6. What are the most asked-about tags?",
    "text": "6. What are the most asked-about tags?\n\nIt’s sure been fun to visualize and compare tags over time. The dplyr and ggplot2 tags may not have as many questions as R, but we can tell they’re both growing quickly as well.\n\n\nWe might like to know which tags have the most questions overall, not just within a particular year. Right now, we have several rows for every tag, but we’ll be combining them into one. That means we want group_by() and summarize().\n\n\nLet’s look at tags that have the most questions in history.\n\n\n# Find total number of questions for each tag\nsorted_tags &lt;- by_tag_year %&gt;%\n    group_by(tag) %&gt;%\n    summarize(tag_total = sum(number)) %&gt;%\n    arrange(desc(tag_total))\n\n# Print the new table\nprint(sorted_tags)\n\n# A tibble: 4,080 x 2\n   tag        tag_total\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 javascript   1632049\n 2 java         1425961\n 3 c#           1217450\n 4 php          1204291\n 5 android      1110261\n 6 python        970768\n 7 jquery        915159\n 8 html          755341\n 9 c++           574263\n10 ios           566075\n# ... with 4,070 more rows",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "R",
      "Programming Lang",
      "{{< fa brands r-project >}} - Rise and Fall of Programming Languages"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/r/programming_lang/notebook.html#how-have-large-programming-languages-changed-over-time",
    "href": "datacamp_projects/guided/r/programming_lang/notebook.html#how-have-large-programming-languages-changed-over-time",
    "title": " - Rise and Fall of Programming Languages",
    "section": "7. How have large programming languages changed over time?",
    "text": "7. How have large programming languages changed over time?\n\nWe’ve looked at selected tags like R, ggplot2, and dplyr, and seen that they’re each growing. What tags might be shrinking? A good place to start is to plot the tags that we just saw that were the most-asked about of all time, including JavaScript, Java and C#.\n\n\n# Get the six largest tags\nhighest_tags &lt;- head(sorted_tags$tag)\n\n# Filter for the six largest tags\nby_tag_subset &lt;- filter(by_tag_year_fraction, tag %in% highest_tags)\n\n# Plot tags over time on a line plot using color to represent tag\nggplot(by_tag_subset, aes(x = year, y = fraction)) +\n    geom_line(aes(color = tag))",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "R",
      "Programming Lang",
      "{{< fa brands r-project >}} - Rise and Fall of Programming Languages"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/r/programming_lang/notebook.html#some-more-tags",
    "href": "datacamp_projects/guided/r/programming_lang/notebook.html#some-more-tags",
    "title": " - Rise and Fall of Programming Languages",
    "section": "8. Some more tags!",
    "text": "8. Some more tags!\n\nWow, based on that graph we’ve seen a lot of changes in what programming languages are most asked about. C# gets fewer questions than it used to, and Python has grown quite impressively.\n\n\nThis Stack Overflow data is incredibly versatile. We can analyze any programming language, web framework, or tool where we’d like to see their change over time. Combined with the reproducibility of R and its libraries, we have ourselves a powerful method of uncovering insights about technology.\n\n\nTo demonstrate its versatility, let’s check out how three big mobile operating systems (Android, iOS, and Windows Phone) have compared in popularity over time. But remember: this code can be modified simply by changing the tag names!\n\n\n# Get tags of interest\nmy_tags &lt;- c('android', 'ios', 'windows-phone')\n\n# Filter for those tags\nby_tag_subset &lt;- filter(by_tag_year_fraction, tag %in% my_tags)\n\n# Plot tags over time on a line plot using color to represent tag\nggplot(by_tag_subset, aes(x = year, y = fraction)) +\n    geom_line(aes(color = tag))",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided",
      "R",
      "Programming Lang",
      "{{< fa brands r-project >}} - Rise and Fall of Programming Languages"
    ]
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "heiner.atze@gmx.net, +33 7 80 84 91 20, Gentilly",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#education",
    "href": "cv/cv.html#education",
    "title": "Curriculum Vitae",
    "section": "EDUCATION",
    "text": "EDUCATION\nPostgraduate diploma – Biostatistics and Methods in Public Health  Paris Saclay University, Paris, France Oct 2023 — Sep 2024\nMaster level courses in :\n\nProbability and Statistics\nClinical Research\nQuantitative Epidemiology\n\nDoctor of Philosophy – Biochemistry, Microbiology  Sorbonne University, Paris, France Nov 2018 — Sep 2021\nMaster of Science(*) - Medicinal Chemistry  Friedrich-Schiller-University, Jena, Germany Sep 2014 — Sep 2016\n(*)German Diplom, degree awarded after 5 years of study and submission of a research thesis\n\nState examination - Pharmacy  Friedrich-Schiller-University, Jena, Germany Sep 2010 — Sep 2014",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#research-experience",
    "href": "cv/cv.html#research-experience",
    "title": "Curriculum Vitae",
    "section": "RESEARCH EXPERIENCE",
    "text": "RESEARCH EXPERIENCE\nDoctoral Researcher – Biochemistry, Microbiology  INSERM, Paris, France Jun 2018 — Sep 2021\nResearcher in Team 12 “Bacterial structures implicated in antibiotic resistance” at the Centre de Recherche des Cordeliers, Paris\nTwo main axes of research:\n\nBiological characterization of new generation β-lactamase inhibitors\nin vitro and in vivo characterization of inhibitors, data analysis and interpretation, feedback into the consult-design-test-repeat cycle in collaboration with the team of organic chemists\nFundamental research on cell wall metabolism in gram-negative bacteria\nDe novo method development: isotopic labeling of cell cultures, sample preparation, analysis by mass spectrometry, custom data analysis tools and pipelines\n\nKey achievements and skills:\n\nExploration of the chemical space around the core inhibitor and identification of curcial ligand-target-interactions\nHandling and managing a large amount of results and data from biological experiments\nDevelopment and maintentance of custom data analysis tools availabale at Gitlab\n\nResearch assistant – Medicinal and Pharmaceutical Chemistry  Friedrich-Schiller-University, Jena, Germany Nov 2014 — Apr 2015\nBiological charaterization of putative anti-inflammatory substances, fundamental research on signaling cascades in inflammation using biochemical methods and imaging techniques",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#work-experience",
    "href": "cv/cv.html#work-experience",
    "title": "Curriculum Vitae",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE\n\nPharmacist\nPharmacie Attal  Fontenay-aux-Roses, France Oct 2021 — present\nPharmaceutical counseling of patients\nResponsible for communication with medical staff in nursing homes\nSupervision of technical staff and pharmacy students\nOther Pharmacies  Jena, Germany Nov 2015 – May 2018\n\n\nPre-registration pharmacist\nF. Hoffmann-La Roche  Basel, Switzerland May 2015 – Oct 2015",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#skills",
    "href": "cv/cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "SKILLS",
    "text": "SKILLS\n\nLanguages\nGerman: Native Speaker\nEnglish: Professional proficiency\nFrench: Professional proficiency\n\n\nData analysis, programming and software packages\n\nProgramming languages\n\n\nroutine use of pandas, numpy, matplotlib\nproject dependent use of scikit-learn, bokeh\n\nCertifications by DataCamp (click pictograms for course overview and statement of accomplishement):\n\n\n\n\n\n\n\n\n\nData Science track\n\n\n\n\n\n\n\nMachine Learning track\n\n\n\n\n\n\n\nCertifications by DataCamp:\n\nData Science\nMachine learning\n\n\n: data analysis, plotting, basic modeling\nSpecialization Data Analysis with R by Duke University on Coursera\n\nStatement of accomplishment\nCourse projects (available at portfolio page):\n\n\nExploring the BRFSS data\nStatistical inference using GSS data\nModeling and prediction of movie scores\n\n\n\n\nExploring the BRFSS data\nStatistical inference using GSS data\nModeling and prediction of movie scores\n\n\n\n: basic database setup and queries, joins and grouping operations\nProjects\n : notions\n\n\n\n\nTechnical lab skills\n\nCellular Biology\nHuman Cell Culture: HEK-293, primary human lymphocytes\nBacterial Cell Culture: Escherichia coli\nMolecular Biology\nmRNA-Extraction, cDNA synthesis\nPolymerase chain reaction\nBiochemistry\nSDS-PAGE, Western Blot\nEnzyme kinetics",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#publications",
    "href": "cv/cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "PUBLICATIONS",
    "text": "PUBLICATIONS\n\nAtze et al. (2022) Bouchet et al. (2021) Bouchet et al. (2020) Le Run et al. (2020) Triboulet et al. (2019) Garscha et al. (2017)\n\n\nJournal articles\n\n\nAtze, H., Liang, Y., Hugonnet, J.-E., Gutierrez, A., Rusconi, F. and Arthur, M. (2022), “Heavy isotope labeling and mass spectrometry reveal unexpected remodeling of bacterial cell wall expansion in response to drugs”, Elife, eLife Sciences Publications Limited, Vol. 11, p. e72863.\n\n\nBouchet, F., Atze, H., Arthur, M., Ethève-Quelquejeu, M. and Iannazzo, L. (2021), “Traceless staudinger ligation to introduce chemical diversity on \\(\\beta\\)-lactamase inhibitors of second generation”, Organic Letters, ACS Publications, Vol. 23 No. 20, pp. 7755–7758.\n\n\nBouchet, F., Atze, H., Fonvielle, M., Edoo, Z., Arthur, M., Ethève-Quelquejeu, M. and Iannazzo, L. (2020), “Diazabicyclooctane functionalization for inhibition of \\(\\beta\\)-lactamases from enterobacteria”, Journal of Medicinal Chemistry, American Chemical Society, Vol. 63 No. 10, pp. 5257–5273.\n\n\nGarscha, U., Romp, E., Pace, S., Rossi, A., Temml, V., Schuster, D., König, S., et al. (2017), “Pharmacological profile and efficiency in vivo of diflapolin, the first dual inhibitor of 5-lipoxygenase-activating protein and soluble epoxide hydrolase”, Scientific Reports, Nature Publishing Group UK London, Vol. 7 No. 1, p. 9398.\n\n\nLe Run, E., Atze, H., Arthur, M. and Mainardi, J.-L. (2020), “Impact of relebactam-mediated inhibition of mycobacterium abscessus BlaMab \\(\\beta\\)-lactamase on the in vitro and intracellular efficacy of imipenem”, Journal of Antimicrobial Chemotherapy, Oxford University Press, Vol. 75 No. 2, pp. 379–383.\n\n\nTriboulet, S., Edoo, Z., Compain, F., Ourghanlian, C., Dupuis, A., Dubée, V., Sutterlin, L., et al. (2019), “Tryptophan fluorescence quenching in \\(\\beta\\)-lactam-interacting proteins is modulated by the structure of intermediates and final products of the acylation reaction”, ACS Infectious Diseases, American Chemical Society, Vol. 5 No. 7, pp. 1169–1176.\n\n\n\n\nCV template adapted from Cynthia Huang",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "datacamp_site/extended.html",
    "href": "datacamp_site/extended.html",
    "title": "Datacamp extended projects",
    "section": "",
    "text": "Can I give some explanation here ?\nTest\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]