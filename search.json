[
  {
    "objectID": "datacamp_site/guided.html",
    "href": "datacamp_site/guided.html",
    "title": "Datacamp",
    "section": "",
    "text": "Do students describe professors differently based on gender?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo students describe professors differently based on gender?\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nExploring ingredients of cosmetics\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nExploring movie plots using NLP\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nSQL - Electric vehicle charging\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nSQL - International debt statistics\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nSQL - London transportation dataset\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nSQL - Mental health of international students\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nTrump vs. Trudeau: Tweet classification\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Guided"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Heiner Atze, PhD, Pharmacist",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\njdaskölfs",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "",
    "text": "So you think you can classify text? How about tweets? In this notebook, we’ll take a dive into the world of social media text classification by investigating how to properly classify tweets from two prominent North American politicians: Donald Trump and Justin Trudeau.\n\n\n\n\n\nPhoto Credit: Executive Office of the President of the United States\n\n\nTweets pose specific problems to NLP, including the fact they are shorter texts. There are also plenty of platform-specific conventions to give you hassles: mentions, #hashtags, emoji, links and short-hand phrases (ikr?). Can we overcome those challenges and build a useful classifier for these two tweeters? Yes! Let’s get started.\n\n\nTo begin, we will import all the tools we need from scikit-learn. We will need to properly vectorize our data (CountVectorizer and TfidfVectorizer). And we will also want to import some models, including MultinomialNB from the naive_bayes module, LinearSVC from the svm module and PassiveAggressiveClassifier from the linear_model module. Finally, we’ll need sklearn.metrics and train_test_split and GridSearchCV from the model_selection module to evaluate and optimize our model.\n\n\n# Set seed for reproducibility\nimport random; random.seed(53)\n\n# Import all we need from sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introduction-and-imports",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introduction-and-imports",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "",
    "text": "So you think you can classify text? How about tweets? In this notebook, we’ll take a dive into the world of social media text classification by investigating how to properly classify tweets from two prominent North American politicians: Donald Trump and Justin Trudeau.\n\n\n\n\n\nPhoto Credit: Executive Office of the President of the United States\n\n\nTweets pose specific problems to NLP, including the fact they are shorter texts. There are also plenty of platform-specific conventions to give you hassles: mentions, #hashtags, emoji, links and short-hand phrases (ikr?). Can we overcome those challenges and build a useful classifier for these two tweeters? Yes! Let’s get started.\n\n\nTo begin, we will import all the tools we need from scikit-learn. We will need to properly vectorize our data (CountVectorizer and TfidfVectorizer). And we will also want to import some models, including MultinomialNB from the naive_bayes module, LinearSVC from the svm module and PassiveAggressiveClassifier from the linear_model module. Finally, we’ll need sklearn.metrics and train_test_split and GridSearchCV from the model_selection module to evaluate and optimize our model.\n\n\n# Set seed for reproducibility\nimport random; random.seed(53)\n\n# Import all we need from sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#transforming-our-collected-data",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#transforming-our-collected-data",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "2. Transforming our collected data",
    "text": "2. Transforming our collected data\n\nTo begin, let’s start with a corpus of tweets which were collected in November 2017. They are available in CSV format. We’ll use a Pandas DataFrame to help import the data and pass it to scikit-learn for further processing.\n\n\nSince the data has been collected via the Twitter API and not split into test and training sets, we’ll need to do this. Let’s use train_test_split() with random_state=53 and a test size of 0.33, just as we did in the DataCamp course. This will ensure we have enough test data and we’ll get the same results no matter where or when we run this code.\n\n\nimport pandas as pd\n\n# Load data\ntweet_df = pd.read_csv('datasets/tweets.csv')\n\n# Create target\ny = tweet_df['author']\nX = tweet_df.drop('author', axis = 1)\n\n# Split training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 53, test_size = 0.33)"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#vectorize-the-tweets",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#vectorize-the-tweets",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "3. Vectorize the tweets",
    "text": "3. Vectorize the tweets\n\nWe have the training and testing data all set up, but we need to create vectorized representations of the tweets in order to apply machine learning.\n\n\nTo do so, we will utilize the CountVectorizer and TfidfVectorizer classes which we will first need to fit to the data.\n\n\nOnce this is complete, we can start modeling with the new vectorized tweets!\n\n\n# Initialize count vectorizer\ncount_vectorizer = CountVectorizer(max_df = 0.9, min_df = 0.05, stop_words = 'english')\ncount_vectorizer.fit(X_train['status'])\n\n# Create count train and test variables\ncount_train = count_vectorizer.transform(X_train['status'])\ncount_test = count_vectorizer.transform(X_test['status'])\n\n# Initialize tfidf vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.9, min_df = 0.05)\\\n                    .fit(X_train['status'])\n\n# Create tfidf train and test variables\ntfidf_train = tfidf_vectorizer.transform(X_train['status'])\ntfidf_test = tfidf_vectorizer.transform(X_test['status'])"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#training-a-multinomial-naive-bayes-model",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#training-a-multinomial-naive-bayes-model",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "4. Training a multinomial naive Bayes model",
    "text": "4. Training a multinomial naive Bayes model\n\nNow that we have the data in vectorized form, we can train the first model. Investigate using the Multinomial Naive Bayes model with both the CountVectorizer and TfidfVectorizer data. Which do will perform better? How come?\n\n\nTo assess the accuracies, we will print the test sets accuracy scores for both models.\n\n\n# Create a MulitnomialNB model\ntfidf_nb = MultinomialNB().fit(tfidf_train, y_train)\n\n# ... Train your model here ...\n\n# Run predict on your TF-IDF test data to get your predictions\ntfidf_nb_pred = tfidf_nb.predict(tfidf_test)\n\n# Calculate the accuracy of your predictions\ntfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n\n# Create a MulitnomialNB model\ncount_nb = MultinomialNB().fit(count_train, y_train)\n# ... Train your model here ...\n\n# Run predict on your count test data to get your predictions\ncount_nb_pred = count_nb.predict(count_test)\n\n# Calculate the accuracy of your predictions\ncount_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\n\nprint('NaiveBayes Tfidf Score: ', tfidf_nb_score)\nprint('NaiveBayes Count Score: ', count_nb_score)\n\nNaiveBayes Tfidf Score:  0.803030303030303\nNaiveBayes Count Score:  0.7954545454545454"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#evaluating-our-model-using-a-confusion-matrix",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#evaluating-our-model-using-a-confusion-matrix",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "5. Evaluating our model using a confusion matrix",
    "text": "5. Evaluating our model using a confusion matrix\n\nWe see that the TF-IDF model performs better than the count-based approach. Based on what we know from the NLP fundamentals course, why might that be? We know that TF-IDF allows unique tokens to have a greater weight - perhaps tweeters are using specific important words that identify them! Let’s continue the investigation.\n\n\nFor classification tasks, an accuracy score doesn’t tell the whole picture. A better evaluation can be made if we look at the confusion matrix, which shows the number correct and incorrect classifications based on each class. We can use the metrics, True Positives, False Positives, False Negatives, and True Negatives, to determine how well the model performed on a given class. How many times was Trump misclassified as Trudeau?\n\n\n%matplotlib inline\n\nfrom datasets.helper_functions import plot_confusion_matrix\n\n# Calculate the confusion matrices for the tfidf_nb model and count_nb models\ntfidf_nb_cm = metrics.confusion_matrix(y_test, tfidf_nb_pred)\ncount_nb_cm = metrics.confusion_matrix(y_test, count_nb_pred)\n\n# Plot the tfidf_nb_cm confusion matrix\nplot_confusion_matrix(tfidf_nb_cm, classes=['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"TF-IDF NB Confusion Matrix\")\n\n# Plot the count_nb_cm confusion matrix without overwriting the first plot \nplot_confusion_matrix(count_nb_cm, classes=['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"Count NB Confusion matrix\", figure=1)\n\nConfusion matrix, without normalization\nConfusion matrix, without normalization"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#trying-out-another-classifier-linear-svc",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#trying-out-another-classifier-linear-svc",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "6. Trying out another classifier: Linear SVC",
    "text": "6. Trying out another classifier: Linear SVC\n\nSo the Bayesian model only has one prediction difference between the TF-IDF and count vectorizers – fairly impressive! Interestingly, there is some confusion when the predicted label is Trump but the actual tweeter is Trudeau. If we were going to use this model, we would want to investigate what tokens are causing the confusion in order to improve the model.\n\n\nNow that we’ve seen what the Bayesian model can do, how about trying a different approach? LinearSVC is another popular choice for text classification. Let’s see if using it with the TF-IDF vectors improves the accuracy of the classifier!\n\n\n# Create a LinearSVM model\ntfidf_svc = LinearSVC().fit(tfidf_train, y_train)\n\n# ... Train your model here ...\n\n# Run predict on your tfidf test data to get your predictions\ntfidf_svc_pred = tfidf_svc.predict(tfidf_test)\n\n# Calculate your accuracy using the metrics module\ntfidf_svc_score = metrics.accuracy_score(y_test, tfidf_svc_pred)\n\nprint(\"LinearSVC Score:   %0.3f\" % tfidf_svc_score)\n\n# Calculate the confusion matrices for the tfidf_svc model\nsvc_cm = metrics.confusion_matrix(y_test, tfidf_svc_pred)\n\n# Plot the confusion matrix using the plot_confusion_matrix function\nplot_confusion_matrix(svc_cm, classes = ['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"TF-IDF LinearSVC Confusion Matrix\")\n\nLinearSVC Score:   0.841\nConfusion matrix, without normalization"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introspecting-our-top-model",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introspecting-our-top-model",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "7. Introspecting our top model",
    "text": "7. Introspecting our top model\n\nWow, the LinearSVC model is even better than the Multinomial Bayesian one. Nice work! Via the confusion matrix we can see that, although there is still some confusion where Trudeau’s tweets are classified as Trump’s, the False Positive rate is better than the previous model. So, we have a performant model, right?\n\n\nWe might be able to continue tweaking and improving all of the previous models by learning more about parameter optimization or applying some better preprocessing of the tweets.\n\n\nNow let’s see what the model has learned. Using the LinearSVC Classifier with two classes (Trump and Trudeau) we can sort the features (tokens), by their weight and see the most important tokens for both Trump and Trudeau. What are the most Trump-like or Trudeau-like words? Did the model learn something useful to distinguish between these two men?\n\n\nfrom datasets.helper_functions import plot_and_return_top_features\n\n# Import pprint from pprint\nfrom pprint import pprint\n\n# Get the top features using the plot_and_return_top_features function and your top model and tfidf vectorizer\ntop_features = plot_and_return_top_features(tfidf_svc, tfidf_vectorizer)\n\n# pprint the top features\npprint(top_features)\n\n\n\n\n\n\n\n\n[(-0.3959834966911922, 'great'),\n (-0.24645580091925237, 'thank'),\n (0.06257998949180026, 'president'),\n (0.48211745246750215, 'https'),\n (0.5960555762649068, 'vietnam'),\n (0.6155609686456073, 'amp'),\n (0.7725857577713344, 'le'),\n (0.8213735137856691, 'les'),\n (0.8286549508433744, 'today'),\n (1.1869092357816051, 'du'),\n (1.3143518952322126, 'pour'),\n (1.4122560793508427, 'nous'),\n (1.4612710235935042, 'rt'),\n (1.4991808273544363, 'et'),\n (1.50564270245237, 'la'),\n (1.6567934485943738, 'canada')]"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#bonus-can-you-write-a-trump-or-trudeau-tweet",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#bonus-can-you-write-a-trump-or-trudeau-tweet",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "8. Bonus: can you write a Trump or Trudeau tweet?",
    "text": "8. Bonus: can you write a Trump or Trudeau tweet?\n\nSo, what did our model learn? It seems like it learned that Trudeau tweets in French!\n\n\nI challenge you to write your own tweet using the knowledge gained to trick the model! Use the printed list or plot above to make some inferences about what words will classify your text as Trump or Trudeau. Can you fool the model into thinking you are Trump or Trudeau?\n\n\nIf you can write French, feel free to make your Trudeau-impersonation tweet in French! As you may have noticed, these French words are common words, or, “stop words”. You could remove both English and French stop words from the tweets as a preprocessing step, but that might decrease the accuracy of the model because Trudeau is the only French-speaker in the group. If you had a dataset with more than one French speaker, this would be a useful preprocessing step.\n\n\nFuture work on this dataset could involve:\n\n\n\nAdd extra preprocessing (such as removing URLs or French stop words) and see the effects\n\n\nUse GridSearchCV to improve both your Bayesian and LinearSVC models by finding the optimal parameters\n\n\nIntrospect your Bayesian model to determine what words are more Trump- or Trudeau- like\n\n\nAdd more recent tweets to your dataset using tweepy and retrain\n\n\n\nGood luck writing your impersonation tweets – feel free to share them on Twitter!\n\n\n# Write two tweets as strings, one which you want to classify as Trump and one as Trudeau\ntrump_tweet = \"Covfeve\"\ntrudeau_tweet = \"Make Canada great again!\"\n\n# Vectorize each tweet using the TF-IDF vectorizer's transform method\n# Note: `transform` needs the string in a list object (i.e. [trump_tweet])\ntrump_tweet_vectorized = tfidf_vectorizer.transform([trump_tweet])\ntrudeau_tweet_vectorized = tfidf_vectorizer.transform([trudeau_tweet])\n\n# Call the predict method on your vectorized tweets\ntrump_tweet_pred = tfidf_svc.predict(trump_tweet_vectorized)\ntrudeau_tweet_pred = tfidf_svc.predict(trudeau_tweet_vectorized)\n\nprint(\"Predicted Trump tweet\", trump_tweet_pred)\nprint(\"Predicted Trudeau tweet\", trudeau_tweet_pred)\n\nPredicted Trump tweet ['Donald J. Trump']\nPredicted Trudeau tweet ['Justin Trudeau']"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/international_debt/notebook.html",
    "href": "datacamp_projects/workspace/SQL/international_debt/notebook.html",
    "title": "SQL - International debt statistics",
    "section": "",
    "text": "Humans not only take debts to manage necessities. A country may also take debt to manage its economy. For example, infrastructure spending is one costly ingredient required for a country’s citizens to lead comfortable lives. The World Bank is the organization that provides debt to countries.\nIn this project, you are going to analyze international debt data collected by The World Bank. The dataset contains information about the amount of debt (in USD) owed by developing countries across several categories. You are going to find the answers to the following questions:\n\nWhat is the number of distinct countries present in the database?\nWhat country has the highest amount of debt?\nWhat country has the lowest amount of repayments?\n\nBelow is a description of the table you will be working with:"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/international_debt/notebook.html#no.-of-distinct-countries",
    "href": "datacamp_projects/workspace/SQL/international_debt/notebook.html#no.-of-distinct-countries",
    "title": "SQL - International debt statistics",
    "section": "No. of distinct countries",
    "text": "No. of distinct countries\nWhat is the number of distinct countries present in the database? The output should be single row and column aliased as total_distinct_countries. Save the query as num_distinct_countries.\n\n-- num_distinct_countries \nSELECT COUNT(DISTINCT country_name) as total_distinct_countries\nFROM international_debt\n\n\n\n\n\n\n\n\ntotal_distinct_countries\n\n\n\n\n0\n124"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/international_debt/notebook.html#country-with-highest-debt",
    "href": "datacamp_projects/workspace/SQL/international_debt/notebook.html#country-with-highest-debt",
    "title": "SQL - International debt statistics",
    "section": "Country with highest debt",
    "text": "Country with highest debt\nWhat country has the highest amount of debt? Your output should contain two columns: country_name and total_debt and one row. Save the query as highest_debt_country.\n\n-- highest_debt_country \nSELECT country_name, SUM(debt) as total_debt\nFROM international_debt\nGROUP BY country_name\nORDER BY total_debt DESC\nLIMIT 1\n\n\n\n\n\n\n\n\ncountry_name\ntotal_debt\n\n\n\n\n0\nChina\n2.857935e+11"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/international_debt/notebook.html#county-with-lowest-amount-of-principal-repayments",
    "href": "datacamp_projects/workspace/SQL/international_debt/notebook.html#county-with-lowest-amount-of-principal-repayments",
    "title": "SQL - International debt statistics",
    "section": "County with lowest amount of principal repayments",
    "text": "County with lowest amount of principal repayments\nWhat country has the lowest amount of principal repayments (indicated by the “DT.AMT.DLXF.CD” indicator code)? The output table should contain three columns: country_name, indicator_name, and lowest_repayment and one row, saved in the query lowest_principal_repayment.\n\n-- lowest_principal_repayment \nSELECT country_name, indicator_name, MAX(debt) as lowest_repayment\nFROM international_debt\nWHERE indicator_code = 'DT.AMT.DLXF.CD'\nGROUP BY country_name, indicator_name\nORDER BY lowest_repayment ASC\nLIMIT 1\n\n\n\n\n\n\n\n\ncountry_name\nindicator_name\nlowest_repayment\n\n\n\n\n0\nTimor-Leste\nPrincipal repayments on external debt, long-te...\n825000"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/london_transport/notebook.html",
    "href": "datacamp_projects/workspace/SQL/london_transport/notebook.html",
    "title": "SQL - London transportation dataset",
    "section": "",
    "text": "tower bridge\n\n\nLondon, or as the Romans called it “Londonium”! Home to over 8.5 million residents who speak over 300 languages. While the City of London is a little over one square mile (hence its nickname “The Square Mile”), Greater London has grown to encompass 32 boroughs spanning a total area of 606 square miles!\n\n\n\nunderground train leaving a platform\n\n\nGiven the city’s roads were originally designed for horse and cart, this area and population growth has required the development of an efficient public transport system! Since the year 2000, this has been through the local government body called Transport for London, or TfL, which is managed by the London Mayor’s office. Their remit covers the London Underground, Overground, Docklands Light Railway (DLR), buses, trams, river services (clipper and Emirates Airline cable car), roads, and even taxis.\nThe Mayor of London’s office make their data available to the public here. In this project, you will work with a slightly modified version of a dataset containing information about public transport journey volume by transport type.\nThe data has been loaded into a Google BigQuery database called TFL with a single table called JOURNEYS, including the following data:"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/london_transport/notebook.html#most-popular-transport-types-by-total-journeys-over-whole-dataset",
    "href": "datacamp_projects/workspace/SQL/london_transport/notebook.html#most-popular-transport-types-by-total-journeys-over-whole-dataset",
    "title": "SQL - London transportation dataset",
    "section": "Most popular transport types by total journeys over whole dataset",
    "text": "Most popular transport types by total journeys over whole dataset\nWhat are the most popular transport types, measured by the total number of journeys?\nThe output should contain two columns, 1) journey_type and 2) total_journeys_millions, and be sorted by the second column in descending order. Save the query as most_popular_transport_types.\n\n-- most_popular_transport_types\nSELECT JOURNEY_TYPE, SUM(JOURNEYS_MILLIONS) as TOTAL_JOURNEYS_MILLIONS\nFROM TFL.JOURNEYS\nGROUP BY JOURNEY_TYPE\nORDER BY TOTAL_JOURNEYS_MILLIONS DESC;\n\n\n\n\n\n\n\n\nJOURNEY_TYPE\nTOTAL_JOURNEYS_MILLIONS\n\n\n\n\n0\nBus\n24905.193947\n\n\n1\nUnderground & DLR\n15020.466544\n\n\n2\nOverground\n1666.845666\n\n\n3\nTfL Rail\n411.313421\n\n\n4\nTram\n314.689875\n\n\n5\nEmirates Airline\n14.583718"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/london_transport/notebook.html#best-five-months-for-emirates-airlines",
    "href": "datacamp_projects/workspace/SQL/london_transport/notebook.html#best-five-months-for-emirates-airlines",
    "title": "SQL - London transportation dataset",
    "section": "Best five months for Emirates Airlines",
    "text": "Best five months for Emirates Airlines\nWhich five months and years were the most popular for the Emirates Airline?\nReturn an output containing month, year, and journeys_millions, with the latter rounded to two decimal places and aliased as rounded_journeys_millions. Exclude null values and order the results by 1) rounded_journeys_millions in descending order and 2) year in ascending order, saving the result as emirates_airline_popularity.\n\n-- emirates_airline_popularity\nSELECT YEAR, MONTH, ROUND(SUM(JOURNEYS_MILLIONS),2) as ROUNDED_JOURNEYS_MILLIONS\nFROM TFL.JOURNEYS\nWHERE JOURNEY_TYPE = 'Emirates Airline' AND JOURNEY_TYPE IS NOT NULL\nGROUP BY YEAR, MONTH, JOURNEY_TYPE\nORDER BY ROUNDED_JOURNEYS_MILLIONS DESC, YEAR ASC\nLIMIT 5\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nROUNDED_JOURNEYS_MILLIONS\n\n\n\n\n0\n2012\n5\n0.53\n\n\n1\n2012\n6\n0.38\n\n\n2\n2012\n4\n0.24\n\n\n3\n2013\n5\n0.19\n\n\n4\n2015\n5\n0.19"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/london_transport/notebook.html#five-worst-years-for-london-underground",
    "href": "datacamp_projects/workspace/SQL/london_transport/notebook.html#five-worst-years-for-london-underground",
    "title": "SQL - London transportation dataset",
    "section": "Five worst years for London Underground",
    "text": "Five worst years for London Underground\nFind the five years with the lowest volume of Underground & DLR journeys, saving as least_popular_years_tube. The results should contain the columns year, journey_type, and total_journeys_millions.\n\n-- least_popular_years_tube\nSELECT YEAR, JOURNEY_TYPE, SUM(JOURNEYS_MILLIONS) as TOTAL_JOURNEYS_MILLIONS\nFROM TFL.JOURNEYS\nWHERE JOURNEY_TYPE = 'Underground & DLR'\nGROUP BY YEAR, JOURNEY_TYPE\nORDER BY TOTAL_JOURNEYS_MILLIONS ASC\nLIMIT 5\n\n\n\n\n\n\n\n\nYEAR\nJOURNEY_TYPE\nTOTAL_JOURNEYS_MILLIONS\n\n\n\n\n0\n2020\nUnderground & DLR\n310.179316\n\n\n1\n2021\nUnderground & DLR\n748.452544\n\n\n2\n2022\nUnderground & DLR\n1064.859009\n\n\n3\n2010\nUnderground & DLR\n1096.145588\n\n\n4\n2011\nUnderground & DLR\n1156.647654"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html",
    "title": "Do students describe professors differently based on gender?",
    "section": "",
    "text": "Note: You can consult the solution of this live training in the file browser as notebook-solution.ipynb\nLanguage plays a crucial role in shaping our perceptions and attitudes towards gender in the workplace, in classrooms, and personal relationships. Studies have shown that gender bias in language can have a significant impact on the way people are perceived and treated.\nFor example, research has found that job advertisements that use masculine-coded language tend to attract more male applicants, while those that use feminine-coded language tend to attract more female applicants. Similarly, gendered language can perpetuate differences in the classroom.\nIn this project, we’ll using scraped student reviews from ratemyprofessors.com to identify differences in language commonly used for male vs. female professors, and explore subtleties in how language in the classroom can be gendered.\nThis excellent tool created by Ben Schmidt allows us to enter the words and phrases that we find in our analysis and explore them in more depth. We’ll do this at the end.\nCatalyst also does some incredible work on decoding gendered language."
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "title": "Do students describe professors differently based on gender?",
    "section": "3a. What additional package imports are required for data visualization and NLP?",
    "text": "3a. What additional package imports are required for data visualization and NLP?\n\nimport numpy as np # For manipulating matrices during NLP\n\nimport nltk # Natural language toolkit\nfrom nltk.tokenize import word_tokenize # Used for breaking up strings of text (e.g. sentences) into words\nfrom nltk.stem.porter import PorterStemmer # Used to return the dictionary base of a word\nfrom nltk.tokenize import WhitespaceTokenizer # Used for breaking up strings of text (e.g. sentences) into words based on white space\n\nnltk.download('punkt')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # Used to count the occurences of words and phrases\nfrom sklearn.feature_extraction import text as sktext# Using to extrat features from text\n\n# For plotting\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set(style='white')\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /home/kantundpeterpan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n3b. How can we assign gender labels to professors?\nLet’s write a custom function that assigns a gender label to professors based on the pronouns most commontly used for him. Specifically: - If any of ['she', 'her', 'herself', 'shes'] occur more than 5 times across all reviews for that professor, we label the professor “F”. - If any of ['him', 'he', 'his', 'himself'] occur more than 5 times across all reviews for that professor, we label the professor “F”.\n\nfrom collections import Counter\n\n\ndef assign_pronoun(review_list):\n    \n    she_ps = ['she', 'her', 'herself', 'shes']\n    he_ps = ['him', 'he', 'his', 'himself']\n    \n    counters = [Counter(word_tokenize(r.lower())) for r in review_list]\n    \n    ### FEMALE\n    she_ps_counter = dict()\n    \n    for sp in she_ps:\n        she_ps_counter[sp] = 0\n        she_ps_counter[sp] = sum([c[sp] if sp in c.keys() else 0 for c in counters])\n        \n    she_ps_counts = np.array([she_ps_counter[sp] for sp in she_ps])\n    \n    if np.sum(she_ps_counts) &gt; 5:\n        return \"F\"\n    \n    ### MALE\n    he_ps_counter = dict()\n    \n    for hp in he_ps:\n        he_ps_counter[hp] = 0\n        he_ps_counter[hp] = sum([c[hp] if hp in c.keys() else 0 for c in counters])\n        \n    he_ps_counts = np.array([he_ps_counter[hp] for hp in he_ps])\n    \n    if np.sum(he_ps_counts) &gt; 5:\n        return \"M\"\n\n\nassign_pronoun(df.review.iloc[6])\n\n'M'\n\n\n\ndf['pronouns'] = df.review.apply(assign_pronoun)\n\n\ndf.pronouns.value_counts()\n\npronouns\nM    417\nF    139\nName: count, dtype: int64\n\n\n\n\n3c. Are there any initial differences between male and female professors based on their overall ratings?\nLet’s start with a barplot.\n\nplt.figure(figsize=(4,4))\nsns.barplot(data = df, x = 'pronouns', y = 'rating', estimator = 'median',\n            palette = 'magma')\nplt.show()\n\n/tmp/ipykernel_832826/2203816405.py:2: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data = df, x = 'pronouns', y = 'rating', estimator = 'median',\n\n\n\n\n\n\n\n\n\nA boxplot overlaid with a stripplot will give us a better sense of the distribution of the data.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(df, x = 'pronouns', y = 'rating', palette = 'magma')\nsns.stripplot(data = df, x = 'pronouns', y = 'rating', jitter = 0.2, color = 'lightblue',\n              edgecolor = 'k', linewidth=1)\nplt.show()\n\n/tmp/ipykernel_832826/1838852018.py:2: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(df, x = 'pronouns', y = 'rating', palette = 'magma')"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "title": "Do students describe professors differently based on gender?",
    "section": "Task 3d. What are the most important words being used to describe professors in reviews?",
    "text": "Task 3d. What are the most important words being used to describe professors in reviews?\nLet’s write a custom function that tokenizes and lemmatizes our list of words. - Word tokenization: process of splitting text into individual words, called tokens. A common preprocessing step in natural language processing (NLP) so that text can be analyzed and processed more easily. Methods include whitespace tokenization, regular expression-based tokenization, and rule-based tokenization. We’ll be using the word_tokenize tokenizer from nltk, with all its defaults. - Lemmatization: process of reducing words to their base or dictionary form, called the lemma. Also a common pre-processing step in NLP, so that words with a common base form are treated the same way. For example, the lemma of “am” is “be”, of “running” is “run”, and of “mice” is “mouse”.\n\nPorterStemmer().stem(\"she\\'s\")\n\n\"she'\"\n\n\n\nword_tokenize('she\\'s a girl')\n\n['she', \"'s\", 'a', 'girl']\n\n\n\nimport string\n\n\ndef tokenize(text):\n    tk = WhitespaceTokenizer()\n    tokens = tk.tokenize(text)\n    stems = []\n    for item in tokens:\n        stems.append(PorterStemmer().stem(item).strip(string.punctuation))\n    return stems\n\nLet’s import a list of stop words, which are common English words that we will be ignoring in our analysis. sklearn provides a common list of stop words, and we can append additional words to this list. Below, we append pronouns, along with the words “class” and “student”. Feel free to add any additional words you’d like to ignore to this list later on as you try to build upon this analysis!\n\nmy_stop_words = sktext.ENGLISH_STOP_WORDS.union([\"he\",\"she\",\"his\",\"her\",\n                                              \"himself\",\"herself\", \"hers\",\"shes\"\n                                              \"class\",\"student\", 'man', 'woman', 'girl',\n                                                 'guy', 'lady', 'mr', 'mrs', 'ms'])\nmy_stop_words = my_stop_words.union([tokenize(word)[0] for word in my_stop_words])\n\nFor the purpose of analyzing review texts, we want to move from having one row for each professor to one row for each review. Lets do this with .explode() from pandas.\n\ndf_quality = df[(df['review'].apply(len) == df['quality'].apply(len))]\nq = df_quality[['pronouns','review','quality']].explode(['review','quality'], ignore_index=True).dropna()\nq['quality'] = q['quality'].astype(float)\n\n\nq.head(5)\n\n\n\n\n\n\n\n\npronouns\nreview\nquality\n\n\n\n\n0\nF\nGood experience for a class online. It was unc...\n4.0\n\n\n1\nF\nHonestly she didnt teach good at all and she w...\n2.0\n\n\n2\nF\nI think if you go by word for word in the modu...\n1.0\n\n\n3\nF\nTook her online class CSS64. We started buildi...\n1.0\n\n\n4\nF\nTook her for a late start hybrid class (Bus43)...\n3.0\n\n\n\n\n\n\n\nTFIDF vectorization is the process of assigning scores to each review in a document based on how frequently the word occurs, normalized by how frequently the word occurs in the dataset overall.\nWe’ll use TfidfVectorizer() to generate these scores. This will return a matrix, with as many rows as reviews, and as many columns as words in our dataset.\n\nvec = TfidfVectorizer(\n    tokenizer = tokenize,\n    stop_words = list(my_stop_words),\n    ngram_range = (1,4)\n)\nX = vec.fit_transform(q.review)\nfeature_names = vec.get_feature_names_out()\n\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anywh', 'becau', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh'] not in stop_words.\n  warnings.warn(\n\n\n\nfeature_names.shape\n\n(365516,)\n\n\n\nnp.random.choice(feature_names, size = 10)\n\narray([\"sense you'v\", 'work easy week class',\n       'technology(blackboard),noon lac know', 'just read quizz',\n       \"can't drop day\", 'lot question understand materi', 'class ad',\n       'languag pain', 'style rel scatter unorganized',\n       \"prof l'heureux help\"], dtype=object)\n\n\nX is a sparse matrix. We’ll now move into filtering X for: - Rows with male professors and reviews of high quality - Rows with female professors and reviews of high quality - Rows with male professors and reviews of low quality - Rows with female professors and reviews of low quality\nWe can explore feature importance in each of these to get a sense of which words and phrases are coming up most often in the data.\n\nm_pos = X[q.pronouns.eq('M') & q.quality.ge(4.5)]\n\n\nm_pos = X[q.pronouns.eq('M') & q.quality.ge(4.5)]\nf_pos = X[q.pronouns.eq('F') & q.quality.ge(4.5)]\nm_neg = X[q.pronouns.eq('M') & q.quality.le(2.5)]\nf_neg = X[q.pronouns.eq('F') & q.quality.le(2.5)]\n\n\nnp.unique(np.array(m_pos[0,:].todense()))\n\narray([0.        , 0.04840186, 0.05048048, 0.05840671, 0.0588915 ,\n       0.06440657, 0.07216968, 0.07743822, 0.08628339, 0.0890904 ,\n       0.09454997, 0.09609708, 0.0982585 , 0.09872669, 0.10101388,\n       0.10570444, 0.11337177, 0.12547472, 0.13281775, 0.13699447,\n       0.146295  , 0.15107613, 0.15781475])\n\n\nLet’s have a look at what language students are using to describe male professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(m_pos.sum(axis = 0)))[0,::-1]\nm_pos_features = feature_names[importance[:300]]\n\nPrint out the 25 most important features\n\nm_pos_features[:25]\n\narray(['comment', 'great', 'class', 'teacher', 'best', 'professor',\n       'prof', 'good', 'help', 'realli', 'make', 'easi', 'love',\n       'great teacher', 'awesom', 'know', 'learn', '', 'work', 'lot',\n       'lectur', 'amaz', 'cours', 'nice', 'test'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively.\n\nimportance = np.argsort(np.asarray(f_pos.sum(axis = 0)))[0,::-1]\nf_pos_features = feature_names[importance[:300]]\nf_pos_features[:25]\n\narray(['comment', 'great', 'class', 'teacher', 'prof', 'help',\n       'professor', 'best', 'good', 'easi', 'realli', 'work', 'nice',\n       'lot', 'love', 'make', 'learn', 'helpful', 'great teacher', '',\n       'amaz', 'great prof', 'cours', 'lectur', 'hard'], dtype=object)\n\n\nIt should be interesting if there are words exclusively used for one gender\n\n#male\nonly_m_pos = ~np.in1d(m_pos_features, f_pos_features)\nm_pos_features[only_m_pos][:25]\n\n/tmp/ipykernel_832826/3886134522.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_m_pos = ~np.in1d(m_pos_features, f_pos_features)\n\n\narray(['fantast', 'excel teacher', 'topic', 'awesom professor',\n       'excel professor', 'u', 'brilliant', 'realli enjoy', '2', 'old',\n       'amaz professor', \"professor i'v\", \"teacher i'v\", 'mark', 'b',\n       'hot', 'review', 'genuin', 'bore', \"he'll\", 'reason', 'hours',\n       'want learn', 'overal', 'feel'], dtype=object)\n\n\n\n#female\nonly_f_pos = ~np.isin(f_pos_features, m_pos_features)\nf_pos_features[only_f_pos][:25]\n\narray(['extra credit', 'especi', 'spanish', \"she'll\", 'assignments',\n       'realli nice', 'offer', 'help prof', 'class nice', 'succeed',\n       'realli want', 'alot', 'onlin class', 'knowledgable', 'group',\n       'good lectur', 'attention', 'nice help', \"i'd\", 'real world',\n       'easi grade', 'awsom', 'account', 'guid', 'semester'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe male professors negatively.\n\nimportance = np.argsort(np.asarray(m_neg.sum(axis = 0)))[0,::-1]\nm_neg_features = feature_names[importance[:300]]\nm_neg_features[:25]\n\narray(['comment', 'class', 'teach', 'hard', 'test', 'worst', 'professor',\n       'teacher', 'lectur', '', 'time', 'know', 'like', 'grade', \"don't\",\n       \"doesn't\", 'just', 'prof', 'avoid', 'bore', 'question', 'good',\n       'make', 'doe', 'read'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors negatively.\n\nimportance = np.argsort(np.asarray(f_neg.sum(axis = 0)))[0,::-1]\nf_neg_features = feature_names[importance[:300]]\nf_neg_features[:25]\n\narray(['comment', 'class', 'worst', 'grade', 'teacher', 'hard', 'teach',\n       \"don't\", \"doesn't\", 'help', 'like', 'test', '', 'just', 'time',\n       'work', 'professor', 'good', 'doe', 'question', 'make', 'know',\n       'horribl', 'learn', 'unclear'], dtype=object)\n\n\nSame analysis for exclusive words:\n\n#male\nonly_m_neg = ~np.in1d(m_neg_features, f_neg_features)\nm_neg_features[only_m_neg][:25]\n\n/tmp/ipykernel_832826/728473888.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_m_neg = ~np.in1d(m_neg_features, f_neg_features)\n\n\narray(['speak', 'hard understand', 'great', 'arrog', 'hear', 'costs',\n       'smart', 'hardest', 'taught', 'listen', 'rambl', 'english',\n       'exampl', 'let', 'sit', 'incred', 'wrote', 'knowledg', 'possible',\n       'probabl', 'avoid costs', 'gpa', 'offic', '1', \"can't teach\"],\n      dtype=object)\n\n\n\n#female\nonly_f_neg = ~np.in1d(f_neg_features, m_neg_features)\nf_neg_features[only_f_neg][:25]\n\n/tmp/ipykernel_832826/1584707764.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_f_neg = ~np.in1d(f_neg_features, m_neg_features)\n\n\narray(['late', 'disorgan', 'gave', 'advis', 'annoy', 'feedback', 'cost',\n       'agre', 'helpful', 'disorganized', 'nice person', 'avoid cost',\n       'unorganized', 'opinion', 'slow', 'quit', 'colleg', 'honestli',\n       'fan', 'papers', 'spanish', 'easi class', 'favorites', 'instead',\n       '5'], dtype=object)"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#congratulations-on-making-it-to-the-end",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#congratulations-on-making-it-to-the-end",
    "title": "Do students describe professors differently based on gender?",
    "section": "Congratulations on making it to the end!",
    "text": "Congratulations on making it to the end!\n\nWhere to from here?\n\nWe can feed these words into Ben Schmidt’s tool to derive insights by field.\nIf you’re interested in learning more about web scraping, take our courses on Web Scraping in Python\nIf you’re intersted in diving in to the world of Natural Language Processing, explore our skill track."
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#parse-lists-generated-by-gemini",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#parse-lists-generated-by-gemini",
    "title": "Do students describe professors differently based on gender?",
    "section": "Parse lists generated by GEMINI",
    "text": "Parse lists generated by GEMINI\n\nFunction parse_gemini_list\n\ndef parse_gemini_list(s: str) -&gt; list:\n    \"\"\"parse comma separated list of words remove annotations in parentheses\"\"\"\n    new = [w.strip() for w in s.split(',')]\n    for i,n in enumerate(new):\n        if '(' in n:\n            new[i] = n.split('(')[0]\n    \n    new = np.unique([x for word in new for x in tokenize(word)])\n            \n    return new\n\n\n\nRead and parse files\n\nwith open('w_pos_lex', 'r') as f:\n    w_pos_lex = parse_gemini_list(f.read())\n\n\nwith open('w_neg_lex', 'r') as f:\n    w_neg_lex = parse_gemini_list(f.read())\n\n\nwith open('m_pos_lex', 'r') as f:\n    m_pos_lex = parse_gemini_list(f.read())\n\n\nwith open('m_neg_lex', 'r') as f:\n    m_neg_lex = parse_gemini_list(f.read())\n\n\n\nCast unique lexicons for each gender and review sentiment\n\nw_pos_lex_uni = w_pos_lex[~np.isin(w_pos_lex, m_pos_lex)]\n\n\nm_pos_lex_uni = m_pos_lex[~np.isin(m_pos_lex, w_pos_lex)]\n\n\nw_neg_lex_uni = w_neg_lex[~np.isin(w_neg_lex, m_neg_lex)]\n\n\nm_neg_lex_uni = m_neg_lex[~np.isin(m_neg_lex, w_neg_lex)]\n\n\nstereo_count_w_pos = q.loc[q.pronouns.eq('F') & q.quality.ge(4.5)].review.apply(tokenize).apply(\n    np.isin, args = (w_pos_lex_uni,)).apply(np.sum)\n\n\ndist_w_pos = stereo_count_w_pos.value_counts()\ndist_w_pos.plot(kind = 'bar')\n\n\n\n\n\n\n\n\n\nstereo_count_m_pos = q.loc[q.pronouns.eq('M') & q.quality.ge(4.5)].review.apply(tokenize).apply(\n    np.isin, args = (m_pos_lex_uni,)).apply(np.sum)\n\n\ndist_m_pos = stereo_count_m_pos.value_counts()\n\n\ndist_m_pos.plot(kind = 'bar')\n\n\n\n\n\n\n\n\n\nstereo_count_w_neg = q.loc[q.pronouns.eq('F') & q.quality.le(2.5)].review.apply(tokenize).apply(\n    np.isin, args = (w_neg_lex_uni,)).apply(np.sum)\n\n\ndist_w_neg = stereo_count_w_neg.value_counts().sort_index()\n\n\ndist_w_neg.plot(kind = 'bar')\n\n\n\n\n\n\n\n\n\nstereo_count_m_neg = q.loc[q.pronouns.eq('M') & q.quality.le(2.5)].review.apply(tokenize).apply(\n    np.isin, args = (m_neg_lex_uni,)).apply(np.sum)\n\n\ndist_m_neg = stereo_count_m_neg.value_counts()\n\n\ndist_m_neg.plot(kind = 'bar')\n\n\n\n\n\n\n\n\n\ndist_m_neg\n\nreview\n0    2218\n1     240\n2      37\n3      10\n4       9\n5       1\nName: count, dtype: int64\n\n\n\nfrom scipy.stats import chi2_contingency\n\n\ntable = np.zeros((2,  max(dist_w_neg.shape[0], dist_m_neg.shape[0])), dtype = int)\ntable[0, :dist_w_neg.shape[0]] = dist_w_neg.values\ntable[1, :dist_m_neg.shape[0]] = dist_m_neg.values\n\n\ntable\n\narray([[ 278,  229,  146,   86,   37,   20,    3,    1,    1],\n       [2218,  240,   37,   10,    9,    1,    0,    0,    0]])\n\n\n\nchi2_table = pd.DataFrame(table, index = ['F', 'M'])\n\n\nchi2_table\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nF\n278\n229\n146\n86\n37\n20\n3\n1\n1\n\n\nM\n2218\n240\n37\n10\n9\n1\n0\n0\n0\n\n\n\n\n\n\n\n\nchi2_contingency(chi2_table)\n\nChi2ContingencyResult(statistic=np.float64(1073.2259818314606), pvalue=np.float64(2.3184709261716882e-226), dof=8, expected_freq=array([[6.02924005e+02, 1.13289807e+02, 4.42047648e+01, 2.31893848e+01,\n        1.11115802e+01, 5.07267793e+00, 7.24668275e-01, 2.41556092e-01,\n        2.41556092e-01],\n       [1.89307600e+03, 3.55710193e+02, 1.38795235e+02, 7.28106152e+01,\n        3.48884198e+01, 1.59273221e+01, 2.27533172e+00, 7.58443908e-01,\n        7.58443908e-01]]))\n\n\n\ntable = np.zeros((2, max(dist_w_pos.shape[0], dist_m_pos.shape[0])), dtype = int)\ntable[0, :dist_w_pos.shape[0]] = dist_w_pos.values\ntable[1, :dist_m_pos.shape[0]] = dist_m_pos.values\n\n\nchi2_table = pd.DataFrame(table, index = ['F', 'M'])\n\n\nchi2_contingency(chi2_table)\n\nChi2ContingencyResult(statistic=np.float64(35.680653072523995), pvalue=np.float64(1.1002520334386392e-06), dof=5, expected_freq=array([[6.76990876e+02, 2.24479927e+02, 6.08759124e+01, 9.13138686e+00,\n        1.26824818e+00, 2.53649635e-01],\n       [1.99200912e+03, 6.60520073e+02, 1.79124088e+02, 2.68686131e+01,\n        3.73175182e+00, 7.46350365e-01]]))"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/web_scraping.html",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/web_scraping.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "This code won’t run in wowrkspace, but you can download it locally if you are interested in learning about how we obtained the URLs of the professors used in this project.\n\n!pip install selenium!pip install webdriver-manager\n\n\nfrom selenium import webdriverfrom selenium.webdriver.chrome.service import Servicefrom webdriver_manager.chrome import ChromeDriverManagerfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECimport requestsfrom bs4 import BeautifulSoupfrom lxml import etreefrom urllib.request import urlopen\n\n\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))driver.get(\"https://www.ratemyprofessors.com/search/teachers?query=?\")# Wait for initialize, in secondswait = WebDriverWait(driver, 8)wait.until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[5]/div/div/button')))button = driver.find_element('xpath','/html/body/div[5]/div/div/button')button.click()clicks = 0while clicks &lt; 140:    if clicks%10==0:        print(f'Clicked {clicks} times.')    wait = WebDriverWait(driver, 7)    time.sleep(3)    wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[4]/button')))    show_more = driver.find_element('xpath','//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[4]/button')    show_more.click()    clicks += 1cards = driver.find_elements(By.XPATH,'//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[3]/a[*]')driver.quit()profs = [i.get_attribute('href') for i in cards]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html",
    "title": "Do students describe professors differently based on gender?",
    "section": "",
    "text": "Language plays a crucial role in shaping our perceptions and attitudes towards gender in the workplace, in classrooms, and personal relationships. Studies have shown that gender bias in language can have a significant impact on the way people are perceived and treated. For example, research has found that job advertisements that use masculine-coded language tend to attract more male applicants, while those that use feminine-coded language tend to attract more female applicants. Similarly, gendered language can perpetuate differences in the classroom.In this project, we’ll using scraped student reviews from ratemyprofessors.com to identify differences in language commonly used for male vs. female professors, and explore subtleties in how language in the classroom can be gendered.This excellent tool created by Ben Schmidt allows us to enter the words and phrases that we find in our analysis and explore them in more depth. We’ll do this at the end.Catalyst also does some incredible work on decoding gendered language."
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "title": "Do students describe professors differently based on gender?",
    "section": "3a. What additional package imports are required for data visualization and NLP?",
    "text": "3a. What additional package imports are required for data visualization and NLP?\n::: {#8d6a7a01-887d-4df1-98b6-5a3c95e35519 .cell jupyter=‘{“outputs_hidden”:false,“source_hidden”:false}’ executionTime=‘841’ lastSuccessfullyExecutedCode=’import numpy as np # For manipulating matrices during NLP\nimport nltk # Natural language toolkit from nltk.tokenize import word_tokenize # Used for breaking up strings of text (e.g. sentences) into words from nltk.stem.porter import PorterStemmer # Used to return the dictionary base of a word from nltk.tokenize import WhitespaceTokenizer # Used for breaking up strings of text (e.g. sentences) into words based on white space\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # Used to count the occurences of words and phrases from sklearn.feature_extraction import text # Using to extrat features from text"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "title": "Do students describe professors differently based on gender?",
    "section": "Task 3d. What are the most important words being used to describe professors in reviews?",
    "text": "Task 3d. What are the most important words being used to describe professors in reviews?\nLet’s write a custom function that tokenizes and lemmatizes our list of words.- Word tokenization: process of splitting text into individual words, called tokens. A common preprocessing step in natural language processing (NLP) so that text can be analyzed and processed more easily. Methods include whitespace tokenization, regular expression-based tokenization, and rule-based tokenization. We’ll be using the word_tokenize tokenizer from nltk, with all its defaults.- Lemmatization: process of reducing words to their base or dictionary form, called the lemma. Also a common pre-processing step in NLP, so that words with a common base form are treated the same way. For example, the lemma of “am” is “be”, of “running” is “run”, and of “mice” is “mouse”.\n\ndef tokenize(text):    tk = WhitespaceTokenizer()    tokens = tk.tokenize(text)    stems = []    for item in tokens:        stems.append(PorterStemmer().stem(item))    return stems    # return tokens\n\nLet’s import a list of stop words, which are common English words that we will be ignoring in our analysis. sklearn provides a common list of stop words, and we can append additional words to this list. Below, we append pronouns, along with the words “class” and “student”.\n\nmy_stop_words = text.ENGLISH_STOP_WORDS.union([\"he\",\"she\",\"his\",\"her\",                                              \"himself\",\"herself\", \"hers\",\"shes\"                                              \"class\",\"student\"])\n\nFor the purpose of analyzing review texts, we want to move from having one row for each professor to one row for each review. Lets do this with .explode() from pandas.\n\ndf_quality = df[(df['review'].apply(len) == df['quality'].apply(len))]q = df_quality[['pronouns','review','quality']].explode(['review','quality'], ignore_index=True).dropna()q['quality'] = q['quality'].astype(float)\n\nTFIDF vectorization is the process of assigning scores to each review in a document based on how frequently the word occurs, normalized by how frequently the word occurs in the dataset overall.We’ll use TfidfVectorizer() to generate these scores. This will return a matrix, with as many rows as reviews, and as many columns as words in our dataset.\n\nvec = TfidfVectorizer(tokenizer=tokenize, stop_words=my_stop_words,                     ngram_range=(1,4))X = vec.fit_transform(q['review'])feature_names = vec.get_feature_names_out()\n\nX is a sparse matrix. We’ll now move into filtering X for:- Male professors only- Female professors only- Rows with male professors and reviews of high quality - Rows with female professors and reviews of high quality - Rows with male professors and reviews of low quality - Rows with female professors and reviews of low quality We can explore feature importance in each of these to get a sense of which words and phrases are coming up most often in the data.\n\nm_pos = X[(q['pronouns']=='M') & (q['quality']&gt;=4.5),:] f_pos = X[(q['pronouns']=='F') & (q['quality']&gt;=4.5),:] m_neg = X[(q['pronouns']=='M') & (q['quality']&lt;2.5),:] f_neg = X[(q['pronouns']=='M') & (q['quality']&lt;2.5),:] \n\nLet’s have a look at what language students are using to describe male professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(m_pos.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'great', 'hi', 'veri', 'class', 'best', 'good',\n       'professor', 'realli', 'teacher', \"he'\", 'wa', 'thi', 'make',\n       'help', 'easi', 'love', 'prof', 'awesom', 'know', 'class.',\n       'learn', 'lectur', 'amaz', 'cours', 'excel', 'just', 'test',\n       'alway', 'prof.', 'lot', 'work', 'nice', 'ha', \"i'v\", 'teach',\n       'teacher.', 'hi class', 'want', 'best professor', 'him.',\n       'professor.', 'like', 'guy', 'class,', 'dr.', 'funni', 'hard',\n       'need', 'read', 'know hi', 'fun', 'clear', 'materi', 'enjoy',\n       'great teacher', 'recommend', 'studi', 'exam', 'care', 'note',\n       'guy.', 'time', 'best prof', 'understand', 'best teacher',\n       'teacher,', 'everi', 'definit', \"don't\", \"you'll\", 'it.', 'onli',\n       'everyth', 'man', 'becaus', 'ani', 'fair', 'took', 'students.',\n       'extrem', 'veri good', 'use', 'had.', 'knowledg', 'explain',\n       'prof!', 'interesting.', 'well.', 'grade', 'veri help', 'question',\n       'fantast', 'math', 'book', 'thi class', 'hi lectur', 'write',\n       'highli', 'think', 'doe', 'attend', 'actual', 'professor,',\n       'thing', 'pretti', 'stuff', 'assign', 'prof,', 'hi students.',\n       'got', 'helpful.', 'befor', 'favorit', 'peopl', 'sure', 'him!',\n       'way', 'care hi', \"it'\", 'long', 'talk', 'absolut', 'super',\n       'guy,', '-', 'passion', 'littl', 'funny,', 'tough', 'hi class.',\n       'cool', '&', 'tri', 'professor!', 'great teacher,', 'wish', 'tell',\n       'person', 'a.', 'good teacher', 'helpful,', 'final', 'make sure',\n       'year', 'expect', 'difficult', 'make class', 'come', 'ask',\n       'love thi', 'subject', 'everyon', 'teacher!', 'man.', 'hi test',\n       'look', 'u', 'material.', 'answer', 'listen', 'sens', 'pay',\n       'class wa', 'great prof.', 'paper', 'offic', 'great professor.',\n       'wa veri', 'worth', 'say', 'far', 'histori', \"you'r\", 'hi stuff',\n       'great teacher.', 'great prof!', 'know hi stuff', 'interesting,',\n       'veri easi', 'easy.', 'bit', 'learn lot', 'veri nice', \"doesn't\",\n       'veri clear', 'classes.', 'thought', \"he' veri\", 'best.', 'smart',\n       'fine.', 'anyon', \"i'v had.\", 'realli enjoy', 'onlin', 'you.',\n       'love hi', 'prepar', 'thi class.', 'stori', 'essay', 'exampl',\n       'entertain', 'attent', 'better', 'course.', 'mani', '2',\n       'homework', 'taken', 'engag', 'bad', 'great prof,', 'notes.',\n       'textbook', 'truli', 'realli know', 'awesome.', 'real',\n       'excel teacher.', \"didn't\", 'inform', 'miss', 'challeng', 'review',\n       'old', 'class!', \"professor i'v\", 'thi guy', 'problem', 'let',\n       'genuin', 'great!', 'too.', 'did', 'work.', 'topic', 'great guy.',\n       \"he'll\", 'best!', 'enjoy hi', 'brilliant', 'veri knowledg',\n       'concept', 'overal', 'bore', \"i'm\", 'dure', \"teacher i'v\",\n       'alway help', 'approach', 'stuff.', 'feel', 'thi professor',\n       'pay attent', 'intellig', 'mark', 'effort', 'midterm', 'great.',\n       'extra', 'tests.', 'veri helpful.', 'instructor', 'text', 'reason',\n       'hi stuff.', 'taught', 'quizz', 'goe', 'lab', 'end', 'teacher!!',\n       'easy,', ':)', 'lot.', 'great professor', 'know hi stuff.',\n       'probabl', 'grade.', 'quit', 'pass', 'exams.', 'fair.', 'kind',\n       'hand', 'incred', 'avail', 'realli know hi', 'highli recommend',\n       \"can't\", 'wonder', 'humor', 'lectures.', 'job', 'him,',\n       'great prof', 'major', 'hour'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(f_pos.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'veri', 'great', 'class', 'help', \"she'\", 'easi',\n       'realli', 'best', 'wa', 'good', 'professor', 'thi', 'teacher',\n       'make', 'love', 'lot', 'prof', 'class.', 'work', 'learn', 'prof.',\n       'nice', 'lectur', 'cours', 'like', 'amaz', 'want', 'excel',\n       'teacher.', 'care', 'know', 'her.', 'alway', 'ha', 'wonder',\n       'just', 'extrem', 'hard', 'awesom', 'recommend', 'dr.', 'need',\n       'test', 'fair', 'time', 'understand', 'professor.', \"don't\",\n       'studi', 'best teacher', 'teach', 'her!', 'read', 'exam', 'grade',\n       'assign', 'class,', 'veri help', 'highli', 'thing',\n       'best professor', 'helpful.', 'materi', 'clear', 'interesting.',\n       'super', \"it'\", \"i'v\", 'onlin', 'great.', 'awesome!', 'enjoy',\n       'extra', 'sure', 'ani', 'thi class', 'definit', 'students.',\n       'pretti', 'becaus', 'question', 'ladi', 'pay', 'everi', 'her,',\n       'talk', 'mrs.', 'veri nice', 'actual', \"you'll\", 'work,',\n       'passion', 'took', 'teacher!', 'veri good', 'homework',\n       \"she' veri\", 'everyth', 'fun', 'class!', 'best prof', 'professor,',\n       'well.', 'come', 'onli', 'better', '-', 'think', 'great prof.',\n       'explain', 'wa veri', 'real', 'highli recommend', 'doe', 'tough',\n       'wish', 'world', 'a.', 'excel prof', 'long', 'use', 'littl',\n       'person', 'material.', 'attent', 'great teacher.', 'sweet',\n       'veri easi', 'peopl', 'prof,', 'helpful,', 'credit', 'had.', '&',\n       'it.', 'anyth', 'great!', 'lab', 'knowledg', 'expect', 'prof!',\n       'problem', 'write', 'professor!', 'teacher,', \"you'r\", 'approach',\n       'look', 'math', 'pass', 'thi cours', 'pay attent', 'discuss',\n       'note', 'you.', 'absolut', 'class wa', 'course.', 'mani', 'ever!',\n       'befor', 'extra credit', 'sens', 'especi', 'taught', 'person.',\n       'got', 'learn lot', 'help.', 'bit', 'work.',\n       'best professor ever!', 'understand.', 'classes.', 'fair.',\n       'lectures.', 'ask', 'great prof', 'nice,', 'grade.', 'tell',\n       'year', 'lady.', 'professor ever!', 'point', 'thought', 'attend',\n       'cool', 'quizz', 'make sure', 'way', 'veri helpful.', 'answer',\n       'students,', 'fine.', \"she'll\", 'easi understand.', 'amazing.',\n       'did', 'end', 'instructor', 'great teacher', 'project', 'challeng',\n       'respect', 'offer', 'great professor!', 'entertain', 'essay',\n       'good lectur', 'veri clear', 'favorit', 'difficult', 'english',\n       'exampl', 'anyon', 'truli', 'great prof!', 'great teacher,', 'goe',\n       'realli want', \"i'm\", 'nice lady.', 'intellig', 'classes!',\n       'awsom', 'concept', 'veri helpful,', 'lot.', 'book', \"doesn't\",\n       'group', 'lectures,', 'them.', 'listen', 'kind', 'smart', '3',\n       'funni', \"i'd\", 'easy.', 'veri fair', 'great professor,', 'tri',\n       \"she' realli\", 'make class', 'open', 'guid', 'recommend her.',\n       'inspir', 'spanish', 'account', 'probabl', 'offic', 'easi grade!',\n       'paper', 'school', 'woman', 'great prof,', 'great teacher!',\n       'questions.', 'taken', 'help prof', 'real world', 'love her!',\n       'stori', 'good professor.', 'let', \"isn't\", 'effort', \"prof i'v\",\n       'worth', 'alot', 'avail', 'funny.', 'subject', 'book.', 'hour',\n       'job', 'too.', 'yummer', 'best!!', 'ã\\x82â', 'knowledgable.',\n       'research', 'say', 'clearli', 'engag', 'realli know', 'helpful!',\n       'requir', 'prepar', 'demand', 'exams.', 'midterm', 'thank', 'far',\n       'great professor.', 'easy,'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe male professors negatively.\n\nimportance = np.argsort(np.asarray(m_neg.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'hi', 'thi', 'class', 'veri', 'wa', 'teach', 'worst',\n       'test', 'lectur', \"doesn't\", 'class.', 'like', 'hard', 'just',\n       \"don't\", 'professor', 'doe', 'know', 'grade', 'make', 'avoid',\n       'bore', 'teacher', \"he'\", 'question', 'onli', 'terribl', 'good',\n       'read', 'horribl', 'ha', 'learn', 'cours', 'prof', 'time', 'exam',\n       'understand', 'thi class', 'talk', 'becaus', 'help', 'realli',\n       'guy', 'him.', 'explain', 'bad', 'say', 'ask', 'ani', 'hi class',\n       \"can't\", 'thing', 'think', \"didn't\", 'way', 'book', 'thi guy',\n       'noth', 'need', 'materi', 'answer', 'wast', 'hi lectur', 'extrem',\n       'work', 'everi', 'want', 'expect', 'nice', 'teacher.', 'class,',\n       'assign', 'worst professor', 'all.', 'studi', 'professor.', 'did',\n       'note', 'anyth', 'use', \"i'v\", 'difficult', 'final', 'unclear',\n       'tell', 'drop', 'hi test', 'complet', 'worst prof', 'confus',\n       'dont', 'man', 'tri', 'absolut', 'got', 'time.', 'everyth', '-',\n       'easi', 'had.', 'rude', 'it.', 'write', 'fail', 'stay',\n       'worst teacher', '3', 'point', 'math', 'hard.', 'class wa', 'lot',\n       'mark', \"it'\", 'day', 'took', 'students.', 'clear', 'anoth',\n       'homework', 'tests.', 'useless', 'recommend', 'awful.', 'peopl',\n       'goe', 'littl', 'entir', 'unless', 'word', 'hi class.', 'someon',\n       'actual', 'come', 'textbook', 'hour', 'boring.', 'thi professor',\n       'lectures.', 'problem', 'dure', 'attend', 'better', 'half', 'went',\n       'veri hard', 'aw', 'care', 'pass', 'doesnt', 'prepar', 'everyon',\n       'person', 'paper', 'course.', 'ever.', 'look', '2', 'whi', 'mean',\n       'feel', 'year', 'arrog', 'midterm', 'prof.', 'ask question',\n       'thi class.', 'u', 'alway', \"i'm\", 'pretti', \"you'r\", 'hardest',\n       'speak', 'hate', 'imposs', 'old', 'befor', '&', 'end', 'thi cours',\n       'poor', 'subject', 'semest', 'said', 'told', 'guy,', 'someth',\n       'test.', 'basic', 'grade.', 'project', 'thi man', \"isn't\", 'mani',\n       'onlin', 'rambl', 'listen', 'idea', 'sit', 'base', 'differ',\n       'tough', 'incred', 'taught', 'essay', 'him,', 'away', 'hear',\n       'major', 'you.', \"won't\", 'terrible.', 'school', 'total', 'random',\n       'refus', 'concept', 'teach.', 'high', 'sure', 'exampl', 'slide',\n       'probabl', \"you'll\", 'anything.', 'veri difficult', 'week',\n       'hi teach', 'averag', 'questions.', 'follow', 'smart', 'save',\n       \"i'v had.\", 'possibl', 'avoid thi', 'right', 'life', 'wrote',\n       'far', 'boring,', 'bore lectur', 'topic', 'offic', 'knowledg',\n       'long', 'present', 'costs.', 'hi note', 'dr.', \"he'll\",\n       'answer question', 'believ', 'worth', 'pleas', \"don't know\", 'els',\n       'lab', '10', 'wrong.', 'run', 'harder', 'material.', 'wast time.',\n       'teacher,', \"doesn't explain\", 'let', 'nice guy', 'again.',\n       'veri bore', 'definit', 'quizz', 'chang', 'email', 'teaching.',\n       'them.', \"can't teach\", 'hi exam', 'minut', 'lecture.',\n       'thi teacher', 'pick', 'veri unclear', 'taken', 'avoid costs.',\n       'hard,', 'theori', 'book.', 'life.', \"wasn't\", 'suck', 'guy.',\n       ':(', \"worst professor i'v\", 'text', 'prof ever.', '....', 'group'],\n      dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively.\n\nimportance = np.argsort(np.asarray(f_neg.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'hi', 'thi', 'class', 'veri', 'wa', 'teach', 'worst',\n       'test', 'lectur', \"doesn't\", 'class.', 'like', 'hard', 'just',\n       \"don't\", 'professor', 'doe', 'know', 'grade', 'make', 'avoid',\n       'bore', 'teacher', \"he'\", 'question', 'onli', 'terribl', 'good',\n       'read', 'horribl', 'ha', 'learn', 'cours', 'prof', 'time', 'exam',\n       'understand', 'thi class', 'talk', 'becaus', 'help', 'realli',\n       'guy', 'him.', 'explain', 'bad', 'say', 'ask', 'ani', 'hi class',\n       \"can't\", 'thing', 'think', \"didn't\", 'way', 'book', 'thi guy',\n       'noth', 'need', 'materi', 'answer', 'wast', 'hi lectur', 'extrem',\n       'work', 'everi', 'want', 'expect', 'nice', 'teacher.', 'class,',\n       'assign', 'worst professor', 'all.', 'studi', 'professor.', 'did',\n       'note', 'anyth', 'use', \"i'v\", 'difficult', 'final', 'unclear',\n       'tell', 'drop', 'hi test', 'complet', 'worst prof', 'confus',\n       'dont', 'man', 'tri', 'absolut', 'got', 'time.', 'everyth', '-',\n       'easi', 'had.', 'rude', 'it.', 'write', 'fail', 'stay',\n       'worst teacher', '3', 'point', 'math', 'hard.', 'class wa', 'lot',\n       'mark', \"it'\", 'day', 'took', 'students.', 'clear', 'anoth',\n       'homework', 'tests.', 'useless', 'recommend', 'awful.', 'peopl',\n       'goe', 'littl', 'entir', 'unless', 'word', 'hi class.', 'someon',\n       'actual', 'come', 'textbook', 'hour', 'boring.', 'thi professor',\n       'lectures.', 'problem', 'dure', 'attend', 'better', 'half', 'went',\n       'veri hard', 'aw', 'care', 'pass', 'doesnt', 'prepar', 'everyon',\n       'person', 'paper', 'course.', 'ever.', 'look', '2', 'whi', 'mean',\n       'feel', 'year', 'arrog', 'midterm', 'prof.', 'ask question',\n       'thi class.', 'u', 'alway', \"i'm\", 'pretti', \"you'r\", 'hardest',\n       'speak', 'hate', 'imposs', 'old', 'befor', '&', 'end', 'thi cours',\n       'poor', 'subject', 'semest', 'said', 'told', 'guy,', 'someth',\n       'test.', 'basic', 'grade.', 'project', 'thi man', \"isn't\", 'mani',\n       'onlin', 'rambl', 'listen', 'idea', 'sit', 'base', 'differ',\n       'tough', 'incred', 'taught', 'essay', 'him,', 'away', 'hear',\n       'major', 'you.', \"won't\", 'terrible.', 'school', 'total', 'random',\n       'refus', 'concept', 'teach.', 'high', 'sure', 'exampl', 'slide',\n       'probabl', \"you'll\", 'anything.', 'veri difficult', 'week',\n       'hi teach', 'averag', 'questions.', 'follow', 'smart', 'save',\n       \"i'v had.\", 'possibl', 'avoid thi', 'right', 'life', 'wrote',\n       'far', 'boring,', 'bore lectur', 'topic', 'offic', 'knowledg',\n       'long', 'present', 'costs.', 'hi note', 'dr.', \"he'll\",\n       'answer question', 'believ', 'worth', 'pleas', \"don't know\", 'els',\n       'lab', '10', 'wrong.', 'run', 'harder', 'material.', 'wast time.',\n       'teacher,', \"doesn't explain\", 'let', 'nice guy', 'again.',\n       'veri bore', 'definit', 'quizz', 'chang', 'email', 'teaching.',\n       'them.', \"can't teach\", 'hi exam', 'minut', 'lecture.',\n       'thi teacher', 'pick', 'veri unclear', 'taken', 'avoid costs.',\n       'hard,', 'theori', 'book.', 'life.', \"wasn't\", 'suck', 'guy.',\n       ':(', \"worst professor i'v\", 'text', 'prof ever.', '....', 'group'],\n      dtype=object)"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#congratulations-on-making-it-to-the-end-where-to-from-here--we-can-feed-these-words-into-ben-schmidts-tool-to-derive-insights-by-field.--if-youre-interested-in-learning-more-about-web-scraping-take-our-courses-on-web-scraping-in-python--if-youre-intersted-in-diving-in-to-the-world-of-natural-language-processing-explore-our-skill-track.",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#congratulations-on-making-it-to-the-end-where-to-from-here--we-can-feed-these-words-into-ben-schmidts-tool-to-derive-insights-by-field.--if-youre-interested-in-learning-more-about-web-scraping-take-our-courses-on-web-scraping-in-python--if-youre-intersted-in-diving-in-to-the-world-of-natural-language-processing-explore-our-skill-track.",
    "title": "Do students describe professors differently based on gender?",
    "section": "Congratulations on making it to the end! ### Where to from here?- We can feed these words into Ben Schmidt’s tool to derive insights by field.- If you’re interested in learning more about web scraping, take our courses on Web Scraping in Python- If you’re intersted in diving in to the world of Natural Language Processing, explore our skill track.",
    "text": "Congratulations on making it to the end! ### Where to from here?- We can feed these words into Ben Schmidt’s tool to derive insights by field.- If you’re interested in learning more about web scraping, take our courses on Web Scraping in Python- If you’re intersted in diving in to the world of Natural Language Processing, explore our skill track."
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/ev_charging/notebook.html",
    "href": "datacamp_projects/workspace/SQL/ev_charging/notebook.html",
    "title": "SQL - Electric vehicle charging",
    "section": "",
    "text": "As electronic vehicles (EVs) become more popular, there is an increasing need for access to charging stations, also known as ports. To that end, many modern apartment buildings have begun retrofitting their parking garages to include shared charging stations. A charging station is shared if it is accessible by anyone in the building.\n\nBut with increasing demand comes competition for these ports — nothing is more frustrating than coming home to find no charging stations available! In this project, you will use a dataset to help apartment building managers better understand their tenants’ EV charging habits.\nThe data has been loaded into a PostgreSQL database with a table named charging_sessions with the following columns:"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#unique-users-per-garage",
    "href": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#unique-users-per-garage",
    "title": "SQL - Electric vehicle charging",
    "section": "Unique users per garage",
    "text": "Unique users per garage\nFind the number of unique individuals that use each garage’s shared charging stations. The output should contain two columns: garage_id and num_unique_users. Sort your results by the number of unique users from highest to lowest. Save the result as unique_users_per_garage.\n\n-- unique_users_per_garage\nSELECT garage_id, COUNT(DISTINCT user_id) as num_unique_users\nFROM charging_sessions\nWHERE user_type = 'Shared'\nGROUP BY garage_id\nORDER BY num_unique_users DESC\n\n\n\n\n\n\n\n\ngarage_id\nnum_unique_users\n\n\n\n\n0\nBl2\n18\n\n\n1\nAsO2\n17\n\n\n2\nUT9\n16\n\n\n3\nAdO3\n3\n\n\n4\nMS1\n2\n\n\n5\nSR2\n2\n\n\n6\nAdA1\n1\n\n\n7\nRis\n1"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#most-popular-starting-times-per-weekday-for-shared-charging-columns",
    "href": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#most-popular-starting-times-per-weekday-for-shared-charging-columns",
    "title": "SQL - Electric vehicle charging",
    "section": "Most popular starting times per weekday for shared charging columns",
    "text": "Most popular starting times per weekday for shared charging columns\nFind the top 10 most popular charging start times (by weekday and start hour) for sessions that use shared charging stations. Your result should contain three columns: weekdays_plugin, start_plugin_hour, and a column named num_charging_sessions containing the number of plugins on that weekday at that hour. Sort your results from the most to the least number of sessions. Save the result as most_popular_shared_start_times.\n\n-- most_popular_shared_start_times\nSELECT weekdays_plugin, start_plugin_hour, Count(*) as num_charging_sessions\nFROM charging_sessions\nWHERE user_type = 'Shared'\nGROUP BY weekdays_plugin, start_plugin_hour\nORDER BY num_charging_sessions DESC\nLIMIT 10\n\n\n\n\n\n\n\n\nweekdays_plugin\nstart_plugin_hour\nnum_charging_sessions\n\n\n\n\n0\nSunday\n17\n30\n\n\n1\nFriday\n15\n28\n\n\n2\nThursday\n19\n26\n\n\n3\nThursday\n16\n26\n\n\n4\nWednesday\n19\n25\n\n\n5\nSunday\n18\n25\n\n\n6\nSunday\n15\n25\n\n\n7\nMonday\n15\n24\n\n\n8\nFriday\n16\n24\n\n\n9\nTuesday\n16\n23"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#users-with-longest-avergae-charging-time-on-shared-columns",
    "href": "datacamp_projects/workspace/SQL/ev_charging/notebook.html#users-with-longest-avergae-charging-time-on-shared-columns",
    "title": "SQL - Electric vehicle charging",
    "section": "Users with longest avergae charging time on shared columns",
    "text": "Users with longest avergae charging time on shared columns\nFind the users whose average charging duration last longer than 10 hours when using shared charging stations. Your result should contain two columns: user_id and avg_charging_duration. Sort your result from highest to lowest average charging duration. Save the result as long_duration_shared_users.\n\n-- long_duration_shared_users\nSELECT * FROM(\n    SELECT user_id, AVG(duration_hours) as avg_charging_duration\n    FROM public.charging_sessions\n    WHERE user_type = 'Shared' \n    GROUP BY public.charging_sessions.user_id\n    ORDER BY avg_charging_duration DESC\n    ) as calc\nWHERE calc.avg_charging_duration is not null AND calc.avg_charging_duration &gt; 10\n\n\n\n\n\n\n\n\nuser_id\navg_charging_duration\n\n\n\n\n0\nShare-9\n16.845833\n\n\n1\nShare-17\n12.894556\n\n\n2\nShare-25\n12.214475\n\n\n3\nShare-18\n12.088807\n\n\n4\nShare-8\n11.550431\n\n\n5\nAdO3-1\n10.369387"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html",
    "href": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html",
    "title": "SQL - Mental health of international students",
    "section": "",
    "text": "Illustration of silhouetted heads"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html#quick-look-at-the-data",
    "href": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html#quick-look-at-the-data",
    "title": "SQL - Mental health of international students",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n-- Run this code to view the data in students\nSELECT * \nFROM students;\n\n\n\n\n\n\n\n\ninter_dom\nregion\ngender\nacademic\nage\nage_cate\nstay\nstay_cate\njapanese\njapanese_cate\nenglish\nenglish_cate\nintimate\nreligion\nsuicide\ndep\ndeptype\ntodep\ndepsev\ntosc\napd\nahome\naph\nafear\nacs\naguilt\namiscell\ntoas\npartner\nfriends\nparents\nrelative\nprofess\nphone\ndoctor\nreli\nalone\nothers\ninternet\npartner_bi\nfriends_bi\nparents_bi\nrelative_bi\nprofessional_bi\nphone_bi\ndoctor_bi\nreligion_bi\nalone_bi\nothers_bi\ninternet_bi\n\n\n\n\n0\nInter\nSEA\nMale\nGrad\n24.0\n4.0\n5.0\nLong\n3.0\nAverage\n5.0\nHigh\n\nYes\nNo\nNo\nNo\n0.0\nMin\n34.0\n23.0\n9.0\n11.0\n8.0\n11.0\n2.0\n27.0\n91.0\n5.0\n5.0\n6.0\n3.0\n2.0\n1.0\n4.0\n1.0\n3.0\n4.0\nNaN\nYes\nYes\nYes\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n1\nInter\nSEA\nMale\nGrad\n28.0\n5.0\n1.0\nShort\n4.0\nHigh\n4.0\nHigh\n\nNo\nNo\nNo\nNo\n2.0\nMin\n48.0\n8.0\n7.0\n5.0\n4.0\n3.0\n2.0\n10.0\n39.0\n7.0\n7.0\n7.0\n4.0\n4.0\n4.0\n4.0\n1.0\n1.0\n1.0\nNaN\nYes\nYes\nYes\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n2\nInter\nSEA\nMale\nGrad\n25.0\n4.0\n6.0\nLong\n4.0\nHigh\n4.0\nHigh\nYes\nYes\nNo\nNo\nNo\n2.0\nMin\n41.0\n13.0\n4.0\n7.0\n6.0\n4.0\n3.0\n14.0\n51.0\n3.0\n3.0\n3.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n3\nInter\nEA\nFemale\nGrad\n29.0\n5.0\n1.0\nShort\n2.0\nLow\n3.0\nAverage\nNo\nNo\nNo\nNo\nNo\n3.0\nMin\n37.0\n16.0\n10.0\n10.0\n8.0\n6.0\n4.0\n21.0\n75.0\n5.0\n5.0\n5.0\n5.0\n5.0\n2.0\n2.0\n2.0\n4.0\n4.0\nNaN\nYes\nYes\nYes\nYes\nYes\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n4\nInter\nEA\nFemale\nGrad\n28.0\n5.0\n1.0\nShort\n1.0\nLow\n3.0\nAverage\nYes\nNo\nNo\nNo\nNo\n3.0\nMin\n37.0\n15.0\n12.0\n5.0\n8.0\n7.0\n4.0\n31.0\n82.0\n5.0\n5.0\n5.0\n2.0\n5.0\n2.0\n5.0\n5.0\n4.0\n4.0\nNaN\nYes\nYes\nYes\nNo\nYes\nNo\nYes\nYes\nNo\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n281\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n128\n140\n\n\n\n\n\n\n\n\n\n\n\n282\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n137\n131\n\n\n\n\n\n\n\n\n\n\n\n283\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n66\n202\n\n\n\n\n\n\n\n\n\n\n\n284\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n61\n207\n\n\n\n\n\n\n\n\n\n\n\n285\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n30\n238\n\n\n\n\n\n\n\n\n\n\n\n\n\n286 rows × 50 columns"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html#summary-statistics-of-internationsl-students-conditioned-on-their-length-of-stay",
    "href": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html#summary-statistics-of-internationsl-students-conditioned-on-their-length-of-stay",
    "title": "SQL - Mental health of international students",
    "section": "Summary statistics of internationsl students conditioned on their length of stay",
    "text": "Summary statistics of internationsl students conditioned on their length of stay\n\nExplore and analyze the students data to see how the length of stay (stay) impacts the average mental health diagnostic scores of the international students present in the study.\n\n\n\nReturn a table with nine rows and five columns.\n\n\nThe five columns should be aliased as: stay, count_int, average_phq, average_scs, and average_as, in that order.\n\n\nThe average columns should contain the average of the todep (PHQ-9 test), tosc (SCS test), and toas (ASISS test) columns for each length of stay, rounded to two decimal places.\n\n\nThe count_int column should be the number of international students for each length of stay.\n\n\nSort the results by the length of stay in descending order.\n\n\n\n-- query\nSELECT \n    stay, \n    COUNT(inter_dom) AS count_int,\n    ROUND(AVG(todep),2) AS average_phq,\n    ROUND(AVG(tosc),2) AS average_scs,\n    ROUND(AVG(toas),2) AS average_as\nFROM students\nWHERE inter_dom = 'Inter'\nGROUP BY stay\nORDER BY stay DESC\n\n\n\n\n\n\n\n\nstay\ncount_int\naverage_phq\naverage_scs\naverage_as\n\n\n\n\n0\n10\n1\n13.00\n32.00\n50.00\n\n\n1\n8\n1\n10.00\n44.00\n65.00\n\n\n2\n7\n1\n4.00\n48.00\n45.00\n\n\n3\n6\n3\n6.00\n38.00\n58.67\n\n\n4\n5\n1\n0.00\n34.00\n91.00\n\n\n5\n4\n14\n8.57\n33.93\n87.71\n\n\n6\n3\n46\n9.09\n37.13\n78.00\n\n\n7\n2\n39\n8.28\n37.08\n77.67\n\n\n8\n1\n95\n7.48\n38.11\n72.80"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html",
    "title": "Exploring ingredients of cosmetics",
    "section": "",
    "text": "Whenever I want to try a new cosmetic item, it’s so difficult to choose. It’s actually more than difficult. It’s sometimes scary because new items that I’ve never tried end up giving me skin trouble. We know the information we need is on the back of each product, but it’s really hard to interpret those ingredient lists unless you’re a chemist. You may be able to relate to this situation.\n\n\n\n\n\nSo instead of buying and hoping for the best, why don’t we use data science to help us predict which products may be good fits for us? In this notebook, we are going to create a content-based recommendation system where the ‘content’ will be the chemical components of cosmetics. Specifically, we will process ingredient lists for 1472 cosmetics on Sephora via word embedding, then visualize ingredient similarity using a machine learning method called t-SNE and an interactive visualization library called Bokeh. Let’s inspect our data first.\n\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.manifold import TSNE\n\n# Load the data\ndf = pd.read_csv('datasets/cosmetics.csv')\n\n# Check the first five rows \ndisplay(df.sample(5))\n\n# Inspect the types of products\nprint(df['Label'].value_counts())\n\n\n\n\n\n\n\n\nLabel\nBrand\nName\nPrice\nRank\nIngredients\nCombination\nDry\nNormal\nOily\nSensitive\n\n\n\n\n63\nMoisturizer\nDR. JART+\nCicapair ™ Tiger Grass Color Correcting Treatm...\n52\n4.2\nWater, Centella Asiatica Leaf Water, Isononyl ...\n1\n1\n1\n1\n1\n\n\n1458\nSun protect\nCOOLA\nSport Continuous Spray SPF 30 - Unscented\n32\n5.0\nAlcohol (Organic), Algae Extract (Organic), Al...\n1\n1\n1\n1\n1\n\n\n696\nTreatment\nPERRICONE MD\nNo Foundation Foundation Serum SPF 30\n60\n4.3\nCyclopentasiloxane, Dimethicone, Water, Titani...\n1\n1\n1\n1\n1\n\n\n357\nCleanser\nKATE SOMERVILLE\nEradiKate® Daily Cleanser Acne Treatment\n38\n4.3\nWater, Sodium Cocoyl Isethionate, Coco-Glucosi...\n1\n0\n1\n1\n0\n\n\n279\nMoisturizer\nCLINIQUE\nLimited Edition Dramatically Different Moistur...\n39\n0.0\nWater, Mineral Oil/Paraffinum Liquidum/Huile M...\n1\n1\n0\n0\n0\n\n\n\n\n\n\n\nMoisturizer    298\nCleanser       281\nFace Mask      266\nTreatment      248\nEye cream      209\nSun protect    170\nName: Label, dtype: int64"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#cosmetics-chemicals-its-complicated",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#cosmetics-chemicals-its-complicated",
    "title": "Exploring ingredients of cosmetics",
    "section": "",
    "text": "Whenever I want to try a new cosmetic item, it’s so difficult to choose. It’s actually more than difficult. It’s sometimes scary because new items that I’ve never tried end up giving me skin trouble. We know the information we need is on the back of each product, but it’s really hard to interpret those ingredient lists unless you’re a chemist. You may be able to relate to this situation.\n\n\n\n\n\nSo instead of buying and hoping for the best, why don’t we use data science to help us predict which products may be good fits for us? In this notebook, we are going to create a content-based recommendation system where the ‘content’ will be the chemical components of cosmetics. Specifically, we will process ingredient lists for 1472 cosmetics on Sephora via word embedding, then visualize ingredient similarity using a machine learning method called t-SNE and an interactive visualization library called Bokeh. Let’s inspect our data first.\n\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.manifold import TSNE\n\n# Load the data\ndf = pd.read_csv('datasets/cosmetics.csv')\n\n# Check the first five rows \ndisplay(df.sample(5))\n\n# Inspect the types of products\nprint(df['Label'].value_counts())\n\n\n\n\n\n\n\n\nLabel\nBrand\nName\nPrice\nRank\nIngredients\nCombination\nDry\nNormal\nOily\nSensitive\n\n\n\n\n63\nMoisturizer\nDR. JART+\nCicapair ™ Tiger Grass Color Correcting Treatm...\n52\n4.2\nWater, Centella Asiatica Leaf Water, Isononyl ...\n1\n1\n1\n1\n1\n\n\n1458\nSun protect\nCOOLA\nSport Continuous Spray SPF 30 - Unscented\n32\n5.0\nAlcohol (Organic), Algae Extract (Organic), Al...\n1\n1\n1\n1\n1\n\n\n696\nTreatment\nPERRICONE MD\nNo Foundation Foundation Serum SPF 30\n60\n4.3\nCyclopentasiloxane, Dimethicone, Water, Titani...\n1\n1\n1\n1\n1\n\n\n357\nCleanser\nKATE SOMERVILLE\nEradiKate® Daily Cleanser Acne Treatment\n38\n4.3\nWater, Sodium Cocoyl Isethionate, Coco-Glucosi...\n1\n0\n1\n1\n0\n\n\n279\nMoisturizer\nCLINIQUE\nLimited Edition Dramatically Different Moistur...\n39\n0.0\nWater, Mineral Oil/Paraffinum Liquidum/Huile M...\n1\n1\n0\n0\n0\n\n\n\n\n\n\n\nMoisturizer    298\nCleanser       281\nFace Mask      266\nTreatment      248\nEye cream      209\nSun protect    170\nName: Label, dtype: int64"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#focus-on-one-product-category-and-one-skin-type",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#focus-on-one-product-category-and-one-skin-type",
    "title": "Exploring ingredients of cosmetics",
    "section": "2. Focus on one product category and one skin type",
    "text": "2. Focus on one product category and one skin type\n\nThere are six categories of product in our data (moisturizers, cleansers, face masks, eye creams, and sun protection) and there are five different skin types (combination, dry, normal, oily and sensitive). Because individuals have different product needs as well as different skin types, let’s set up our workflow so its outputs (a t-SNE model and a visualization of that model) can be customized. For the example in this notebook, let’s focus in on moisturizers for those with dry skin by filtering the data accordingly.\n\n\ndf.columns\n\nIndex(['Label', 'Brand', 'Name', 'Price', 'Rank', 'Ingredients', 'Combination',\n       'Dry', 'Normal', 'Oily', 'Sensitive'],\n      dtype='object')\n\n\n\ndf.Label.unique()\n\narray(['Moisturizer', 'Cleanser', 'Treatment', 'Face Mask', 'Eye cream',\n       'Sun protect'], dtype=object)\n\n\n\n# Filter for moisturizers\nmoisturizers = df.query('Label == \"Moisturizer\"')\n\n# Filter for dry skin as well\nmoisturizers_dry = moisturizers.query('Dry == 1')\n\n# Reset index\nmoisturizers_dry = moisturizers_dry.reset_index(drop = True)"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#tokenizing-the-ingredients",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#tokenizing-the-ingredients",
    "title": "Exploring ingredients of cosmetics",
    "section": "3. Tokenizing the ingredients",
    "text": "3. Tokenizing the ingredients\n\nTo get to our end goal of comparing ingredients in each product, we first need to do some preprocessing tasks and bookkeeping of the actual words in each product’s ingredients list. The first step will be tokenizing the list of ingredients in Ingredients column. After splitting them into tokens, we’ll make a binary bag of words. Then we will create a dictionary with the tokens, ingredient_idx, which will have the following format:\n\n\n{ “ingredient”: index value, … }\n\n\n# Initialize dictionary, list, and initial index\ningredient_idx = {}\ncorpus = []\nidx = 0\n\n# For loop for tokenization\nfor i in range(len(moisturizers_dry)):    \n    ingredients = moisturizers_dry['Ingredients'][i]\n    ingredients_lower = ingredients.lower()\n    tokens = ingredients_lower.split(', ')\n    corpus.append(tokens)\n    for ingredient in tokens:\n        if ingredient not in ingredient_idx:\n            ingredient_idx[ingredient] = idx\n            idx += 1\n            \n# Check the result \nprint(\"The index for decyl oleate is\", ingredient_idx['decyl oleate'])\n\nThe index for decyl oleate is 25"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#initializing-a-document-term-matrix-dtm",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#initializing-a-document-term-matrix-dtm",
    "title": "Exploring ingredients of cosmetics",
    "section": "4. Initializing a document-term matrix (DTM)",
    "text": "4. Initializing a document-term matrix (DTM)\n\nThe next step is making a document-term matrix (DTM). Here each cosmetic product will correspond to a document, and each chemical composition will correspond to a term. This means we can think of the matrix as a “cosmetic-ingredient” matrix. The size of the matrix should be as the picture shown below.  To create this matrix, we’ll first make an empty matrix filled with zeros. The length of the matrix is the total number of cosmetic products in the data. The width of the matrix is the total number of ingredients. After initializing this empty matrix, we’ll fill it in the following tasks.\n\n\n# Get the number of items and tokens \nM = moisturizers_dry.shape[0]\nN = len(ingredient_idx.keys())\n\n# Initialize a matrix of zeros\nA = np.zeros((M, N))"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#creating-a-counter-function",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#creating-a-counter-function",
    "title": "Exploring ingredients of cosmetics",
    "section": "5. Creating a counter function",
    "text": "5. Creating a counter function\n\nBefore we can fill the matrix, let’s create a function to count the tokens (i.e., an ingredients list) for each row. Our end goal is to fill the matrix with 1 or 0: if an ingredient is in a cosmetic, the value is 1. If not, it remains 0. The name of this function, oh_encoder, will become clear next.\n\n\n# Define the oh_encoder function\ndef oh_encoder(tokens):\n    x = [0 for i in range(N)]\n    for ingredient in tokens:\n        # Get the index for each ingredient\n        idx = ingredient_idx[ingredient]\n        # Put 1 at the corresponding indices\n        x[idx] = 1\n    return x"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#the-cosmetic-ingredient-matrix",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#the-cosmetic-ingredient-matrix",
    "title": "Exploring ingredients of cosmetics",
    "section": "6. The Cosmetic-Ingredient matrix!",
    "text": "6. The Cosmetic-Ingredient matrix!\n\nNow we’ll apply the oh_encoder() functon to the tokens in corpus and set the values at each row of this matrix. So the result will tell us what ingredients each item is composed of. For example, if a cosmetic item contains water, niacin, decyl aleate and sh-polypeptide-1, the outcome of this item will be as follows.  This is what we called one-hot encoding. By encoding each ingredient in the items, the Cosmetic-Ingredient matrix will be filled with binary values.\n\n\n# Make a document-term matrix\ni = 0\nfor tokens in corpus:\n    A[i, :] = oh_encoder(tokens)\n    i += 1"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#dimension-reduction-with-t-sne",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#dimension-reduction-with-t-sne",
    "title": "Exploring ingredients of cosmetics",
    "section": "7. Dimension reduction with t-SNE",
    "text": "7. Dimension reduction with t-SNE\n\nThe dimensions of the existing matrix is (190, 2233), which means there are 2233 features in our data. For visualization, we should downsize this into two dimensions. We’ll use t-SNE for reducing the dimension of the data here.\n\n\nT-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique that is well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, this technique can reduce the dimension of data while keeping the similarities between the instances. This enables us to make a plot on the coordinate plane, which can be said as vectorizing. All of these cosmetic items in our data will be vectorized into two-dimensional coordinates, and the distances between the points will indicate the similarities between the items.\n\n\n# Dimension reduction with t-SNE\nmodel = TSNE(n_components = 2, learning_rate=200, random_state = 42)\ntsne_features = model.fit_transform(A)\n\n# Make X, Y columns \nmoisturizers_dry['X'] = tsne_features[:,0]\nmoisturizers_dry['Y'] = tsne_features[:,1]"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#lets-map-the-items-with-bokeh",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#lets-map-the-items-with-bokeh",
    "title": "Exploring ingredients of cosmetics",
    "section": "8. Let’s map the items with Bokeh",
    "text": "8. Let’s map the items with Bokeh\n\nWe are now ready to start creating our plot. With the t-SNE values, we can plot all our items on the coordinate plane. And the coolest part here is that it will also show us the name, the brand, the price and the rank of each item. Let’s make a scatter plot using Bokeh and add a hover tool to show that information. Note that we won’t display the plot yet as we will make some more additions to it.\n\n\nfrom bokeh.io import show, output_notebook, push_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool\noutput_notebook()\n\n# Make a source and a scatter plot  \nsource = ColumnDataSource(moisturizers_dry)\nplot = figure(x_axis_label = 'X', \n              y_axis_label = 'Y', \n              width = 500, height = 400)\nplot.circle(x = 'X', \n    y = 'Y', \n    source = source, \n    size = 10, color = '#FF7373', alpha = .8)\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\nGlyphRenderer(id = '1195', …)data_source = ColumnDataSource(id='1157', ...),glyph = Circle(id='1193', ...),hover_glyph = None,js_event_callbacks = {},js_property_callbacks = {},level = 'glyph',muted = False,muted_glyph = None,name = None,nonselection_glyph = Circle(id='1194', ...),selection_glyph = None,subscribed_events = [],tags = [],view = CDSView(id='1196', ...),visible = True,x_range_name = 'default',y_range_name = 'default')"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#adding-a-hover-tool",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#adding-a-hover-tool",
    "title": "Exploring ingredients of cosmetics",
    "section": "9. Adding a hover tool",
    "text": "9. Adding a hover tool\n\nWhy don’t we add a hover tool? Adding a hover tool allows us to check the information of each item whenever the cursor is directly over a glyph. We’ll add tooltips with each product’s name, brand, price, and rank (i.e., rating).\n\n\n# Create a HoverTool object\nhover = HoverTool(tooltips = [('Item', '@Name'),\n                              ('Brand', '@Brand'),\n                              ('Price', '$@Price'),\n                              ('Rank', '@Rank')])\nplot.add_tools(hover)"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#mapping-the-cosmetic-items",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#mapping-the-cosmetic-items",
    "title": "Exploring ingredients of cosmetics",
    "section": "10. Mapping the cosmetic items",
    "text": "10. Mapping the cosmetic items\n\nFinally, it’s show time! Let’s see how the map we’ve made looks like. Each point on the plot corresponds to the cosmetic items. Then what do the axes mean here? The axes of a t-SNE plot aren’t easily interpretable in terms of the original data. Like mentioned above, t-SNE is a visualizing technique to plot high-dimensional data in a low-dimensional space. Therefore, it’s not desirable to interpret a t-SNE plot quantitatively.\n\n\nInstead, what we can get from this map is the distance between the points (which items are close and which are far apart). The closer the distance between the two items is, the more similar the composition they have. Therefore this enables us to compare the items without having any chemistry background.\n\n\n# Plot the map\nshow(plot)"
  },
  {
    "objectID": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#comparing-two-products",
    "href": "datacamp_projects/guided/python/cosmetic_ingredients/notebook.html#comparing-two-products",
    "title": "Exploring ingredients of cosmetics",
    "section": "11. Comparing two products",
    "text": "11. Comparing two products\n\nSince there are so many cosmetics and so many ingredients, the plot doesn’t have many super obvious patterns that simpler t-SNE plots can have (example). Our plot requires some digging to find insights, but that’s okay!\n\n\nSay we enjoyed a specific product, there’s an increased chance we’d enjoy another product that is similar in chemical composition. Say we enjoyed AmorePacific’s Color Control Cushion Compact Broad Spectrum SPF 50+. We could find this product on the plot and see if a similar product(s) exist. And it turns out it does! If we look at the points furthest left on the plot, we see LANEIGE’s BB Cushion Hydra Radiance SPF 50 essentially overlaps with the AmorePacific product. By looking at the ingredients, we can visually confirm the compositions of the products are similar (though it is difficult to do, which is why we did this analysis in the first place!), plus LANEIGE’s version is $22 cheaper and actually has higher ratings.\n\n\nIt’s not perfect, but it’s useful. In real life, we can actually use our little ingredient-based recommendation engine help us make educated cosmetic purchase choices.\n\n\n# Print the ingredients of two similar cosmetics\ncosmetic_1 = moisturizers_dry[moisturizers_dry['Name'] == \"Color Control Cushion Compact Broad Spectrum SPF 50+\"]\ncosmetic_2 = moisturizers_dry[moisturizers_dry['Name'] == \"BB Cushion Hydra Radiance SPF 50\"]\n\n# Display each item's data and ingredients\ndisplay(cosmetic_1)\nprint(cosmetic_1.Ingredients.values)\ndisplay(cosmetic_2)\nprint(cosmetic_2.Ingredients.values)\n\n\n\n\n\n\n\n\nLabel\nBrand\nName\nPrice\nRank\nIngredients\nCombination\nDry\nNormal\nOily\nSensitive\nX\nY\n\n\n\n\n45\nMoisturizer\nAMOREPACIFIC\nColor Control Cushion Compact Broad Spectrum S...\n60\n4.0\nPhyllostachis Bambusoides Juice, Cyclopentasil...\n1\n1\n1\n1\n1\n2.775364\n-0.274434\n\n\n\n\n\n\n\n['Phyllostachis Bambusoides Juice, Cyclopentasiloxane, Cyclohexasiloxane, Peg-10 Dimethicone, Phenyl Trimethicone, Butylene Glycol, Butylene Glycol Dicaprylate/Dicaprate, Alcohol, Arbutin, Lauryl Peg-9 Polydimethylsiloxyethyl Dimethicone, Acrylates/Ethylhexyl Acrylate/Dimethicone Methacrylate Copolymer, Polyhydroxystearic Acid, Sodium Chloride, Polymethyl Methacrylate, Aluminium Hydroxide, Stearic Acid, Disteardimonium Hectorite, Triethoxycaprylylsilane, Ethylhexyl Palmitate, Lecithin, Isostearic Acid, Isopropyl Palmitate, Phenoxyethanol, Polyglyceryl-3 Polyricinoleate, Acrylates/Stearyl Acrylate/Dimethicone Methacrylate Copolymer, Dimethicone, Disodium Edta, Trimethylsiloxysilicate, Ethylhexyglycerin, Dimethicone/Vinyl Dimethicone Crosspolymer, Water, Silica, Camellia Japonica Seed Oil, Camillia Sinensis Leaf Extract, Caprylyl Glycol, 1,2-Hexanediol, Fragrance, Titanium Dioxide, Iron Oxides (Ci 77492, Ci 77491, Ci77499).']\n\n\n\n\n\n\n\n\n\nLabel\nBrand\nName\nPrice\nRank\nIngredients\nCombination\nDry\nNormal\nOily\nSensitive\nX\nY\n\n\n\n\n55\nMoisturizer\nLANEIGE\nBB Cushion Hydra Radiance SPF 50\n38\n4.3\nWater, Cyclopentasiloxane, Zinc Oxide (CI 7794...\n1\n1\n1\n1\n1\n2.814905\n-0.277909\n\n\n\n\n\n\n\n['Water, Cyclopentasiloxane, Zinc Oxide (CI 77947), Ethylhexyl Methoxycinnamate, PEG-10 Dimethicone, Cyclohexasiloxane, Phenyl Trimethicone, Iron Oxides (CI 77492), Butylene Glycol Dicaprylate/Dicaprate, Niacinamide, Lauryl PEG-9 Polydimethylsiloxyethyl Dimethicone, Acrylates/Ethylhexyl Acrylate/Dimethicone Methacrylate Copolymer, Titanium Dioxide (CI 77891 , Iron Oxides (CI 77491), Butylene Glycol, Sodium Chloride, Iron Oxides (CI 77499), Aluminum Hydroxide, HDI/Trimethylol Hexyllactone Crosspolymer, Stearic Acid, Methyl Methacrylate Crosspolymer, Triethoxycaprylylsilane, Phenoxyethanol, Fragrance, Disteardimonium Hectorite, Caprylyl Glycol, Yeast Extract, Acrylates/Stearyl Acrylate/Dimethicone Methacrylate Copolymer, Dimethicone, Trimethylsiloxysilicate, Polysorbate 80, Disodium EDTA, Hydrogenated Lecithin, Dimethicone/Vinyl Dimethicone Crosspolymer, Mica (CI 77019), Silica, 1,2-Hexanediol, Polypropylsilsesquioxane, Chenopodium Quinoa Seed Extract, Magnesium Sulfate, Calcium Chloride, Camellia Sinensis Leaf Extract, Manganese Sulfate, Zinc Sulfate, Ascorbyl Glucoside.']"
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html",
    "title": "Exploring movie plots using NLP",
    "section": "",
    "text": "We all love watching movies! There are some movies we like, some we don’t. Most people have a preference for movies of a similar genre. Some of us love watching action movies, while some of us like watching horror. Some of us like watching movies that have ninjas in them, while some of us like watching superheroes.\n\n\nMovies within a genre often share common base parameters. Consider the following two movies:\n\n\n \n\n\nBoth movies, 2001: A Space Odyssey and Close Encounters of the Third Kind, are movies based on aliens coming to Earth. I’ve seen both, and they indeed share many similarities. We could conclude that both of these fall into the same genre of movies based on intuition, but that’s no fun in a data science context. In this notebook, we will quantify the similarity of movies based on their plot summaries available on IMDb and Wikipedia, then separate them into groups, also known as clusters. We’ll create a dendrogram to represent how closely the movies are related to each other.\n\n\nLet’s start by importing the dataset and observing the data provided.\n\n\n# Import modules\nimport numpy as np\nimport pandas as pd\nimport nltk\n\n# Set seed for reproducibility\nnp.random.seed(5)\n\n# Read in IMDb and Wikipedia movie data (both in same file)\nmovies_df = pd.read_csv('datasets/movies.csv')\n\nprint(\"Number of movies loaded: %s \" % (len(movies_df)))\n\n# Display the data\nmovies_df.head(10)\n\nNumber of movies loaded: 100 \n\n\n\n\n\n\n\n\n\nrank\ntitle\ngenre\nwiki_plot\nimdb_plot\n\n\n\n\n0\n0\nThe Godfather\n[u' Crime', u' Drama']\nOn the day of his only daughter's wedding, Vit...\nIn late summer 1945, guests are gathered for t...\n\n\n1\n1\nThe Shawshank Redemption\n[u' Crime', u' Drama']\nIn 1947, banker Andy Dufresne is convicted of ...\nIn 1947, Andy Dufresne (Tim Robbins), a banker...\n\n\n2\n2\nSchindler's List\n[u' Biography', u' Drama', u' History']\nIn 1939, the Germans move Polish Jews into the...\nThe relocation of Polish Jews from surrounding...\n\n\n3\n3\nRaging Bull\n[u' Biography', u' Drama', u' Sport']\nIn a brief scene in 1964, an aging, overweight...\nThe film opens in 1964, where an older and fat...\n\n\n4\n4\nCasablanca\n[u' Drama', u' Romance', u' War']\nIt is early December 1941. American expatriate...\nIn the early years of World War II, December 1...\n\n\n5\n5\nOne Flew Over the Cuckoo's Nest\n[u' Drama']\nIn 1963 Oregon, Randle Patrick \"Mac\" McMurphy ...\nIn 1963 Oregon, Randle Patrick McMurphy (Nicho...\n\n\n6\n6\nGone with the Wind\n[u' Drama', u' Romance', u' War']\n\\nPart 1\\n \\n Part 1 Part 1 \\n \\n On the...\nThe film opens in Tara, a cotton plantation ow...\n\n\n7\n7\nCitizen Kane\n[u' Drama', u' Mystery']\n\\n\\n\\n\\nOrson Welles as Charles Foster Kane\\n\\...\nIt's 1941, and newspaper tycoon Charles Foster...\n\n\n8\n8\nThe Wizard of Oz\n[u' Adventure', u' Family', u' Fantasy', u' Mu...\nThe film starts in sepia-tinted Kansas in the ...\nDorothy Gale (Judy Garland) is an orphaned tee...\n\n\n9\n9\nTitanic\n[u' Drama', u' Romance']\nIn 1996, treasure hunter Brock Lovett and his ...\nIn 1996, treasure hunter Brock Lovett and his ..."
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-and-observe-dataset",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-and-observe-dataset",
    "title": "Exploring movie plots using NLP",
    "section": "",
    "text": "We all love watching movies! There are some movies we like, some we don’t. Most people have a preference for movies of a similar genre. Some of us love watching action movies, while some of us like watching horror. Some of us like watching movies that have ninjas in them, while some of us like watching superheroes.\n\n\nMovies within a genre often share common base parameters. Consider the following two movies:\n\n\n \n\n\nBoth movies, 2001: A Space Odyssey and Close Encounters of the Third Kind, are movies based on aliens coming to Earth. I’ve seen both, and they indeed share many similarities. We could conclude that both of these fall into the same genre of movies based on intuition, but that’s no fun in a data science context. In this notebook, we will quantify the similarity of movies based on their plot summaries available on IMDb and Wikipedia, then separate them into groups, also known as clusters. We’ll create a dendrogram to represent how closely the movies are related to each other.\n\n\nLet’s start by importing the dataset and observing the data provided.\n\n\n# Import modules\nimport numpy as np\nimport pandas as pd\nimport nltk\n\n# Set seed for reproducibility\nnp.random.seed(5)\n\n# Read in IMDb and Wikipedia movie data (both in same file)\nmovies_df = pd.read_csv('datasets/movies.csv')\n\nprint(\"Number of movies loaded: %s \" % (len(movies_df)))\n\n# Display the data\nmovies_df.head(10)\n\nNumber of movies loaded: 100 \n\n\n\n\n\n\n\n\n\nrank\ntitle\ngenre\nwiki_plot\nimdb_plot\n\n\n\n\n0\n0\nThe Godfather\n[u' Crime', u' Drama']\nOn the day of his only daughter's wedding, Vit...\nIn late summer 1945, guests are gathered for t...\n\n\n1\n1\nThe Shawshank Redemption\n[u' Crime', u' Drama']\nIn 1947, banker Andy Dufresne is convicted of ...\nIn 1947, Andy Dufresne (Tim Robbins), a banker...\n\n\n2\n2\nSchindler's List\n[u' Biography', u' Drama', u' History']\nIn 1939, the Germans move Polish Jews into the...\nThe relocation of Polish Jews from surrounding...\n\n\n3\n3\nRaging Bull\n[u' Biography', u' Drama', u' Sport']\nIn a brief scene in 1964, an aging, overweight...\nThe film opens in 1964, where an older and fat...\n\n\n4\n4\nCasablanca\n[u' Drama', u' Romance', u' War']\nIt is early December 1941. American expatriate...\nIn the early years of World War II, December 1...\n\n\n5\n5\nOne Flew Over the Cuckoo's Nest\n[u' Drama']\nIn 1963 Oregon, Randle Patrick \"Mac\" McMurphy ...\nIn 1963 Oregon, Randle Patrick McMurphy (Nicho...\n\n\n6\n6\nGone with the Wind\n[u' Drama', u' Romance', u' War']\n\\nPart 1\\n \\n Part 1 Part 1 \\n \\n On the...\nThe film opens in Tara, a cotton plantation ow...\n\n\n7\n7\nCitizen Kane\n[u' Drama', u' Mystery']\n\\n\\n\\n\\nOrson Welles as Charles Foster Kane\\n\\...\nIt's 1941, and newspaper tycoon Charles Foster...\n\n\n8\n8\nThe Wizard of Oz\n[u' Adventure', u' Family', u' Fantasy', u' Mu...\nThe film starts in sepia-tinted Kansas in the ...\nDorothy Gale (Judy Garland) is an orphaned tee...\n\n\n9\n9\nTitanic\n[u' Drama', u' Romance']\nIn 1996, treasure hunter Brock Lovett and his ...\nIn 1996, treasure hunter Brock Lovett and his ..."
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#combine-wikipedia-and-imdb-plot-summaries",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#combine-wikipedia-and-imdb-plot-summaries",
    "title": "Exploring movie plots using NLP",
    "section": "2. Combine Wikipedia and IMDb plot summaries",
    "text": "2. Combine Wikipedia and IMDb plot summaries\n\nThe dataset we imported currently contains two columns titled wiki_plot and imdb_plot. They are the plot found for the movies on Wikipedia and IMDb, respectively. The text in the two columns is similar, however, they are often written in different tones and thus provide context on a movie in a different manner of linguistic expression. Further, sometimes the text in one column may mention a feature of the plot that is not present in the other column. For example, consider the following plot extracts from The Godfather:\n\n\n\nWikipedia: “On the day of his only daughter’s wedding, Vito Corleone”\n\n\nIMDb: “In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone’s daughter Connie”\n\n\n\nWhile the Wikipedia plot only mentions it is the day of the daughter’s wedding, the IMDb plot also mentions the year of the scene and the name of the daughter.\n\n\nLet’s combine both the columns to avoid the overheads in computation associated with extra columns to process.\n\n\n# Combine wiki_plot and imdb_plot into a single column\nmovies_df['plot'] = movies_df['wiki_plot'].astype(str) + \"\\n\" + \\\n                 movies_df['imdb_plot'].astype(str)\n\n# Inspect the new DataFrame\nmovies_df.head()\n\n\n\n\n\n\n\n\nrank\ntitle\ngenre\nwiki_plot\nimdb_plot\nplot\n\n\n\n\n0\n0\nThe Godfather\n[u' Crime', u' Drama']\nOn the day of his only daughter's wedding, Vit...\nIn late summer 1945, guests are gathered for t...\nOn the day of his only daughter's wedding, Vit...\n\n\n1\n1\nThe Shawshank Redemption\n[u' Crime', u' Drama']\nIn 1947, banker Andy Dufresne is convicted of ...\nIn 1947, Andy Dufresne (Tim Robbins), a banker...\nIn 1947, banker Andy Dufresne is convicted of ...\n\n\n2\n2\nSchindler's List\n[u' Biography', u' Drama', u' History']\nIn 1939, the Germans move Polish Jews into the...\nThe relocation of Polish Jews from surrounding...\nIn 1939, the Germans move Polish Jews into the...\n\n\n3\n3\nRaging Bull\n[u' Biography', u' Drama', u' Sport']\nIn a brief scene in 1964, an aging, overweight...\nThe film opens in 1964, where an older and fat...\nIn a brief scene in 1964, an aging, overweight...\n\n\n4\n4\nCasablanca\n[u' Drama', u' Romance', u' War']\nIt is early December 1941. American expatriate...\nIn the early years of World War II, December 1...\nIt is early December 1941. American expatriate..."
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#tokenization",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#tokenization",
    "title": "Exploring movie plots using NLP",
    "section": "3. Tokenization",
    "text": "3. Tokenization\n\nTokenization is the process by which we break down articles into individual sentences or words, as needed. Besides the tokenization method provided by NLTK, we might have to perform additional filtration to remove tokens which are entirely numeric values or punctuation.\n\n\nWhile a program may fail to build context from “While waiting at a bus stop in 1981” (Forrest Gump), because this string would not match in any dictionary, it is possible to build context from the words “while”, “waiting” or “bus” because they are present in the English dictionary.\n\n\nLet us perform tokenization on a small extract from The Godfather.\n\n\n# Tokenize a paragraph into sentences and store in sent_tokenized\nsent_tokenized = [sent for sent in nltk.sent_tokenize(\"\"\"\n                        Today (May 19, 2016) is his only daughter's wedding. \n                        Vito Corleone is the Godfather.\n                        \"\"\")]\n\n# Word Tokenize first sentence from sent_tokenized, save as words_tokenized\nwords_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]\n\n# Remove tokens that do not contain any letters from words_tokenized\nimport re\n\nfiltered = [word for word in words_tokenized if re.search('[a-zA-Z]', word)]\n\n# Display filtered words to observe words after tokenization\nfiltered\n\n['Today', 'May', 'is', 'his', 'only', 'daughter', \"'s\", 'wedding']"
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#stemming",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#stemming",
    "title": "Exploring movie plots using NLP",
    "section": "4. Stemming",
    "text": "4. Stemming\n\nStemming is the process by which we bring down a word from its different forms to the root word. This helps us establish meaning to different forms of the same words without having to deal with each form separately. For example, the words ‘fishing’, ‘fished’, and ‘fisher’ all get stemmed to the word ‘fish’.\n\n\nConsider the following sentences:\n\n\n\n“Young William Wallace witnesses the treachery of Longshanks” ~ Gladiator\n\n\n“escapes to the city walls only to witness Cicero’s death” ~ Braveheart\n\n\n\nInstead of building separate dictionary entries for both witnesses and witness, which mean the same thing outside of quantity, stemming them reduces them to ‘wit’.\n\n\nThere are different algorithms available for stemming such as the Porter Stemmer, Snowball Stemmer, etc. We shall use the Snowball Stemmer.\n\n\n# Import the SnowballStemmer to perform stemming\nfrom nltk.stem.snowball import SnowballStemmer\n\n# Create an English language SnowballStemmer object\nstemmer = SnowballStemmer(\"english\")\n\n# Print filtered to observe words without stemming\nprint(\"Without stemming: \", filtered)\n\n# Stem the words from filtered and store in stemmed_words\nstemmed_words = [stemmer.stem(word) for word in filtered]\n\n# Print the stemmed_words to observe words after stemming\nprint(\"After stemming:   \", stemmed_words)\n\nWithout stemming:  ['Today', 'May', 'is', 'his', 'only', 'daughter', \"'s\", 'wedding']\nAfter stemming:    ['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']"
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#club-together-tokenize-stem",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#club-together-tokenize-stem",
    "title": "Exploring movie plots using NLP",
    "section": "5. Club together Tokenize & Stem",
    "text": "5. Club together Tokenize & Stem\n\nWe are now able to tokenize and stem sentences. But we may have to use the two functions repeatedly one after the other to handle a large amount of data, hence we can think of wrapping them in a function and passing the text to be tokenized and stemmed as the function argument. Then we can pass the new wrapping function, which shall perform both tokenizing and stemming instead of just tokenizing, as the tokenizer argument while creating the TF-IDF vector of the text.\n\n\nWhat difference does it make though? Consider the sentence from the plot of The Godfather: “Today (May 19, 2016) is his only daughter’s wedding.” If we do a ‘tokenize-only’ for this sentence, we have the following result:\n\n\n\n‘today’, ‘may’, ‘is’, ‘his’, ‘only’, ‘daughter’, “‘s”, ’wedding’\n\n\n\nBut when we do a ‘tokenize-and-stem’ operation we get:\n\n\n\n‘today’, ‘may’, ‘is’, ‘his’, ‘onli’, ‘daughter’, “‘s”, ’wed’\n\n\n\nAll the words are in their root form, which will lead to a better establishment of meaning as some of the non-root forms may not be present in the NLTK training corpus.\n\n\n# Define a function to perform both stemming and tokenization\ndef tokenize_and_stem(text):\n    \n    # Tokenize by sentence, then by word\n    tokens = [sent for sent in nltk.sent_tokenize(text)]\n    tokens = [word for x in tokens for word in nltk.word_tokenize(x)]\n    \n    # Filter out raw tokens to remove noise\n    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n    \n    # Stem the filtered_tokens\n    stems = [stemmer.stem(token) for token in filtered_tokens]\n    \n    return stems\n\nwords_stemmed = tokenize_and_stem(\"Today (May 19, 2016) is his only daughter's wedding.\")\nprint(words_stemmed)\n\n['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']"
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#create-tfidfvectorizer",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#create-tfidfvectorizer",
    "title": "Exploring movie plots using NLP",
    "section": "6. Create TfidfVectorizer",
    "text": "6. Create TfidfVectorizer\n\nComputers do not understand text. These are machines only capable of understanding numbers and performing numerical computation. Hence, we must convert our textual plot summaries to numbers for the computer to be able to extract meaning from them. One simple method of doing this would be to count all the occurrences of each word in the entire vocabulary and return the counts in a vector. Enter CountVectorizer.\n\n\nConsider the word ‘the’. It appears quite frequently in almost all movie plots and will have a high count in each case. But obviously, it isn’t the theme of all the movies! Term Frequency-Inverse Document Frequency (TF-IDF) is one method which overcomes the shortcomings of CountVectorizer. The Term Frequency of a word is the measure of how often it appears in a document, while the Inverse Document Frequency is the parameter which reduces the importance of a word if it frequently appears in several documents.\n\n\nFor example, when we apply the TF-IDF on the first 3 sentences from the plot of The Wizard of Oz, we are told that the most important word there is ‘Toto’, the pet dog of the lead character. This is because the movie begins with ‘Toto’ biting someone due to which the journey of Oz begins!\n\n\nIn simplest terms, TF-IDF recognizes words which are unique and important to any given document. Let’s create one for our purposes.\n\n\n# Import TfidfVectorizer to create TF-IDF vectors\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Instantiate TfidfVectorizer object with stopwords and tokenizer\n# parameters for efficient processing of text\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=0.2, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem,\n                                 ngram_range=(1,3))"
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#fit-transform-tfidfvectorizer",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#fit-transform-tfidfvectorizer",
    "title": "Exploring movie plots using NLP",
    "section": "7. Fit transform TfidfVectorizer",
    "text": "7. Fit transform TfidfVectorizer\n\nOnce we create a TF-IDF Vectorizer, we must fit the text to it and then transform the text to produce the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we use the fit_transform() method of the TfidfVectorizer object.\n\n\nIf we observe the TfidfVectorizer object we created, we come across a parameter stopwords. ‘stopwords’ are those words in a given text which do not contribute considerably towards the meaning of the sentence and are generally grammatical filler words. For example, in the sentence ‘Dorothy Gale lives with her dog Toto on the farm of her Aunt Em and Uncle Henry’, we could drop the words ‘her’ and ‘the’, and still have a similar overall meaning to the sentence. Thus, ‘her’ and ‘the’ are stopwords and can be conveniently dropped from the sentence.\n\n\nOn setting the stopwords to ‘english’, we direct the vectorizer to drop all stopwords from a pre-defined list of English language stopwords present in the nltk module. Another parameter, ngram_range, defines the length of the ngrams to be formed while vectorizing the text.\n\n\n# Fit and transform the tfidf_vectorizer with the \"plot\" of each movie\n# to create a vector representation of the plot summaries\ntfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[\"plot\"]])\n\nprint(tfidf_matrix.shape)\n\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n  warnings.warn(\n\n\n(100, 564)"
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-kmeans-and-create-clusters",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-kmeans-and-create-clusters",
    "title": "Exploring movie plots using NLP",
    "section": "8. Import KMeans and create clusters",
    "text": "8. Import KMeans and create clusters\n\nTo determine how closely one movie is related to the other by the help of unsupervised learning, we can use clustering techniques. Clustering is the method of grouping together a number of items such that they exhibit similar properties. According to the measure of similarity desired, a given sample of items can have one or more clusters.\n\n\nA good basis of clustering in our dataset could be the genre of the movies. Say we could have a cluster ‘0’ which holds movies of the ‘Drama’ genre. We would expect movies like Chinatown or Psycho to belong to this cluster. Similarly, the cluster ‘1’ in this project holds movies which belong to the ‘Adventure’ genre (Lawrence of Arabia and the Raiders of the Lost Ark, for example).\n\n\nK-means is an algorithm which helps us to implement clustering in Python. The name derives from its method of implementation: the given sample is divided into K clusters where each cluster is denoted by the mean of all the items lying in that cluster.\n\n\nWe get the following distribution for the clusters:\n\n\n\n\n\n# Import k-means to perform clusters\nfrom sklearn.cluster import KMeans\n\n# Create a KMeans object with 5 clusters and save as km\nkm = KMeans(n_clusters=5)\n\n# Fit the k-means object with tfidf_matrix\nkm.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()\n\n# Create a column cluster to denote the generated cluster for each movie\nmovies_df[\"cluster\"] = clusters\n\n# Display number of films per cluster (clusters from 0 to 4)\nmovies_df['cluster'].value_counts() \n\ncluster\n1    38\n2    27\n3    18\n4    14\n0     3\nName: count, dtype: int64"
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#calculate-similarity-distance",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#calculate-similarity-distance",
    "title": "Exploring movie plots using NLP",
    "section": "9. Calculate similarity distance",
    "text": "9. Calculate similarity distance\n\nConsider the following two sentences from the movie The Wizard of Oz:\n\n\n\n“they find in the Emerald City”\n\n\n“they finally reach the Emerald City”\n\n\n\nIf we put the above sentences in a CountVectorizer, the vocabulary produced would be “they, find, in, the, Emerald, City, finally, reach” and the vectors for each sentence would be as follows:\n\n\n\n1, 1, 1, 1, 1, 1, 0, 0\n\n\n1, 0, 0, 1, 1, 1, 1, 1\n\n\n\nWhen we calculate the cosine angle formed between the vectors represented by the above, we get a score of 0.667. This means the above sentences are very closely related. Similarity distance is 1 - cosine similarity angle. This follows from that if the vectors are similar, the cosine of their angle would be 1 and hence, the distance between then would be 1 - 1 = 0.\n\n\nLet’s calculate the similarity distance for all of our movies.\n\n\n# Import cosine_similarity to calculate similarity of movie plots\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Calculate the similarity distance\nsimilarity_distance = 1 - cosine_similarity(tfidf_matrix)"
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-matplotlib-linkage-and-dendrograms",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#import-matplotlib-linkage-and-dendrograms",
    "title": "Exploring movie plots using NLP",
    "section": "10. Import Matplotlib, Linkage, and Dendrograms",
    "text": "10. Import Matplotlib, Linkage, and Dendrograms\n\nWe shall now create a tree-like diagram (called a dendrogram) of the movie titles to help us understand the level of similarity between them visually. Dendrograms help visualize the results of hierarchical clustering, which is an alternative to k-means clustering. Two pairs of movies at the same level of hierarchical clustering are expected to have similar strength of similarity between the corresponding pairs of movies. For example, the movie Fargo would be as similar to North By Northwest as the movie Platoon is to Saving Private Ryan, given both the pairs exhibit the same level of the hierarchy.\n\n\nLet’s import the modules we’ll need to create our dendrogram.\n\n\n# Import matplotlib.pyplot for plotting graphs\nimport matplotlib.pyplot as plt\n\n# Configure matplotlib to display the output inline\n%matplotlib inline\n\n# Import modules necessary to plot dendrogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram"
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#create-merging-and-plot-dendrogram",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#create-merging-and-plot-dendrogram",
    "title": "Exploring movie plots using NLP",
    "section": "11. Create merging and plot dendrogram",
    "text": "11. Create merging and plot dendrogram\n\nWe shall plot a dendrogram of the movies whose similarity measure will be given by the similarity distance we previously calculated. The lower the similarity distance between any two movies, the lower their linkage will make an intercept on the y-axis. For instance, the lowest dendrogram linkage we shall discover will be between the movies, It’s a Wonderful Life and A Place in the Sun. This indicates that the movies are very similar to each other in their plots.\n\n\n# Create mergings matrix \nmergings = linkage(similarity_distance, method='complete')\n\n# Plot the dendrogram, using title as label column\ndendrogram_ = dendrogram(mergings,\n               labels=[x for x in movies_df[\"title\"]],\n               leaf_rotation=90,\n               leaf_font_size=16,\n)\n\n# Adjust the plot\nfig = plt.gcf()\n_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]\nfig.set_size_inches(108, 21)\n\n# Show the plotted dendrogram\nplt.show()"
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#which-movies-are-most-similar",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html#which-movies-are-most-similar",
    "title": "Exploring movie plots using NLP",
    "section": "12. Which movies are most similar?",
    "text": "12. Which movies are most similar?\n\nWe can now determine the similarity between movies based on their plots! To wrap up, let’s answer one final question: which movie is most similar to the movie Braveheart?\n\n\n#find row index of Braveheart\nbidx = movies_df.title.eq('Braveheart').idxmax()\n#extract the vector containing the similarity vector for Bravheart\n#sort indices by descending similarity\n#first entry is Braveheart itself, second index points to most similar movie\nsimilar_idx = np.argsort(similarity_distance[bidx])[1]\n\n\n# Answer the question\nans = movies_df.title.iloc[similar_idx]\nprint(ans)\n\nGladiator\n\n\nWell, that makes sense!"
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "heiner.atze@gmx.net, +33 7 80 84 91 20, Gentilly",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#education",
    "href": "cv/cv.html#education",
    "title": "Curriculum Vitae",
    "section": "EDUCATION",
    "text": "EDUCATION\nPostgraduate diploma – Biostatistics and Methods in Public Health  Paris Saclay University, Paris, France Oct 2023 — Sep 2024\nMaster level courses in :\n\nProbability and Statistics\nClinical Research\nQuantitative Epidemiology\n\nDoctor of Philosophy – Biochemistry, Microbiology  Sorbonne University, Paris, France Nov 2018 — Sep 2021\nMaster of Science(*) - Medicinal Chemistry  Friedrich-Schiller-University, Jena, Germany Sep 2014 — Sep 2016\n(*)German Diplom, degree awarded after 5 years of study and submission of a research thesis\n\nState examination - Pharmacy  Friedrich-Schiller-University, Jena, Germany Sep 2010 — Sep 2014",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#research-experience",
    "href": "cv/cv.html#research-experience",
    "title": "Curriculum Vitae",
    "section": "RESEARCH EXPERIENCE",
    "text": "RESEARCH EXPERIENCE\nDoctoral Researcher – Biochemistry, Microbiology  INSERM, Paris, France Jun 2018 — Sep 2021\nResearcher in Team 12 “Bacterial structures implicated in antibiotic resistance” at the Centre de Recherche des Cordeliers, Paris\nTwo main axes of research:\n\nBiological characterization of new generation β-lactamase inhibitors\nin vitro and in vivo characterization of inhibitors, data analysis and interpretation, feedback into the consult-design-test-repeat cycle in collaboration with the team of organic chemists\nFundamental research on cell wall metabolism in gram-negative bacteria\nDe novo method development: isotopic labeling of cell cultures, sample preparation, analysis by mass spectrometry, custom data analysis tools and pipelines\n\nKey achievements and skills:\n\nExploration of the chemical space around the core inhibitor and identification of curcial ligand-target-interactions\nHandling and managing a large amount of results and data from biological experiments\nDevelopment and maintentance of custom data analysis tools availabale at Gitlab\n\nResearch assistant – Medicinal and Pharmaceutical Chemistry  Friedrich-Schiller-University, Jena, Germany Nov 2014 — Apr 2015\nBiological charaterization of putative anti-inflammatory substances, fundamental research on signaling cascades in inflammation using biochemical methods and imaging techniques",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#work-experience",
    "href": "cv/cv.html#work-experience",
    "title": "Curriculum Vitae",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE\n\nPharmacist\nPharmacie Attal  Fontenay-aux-Roses, France Oct 2021 — present\nPharmaceutical counseling of patients\nResponsible for communication with medical staff in nursing homes\nSupervision of technical staff and pharmacy students\nOther Pharmacies  Jena, Germany Nov 2015 – May 2018\n\n\nPre-registration pharmacist\nF. Hoffmann-La Roche  Basel, Switzerland May 2015 – Oct 2015",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#skills",
    "href": "cv/cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "SKILLS",
    "text": "SKILLS\n\nLanguages\nGerman: Native Speaker\nEnglish: Professional proficiency\nFrench: Professional proficiency\n\n\nData analysis, programming and software packages\n\nProgramming languages:\n\n\nroutine use of pandas, numpy, matplotlib\nproject dependent use of scikit-learn, bokeh\n\n\nbasic database setup and queries, joins and grouping operations\n,  : notions\n\n\n\n\nTechnical lab skills\n\nCellular Biology\nHuman Cell Culture: HEK-293, primary human lymphocytes\nBacterial Cell Culture: Escherichia coli\nMolecular Biology\nmRNA-Extraction, cDNA synthesis\nPolymerase chain reaction\nBiochemistry\nSDS-PAGE, Western Blot\nEnzyme kinetics",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#publications",
    "href": "cv/cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "PUBLICATIONS",
    "text": "PUBLICATIONS\n\nAtze et al. (2022) Bouchet et al. (2021) Bouchet et al. (2020) Le Run et al. (2020) Triboulet et al. (2019) Garscha et al. (2017)\n\n\nJournal articles\n\n\nAtze, H., Liang, Y., Hugonnet, J.-E., Gutierrez, A., Rusconi, F. and Arthur, M. (2022), “Heavy isotope labeling and mass spectrometry reveal unexpected remodeling of bacterial cell wall expansion in response to drugs”, Elife, eLife Sciences Publications Limited, Vol. 11, p. e72863.\n\n\nBouchet, F., Atze, H., Arthur, M., Ethève-Quelquejeu, M. and Iannazzo, L. (2021), “Traceless staudinger ligation to introduce chemical diversity on \\(\\beta\\)-lactamase inhibitors of second generation”, Organic Letters, ACS Publications, Vol. 23 No. 20, pp. 7755–7758.\n\n\nBouchet, F., Atze, H., Fonvielle, M., Edoo, Z., Arthur, M., Ethève-Quelquejeu, M. and Iannazzo, L. (2020), “Diazabicyclooctane functionalization for inhibition of \\(\\beta\\)-lactamases from enterobacteria”, Journal of Medicinal Chemistry, American Chemical Society, Vol. 63 No. 10, pp. 5257–5273.\n\n\nGarscha, U., Romp, E., Pace, S., Rossi, A., Temml, V., Schuster, D., König, S., et al. (2017), “Pharmacological profile and efficiency in vivo of diflapolin, the first dual inhibitor of 5-lipoxygenase-activating protein and soluble epoxide hydrolase”, Scientific Reports, Nature Publishing Group UK London, Vol. 7 No. 1, p. 9398.\n\n\nLe Run, E., Atze, H., Arthur, M. and Mainardi, J.-L. (2020), “Impact of relebactam-mediated inhibition of mycobacterium abscessus BlaMab \\(\\beta\\)-lactamase on the in vitro and intracellular efficacy of imipenem”, Journal of Antimicrobial Chemotherapy, Oxford University Press, Vol. 75 No. 2, pp. 379–383.\n\n\nTriboulet, S., Edoo, Z., Compain, F., Ourghanlian, C., Dupuis, A., Dubée, V., Sutterlin, L., et al. (2019), “Tryptophan fluorescence quenching in \\(\\beta\\)-lactam-interacting proteins is modulated by the structure of intermediates and final products of the acylation reaction”, ACS Infectious Diseases, American Chemical Society, Vol. 5 No. 7, pp. 1169–1176.\n\n\n\n\nCV template adapted from Cynthia Huang",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "datacamp_site/extended.html",
    "href": "datacamp_site/extended.html",
    "title": "Datacamp extended projects",
    "section": "",
    "text": "Can I give some explanation here ?\nTest\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Projects",
      "Data Science",
      "Datacamp",
      "Extended"
    ]
  }
]